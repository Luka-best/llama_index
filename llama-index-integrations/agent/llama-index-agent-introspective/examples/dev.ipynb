{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a049c6d-9d26-4bcc-a0dc-ef259b3ea66e",
   "metadata": {},
   "source": [
    "### CRITIC AgentWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805b353-76d9-4793-860f-3601c30699b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-openai\n",
      "  Using cached llama_index_llms_openai-0.1.16-py3-none-any.whl.metadata (559 bytes)\n",
      "Collecting llama-index-program-openai\n",
      "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.24 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-llms-openai) (0.10.31)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.1 (from llama-index-program-openai)\n",
      "  Using cached llama_index_agent_openai-0.2.3-py3-none-any.whl.metadata (678 bytes)\n",
      "Requirement already satisfied: openai>=1.14.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-agent-openai<0.3.0,>=0.1.1->llama-index-program-openai) (1.23.4)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.1.18)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.24.4)\n",
      "Requirement already satisfied: pandas in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.0.3)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.7.1)\n",
      "Requirement already satisfied: anyio in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.3.0)\n",
      "Requirement already satisfied: certifi in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.4.16)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.1->llama-index-program-openai) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.2.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (24.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.16.0)\n",
      "Using cached llama_index_llms_openai-0.1.16-py3-none-any.whl (10 kB)\n",
      "Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
      "Using cached llama_index_agent_openai-0.2.3-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: llama-index-llms-openai, llama-index-agent-openai, llama-index-program-openai\n",
      "Successfully installed llama-index-agent-openai-0.2.3 llama-index-llms-openai-0.1.16 llama-index-program-openai-0.1.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-openai llama-index-program-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab7469-aacd-4ce1-b4b4-3b085be796b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede28e66-e7a6-42a2-958e-763b2a6f311f",
   "metadata": {},
   "source": [
    "example:\n",
    "- old text\n",
    "- critique\n",
    "- correction\n",
    "\n",
    "chat message few-shot examples: an interaction between user and ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4edbb44-c0f9-4c17-8f72-da9cf13956ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.127.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 (from google-api-python-client)\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Downloading google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-agent-critic-urlArvBc-py3.10/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.2.2)\n",
      "Downloading google_api_python_client-2.127.0-py2.py3-none-any.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.18.0-py3-none-any.whl (138 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uritemplate, pyparsing, pyasn1, protobuf, cachetools, rsa, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.3.3 google-api-core-2.18.0 google-api-python-client-2.127.0 google-auth-2.29.0 google-auth-httplib2-0.2.0 googleapis-common-protos-1.63.0 httplib2-0.22.0 proto-plus-1.23.0 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 pyparsing-3.1.2 rsa-4.9 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-api-python-client -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae4741-d04e-4898-90e0-082c40e66e83",
   "metadata": {},
   "source": [
    "### Working with Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457730c-1e81-4da5-8e2f-157b0683d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from typing import Dict, Optional\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Perspective:\n",
    "    \"\"\"Custom class to interact with Perspective API.\"\"\"\n",
    "\n",
    "    attributes = [\n",
    "        \"toxicity\",\n",
    "        \"severe_toxicity\",\n",
    "        \"identity_attack\",\n",
    "        \"insult\",\n",
    "        \"profanity\",\n",
    "        \"threat\",\n",
    "        \"sexually_explicit\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None) -> None:\n",
    "        if api_key is None:\n",
    "            try:\n",
    "                api_key = os.environ[\"PERSPECTIVE_API_KEY\"]\n",
    "            except KeyError:\n",
    "                raise ValueError(\n",
    "                    \"Please provide an api key or set PERSPECTIVE_API_KEY env var.\"\n",
    "                )\n",
    "\n",
    "        self._client = discovery.build(\n",
    "            \"commentanalyzer\",\n",
    "            \"v1alpha1\",\n",
    "            developerKey=api_key,\n",
    "            discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "            static_discovery=False,\n",
    "        )\n",
    "\n",
    "    def get_toxicity_scores(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Function that makes API call to Perspective to get toxicity scores across various attributes.\"\"\"\n",
    "\n",
    "        analyze_request = {\n",
    "            \"comment\": {\"text\": text},\n",
    "            \"requestedAttributes\": {att.upper(): {} for att in self.attributes},\n",
    "        }\n",
    "\n",
    "        response = self._client.comments().analyze(body=analyze_request).execute()\n",
    "        try:\n",
    "            return {\n",
    "                att: response[\"attributeScores\"][att.upper()][\"summaryScore\"][\"value\"]\n",
    "                for att in self.attributes\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Unable to parse response\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4abf8-a189-41ba-bf8f-2fd05f83ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "perspective = Perspective()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f709c-f9cf-4ba8-a9de-fce6241b60fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxicity': 0.02543884,\n",
       " 'severe_toxicity': 0.0018405914,\n",
       " 'identity_attack': 0.0061046463,\n",
       " 'insult': 0.012300906,\n",
       " 'profanity': 0.01805087,\n",
       " 'threat': 0.008285114,\n",
       " 'sexually_explicit': 0.016395653}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspective.get_toxicity_scores(text=\"friendly greetings from python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c0875-1333-47ed-83c6-91224f26bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from llama_index.core.bridge.pydantic import Field\n",
    "\n",
    "\n",
    "def perspective_function_tool(\n",
    "    text: str = Field(\n",
    "        default_factory=str, description=\"The text to compute toxicity scores on.\"\n",
    "    )\n",
    ") -> Tuple[str, float]:\n",
    "    \"\"\"Returns the toxicity score of the most problematic toxic attribute.\"\"\"\n",
    "\n",
    "    scores = perspective.get_toxicity_scores(text=text)\n",
    "    max_key = max(scores, key=scores.get)\n",
    "    return (max_key, scores[max_key] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e206ce-b76c-4a11-ac68-25caf8b838d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('toxicity', 2.5438840000000003)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspective_function_tool(text=\"friendly greetings from python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493bb4bf-5453-444a-91a9-9c84f2785967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "pespective_tool = FunctionTool.from_defaults(\n",
    "    perspective_function_tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b879915-f7c2-4e96-93e7-327a3c5cdf19",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313455d8-63bb-41ad-845b-22a57b65e690",
   "metadata": {},
   "source": [
    "```\n",
    "system: you are an assistant that improves toxicity of text\n",
    "\n",
    "user: provide a critique for the following text using tools: are some of the most hypocritical people I've ever met. One vegan acquantiance routinely beat her\n",
    "\n",
    "assistant: the text has 57% toxicity of insult. \n",
    "\n",
    "user: provide a correction of the text based on your critique\n",
    "\n",
    "assistant: then turn around after their day job and eat meat when nobody is looking.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0232b-29a3-44b1-9ec0-6339dab2bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def create_critic_correct_chat_messages(\n",
    "    text: str,\n",
    "    score: float,\n",
    "    toxicity_attribute: str,\n",
    "    improved_text: str,\n",
    "    tool_name: str,\n",
    "    tool_id: str,\n",
    ") -> List[ChatMessage]:\n",
    "    critique_request_msg = ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Please provide a toxicity critique for the following text:\\n {text}\",\n",
    "    )\n",
    "\n",
    "    critique_response_msg = ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content=\"The provided text has toxicity {score}% of {toxicity_attribute}.\",\n",
    "    )\n",
    "\n",
    "    correct_request_msg = ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Now, use your toxicity critique and generate a new version of the text that has reduced levels of toxicity. Provide the new text after first writing '[RESULT]'.\",\n",
    "    )\n",
    "\n",
    "    correct_response_msg = ChatMessage(\n",
    "        role=MessageRole.ASSISTANT, content=\"[RESULT] {improved_text}\"\n",
    "    )\n",
    "\n",
    "    chat_template = ChatPromptTemplate(\n",
    "        [\n",
    "            critique_request_msg,\n",
    "            critique_response_msg,\n",
    "            # correct_request_msg,\n",
    "            # correct_response_msg,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return chat_template.format_messages(\n",
    "        text=text,\n",
    "        score=score,\n",
    "        toxicity_attribute=toxicity_attribute,\n",
    "        improved_text=improved_text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644d732-4711-4ea1-ab82-377c646fc605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"Please provide a toxicity critique for the following text:\\n are some of the most hypocritical people I've ever met. One vegan acquantiance routinely beat her\", additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='The provided text has toxicity 57.2% of insult.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_critic_correct_chat_messages(\n",
    "    text=\"are some of the most hypocritical people I've ever met. One vegan acquantiance routinely beat her\",\n",
    "    score=\"57.2\",\n",
    "    toxicity_attribute=\"insult\",\n",
    "    improved_text=\"then turn around after their day job and eat meat when nobody is looking\",\n",
    "    tool_name=\"perspective_function_tool\",\n",
    "    tool_id=\"call_LnlG5Z6VvDerop2XuHSSUzy0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b54e5-4a7e-4b2b-9486-d95abb54e8d8",
   "metadata": {},
   "source": [
    "#### STEPS\n",
    "\n",
    "0. Create a critique user request and add it to the chat history using the current version of the text\n",
    "1. implement LLM function call for critique\n",
    "2. check if critiquing LLM deems the text to be sufficient -> in which case exit\n",
    "4. create a critique `ChatMessage` and add it to the chat history\n",
    "5. create a correct user request and add it to the chat history\n",
    "6. get correction from the critic agent and add it to the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e049bf9-7e07-4c71-b510-3f4c6099bd77",
   "metadata": {},
   "source": [
    "### ABSTRACTIONS\n",
    "\n",
    "- CritiqueLLM (Evaluator), optionally uses tools, provides evaluation according to rubric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e77c90-f7ef-4f47-a613-33f3087acc5c",
   "metadata": {},
   "source": [
    "### Critique LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413fcdb-0526-40c9-9e9b-55db114e1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544768bd-f711-4173-80f5-c3c825f656ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05dfe70-c862-4be1-93eb-3585d6215479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='You are a helpful assistant that provides toxcity critiques on provided texts.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content=\"Please provide a toxicity critique for the following text:\\n are some of the most hypocritical people I've ever met. One vegan acquantiance routinely beat her\", additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='The provided text has toxicity 57.2% of insult.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critique_llm_system = ChatMessage(\n",
    "    role=MessageRole.SYSTEM,\n",
    "    content=\"You are a helpful assistant that provides toxcity critiques on provided texts.\",\n",
    ")\n",
    "\n",
    "example = create_critic_correct_chat_messages(\n",
    "    text=\"are some of the most hypocritical people I've ever met. One vegan acquantiance routinely beat her\",\n",
    "    score=\"57.2\",\n",
    "    toxicity_attribute=\"insult\",\n",
    "    improved_text=\"then turn around after their day job and eat meat when nobody is looking\",\n",
    "    tool_name=\"perspective_function_tool\",\n",
    "    tool_id=\"call_LnlG5Z6VvDerop2XuHSSUzy0\",\n",
    ")\n",
    "\n",
    "chat_history = [critique_llm_system] + example\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e2549-f501-4095-8a13-1796dacba8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.chat_with_tools(\n",
    "    tools=[pespective_tool],\n",
    "    user_msg=ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Please provide a toxicity critique for the following text:\\n {text}\".format(\n",
    "            text=\"you can barely see the vulva. I tend to holder her hands and wait for\"\n",
    "        ),\n",
    "    ),\n",
    "    chat_history=chat_history,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bdf8e1-e580-411a-ad75-da2f51feaa52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=None, additional_kwargs={'tool_calls': [ChatCompletionMessageToolCall(id='call_seBc4PQWfhoBW4dZxxARL6Ct', function=Function(arguments='{\"text\":\"you can barely see the vulva. I tend to holder her hands and wait for\"}', name='perspective_function_tool'), type='function')]})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664d0ffa-e75b-4995-a6a1-cffecb7ba2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assistant: None'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb089f7-642b-4ce5-ace3-2d5d1f010c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolSelection(tool_id='call_yUOsm21ZrZuoeomeJOB75JCf', tool_name='perspective_function_tool', tool_kwargs={'text': 'you can barely see the vulva. I tend to holder her hands and wait for'})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls = llm.get_tool_calls_from_response(response, error_on_no_tool_call=False)\n",
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458ccdc-d6d6-4b29-8fdd-880b64ee6f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: perspective_function_tool with args: {\"text\": \"you can barely see the vulva. I tend to holder her hands and wait for\"}\n",
      "=== Function Output ===\n",
      "('sexually_explicit', 76.80230999999999)\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    tools=[pespective_tool],\n",
    "    user_msg=ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Please provide a toxicity critique for the following text:\\n {text}\".format(\n",
    "            text=\"you can barely see the vulva. I tend to holder her hands and wait for\"\n",
    "        ),\n",
    "    ),\n",
    "    chat_history=chat_history,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe7ff8-22d3-4993-8381-27dc19e319ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response=\"('sexually_explicit', 76.80230999999999)\", sources=[ToolOutput(content=\"('sexually_explicit', 76.80230999999999)\", tool_name='perspective_function_tool', raw_input={'args': ('you can barely see the vulva. I tend to holder her hands and wait for',), 'kwargs': {}}, raw_output=('sexually_explicit', 76.80230999999999), is_error=False)], source_nodes=[], is_dummy_stream=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc5fad0-6b8f-4dd0-9cd8-f5baee959312",
   "metadata": {},
   "source": [
    "### Using FunctionCallingAgentWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8426954-b155-4db7-a910-66499b6c81db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "\n",
    "critiquer = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[pespective_tool], llm=OpenAI(\"gpt-3.5-turbo\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b684b0f-4594-4457-aa90-73b84a21aef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step f45f862f-5ff6-437d-b3cf-20dd4ca95799. Step input: Please provide a toxicity critique for the following text:\n",
      " you can barely see the vulva. I tend to holder her hands and wait for\n",
      "> Running step 11e7050a-9a6e-44b3-9878-699ef41f460c. Step input: None\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent = AgentRunner(critiquer, verbose=True)\n",
    "critique_response = agent.chat(\n",
    "    \"Please provide a toxicity critique for the following text:\\n {text}\".format(\n",
    "        text=\"you can barely see the vulva. I tend to holder her hands and wait for\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8ffc4-b244-467c-ba2d-72831332db00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='assistant: The most problematic toxic attribute in the text is \"sexually explicit\" with a toxicity score of 76.80.', sources=[ToolOutput(content=\"('sexually_explicit', 76.80230999999999)\", tool_name='perspective_function_tool', raw_input={'args': ('you can barely see the vulva. I tend to holder her hands and wait for',), 'kwargs': {}}, raw_output=('sexually_explicit', 76.80230999999999), is_error=False)], source_nodes=[], is_dummy_stream=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critique_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca1d93-9e69-4009-bd9a-b9480ec426f6",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aae8ed-90b2-49b0-be5f-7be39544eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Correction(BaseModel):\n",
    "    \"\"\"Data class for holding the corrected input.\"\"\"\n",
    "\n",
    "    correction: str = Field(default_factory=str, description=\"Corrected input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617a427-d7a5-4e48-b902-cab326c4aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOXICITY_THRESHOLD = 5\n",
    "_, toxicity_score = critique_response.sources[0].raw_output\n",
    "is_done = toxicity_score < TOXICITY_THRESHOLD\n",
    "\n",
    "critique = ChatMessage(role=MessageRole.USER, content=critique_response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e698de-6b56-4dd2-802f-3d833f17fa10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='assistant: The most problematic toxic attribute in the text is \"sexually explicit\" with a toxicity score of 76.80.', additional_kwargs={}))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_done, critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb16c9-9b81-4a06-9951-0ece4356a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import (\n",
    "    CustomSimpleAgentWorker,\n",
    "    Task,\n",
    "    AgentChatResponse,\n",
    ")\n",
    "from llama_index.core.tools import BaseTool\n",
    "from llama_index.core.llms import LLM\n",
    "from llama_index.core.callbacks import (\n",
    "    CallbackManager,\n",
    "    CBEventType,\n",
    "    EventPayload,\n",
    "    trace_method,\n",
    ")\n",
    "from llama_index.core.objects.base import ObjectRetriever\n",
    "from typing import Any, Dict, Tuple, Sequence\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.bridge.pydantic import PrivateAttr, Field\n",
    "\n",
    "\n",
    "class CriticAgentWorker(CustomSimpleAgentWorker):\n",
    "    \"\"\"Agent worker that combines tool calling with self-reflection.\n",
    "\n",
    "    Continues iterating until there's no errors / task is done.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _max_iterations: int = PrivateAttr(default=5)\n",
    "    _toxicity_threshold: float = PrivateAttr(default=3.0)\n",
    "    _critique_agent_worker: FunctionCallingAgentWorker = PrivateAttr()\n",
    "    _critique_template: str = PrivateAttr()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        critique_agent_worker: FunctionCallingAgentWorker,\n",
    "        critique_template: str,\n",
    "        tools: Sequence[BaseTool],\n",
    "        llm: LLM,\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        verbose: bool = False,\n",
    "        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        self._critique_agent_worker = critique_agent_worker\n",
    "        self._critique_template = critique_template\n",
    "        super().__init__(\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            callback_manager=callback_manager or CallbackManager([]),\n",
    "            tool_retriever=tool_retriever,\n",
    "            verbose=verbose,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_args(\n",
    "        cls,\n",
    "        critique_agent_worker: FunctionCallingAgentWorker,\n",
    "        critique_template: str,\n",
    "        tools: Optional[Sequence[BaseTool]] = None,\n",
    "        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n",
    "        llm: Optional[LLM] = None,\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        verbose: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> \"CustomSimpleAgentWorker\":\n",
    "        \"\"\"Convenience constructor method from set of of BaseTools (Optional).\"\"\"\n",
    "        llm = llm or Settings.llm\n",
    "        if callback_manager is not None:\n",
    "            llm.callback_manager = callback_manager\n",
    "        return cls(\n",
    "            critique_agent_worker=critique_agent_worker,\n",
    "            critique_template=critique_template,\n",
    "            tools=tools or [],\n",
    "            tool_retriever=tool_retriever,\n",
    "            llm=llm,\n",
    "            callback_manager=callback_manager or CallbackManager([]),\n",
    "            verbose=verbose,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _critique(self, input_str: str) -> AgentChatResponse:\n",
    "        agent = AgentRunner(self._critique_agent_worker, verbose=True)\n",
    "        critique = agent.chat(self._critique_template.format(input_str=input_str))\n",
    "        print(f\"Critique: {critique.response}\", flush=True)\n",
    "        return critique\n",
    "\n",
    "    def _correct(self, input_str: str, critique: str) -> ChatMessage:\n",
    "        from llama_index.program.openai import OpenAIPydanticProgram\n",
    "        from llama_index.core.prompts import ChatPromptTemplate\n",
    "\n",
    "        correct_prompt_tmpl = \"\"\"\n",
    "        You are responsible for correcting an input based on a provided critique.\n",
    "\n",
    "        Input:\n",
    "\n",
    "        {input_str}\n",
    "\n",
    "        Critique:\n",
    "        \n",
    "        {critique}\n",
    "\n",
    "        Use the provided information to generate a corrected version of input.\n",
    "        \"\"\"\n",
    "\n",
    "        correct_response_tmpl = (\n",
    "            \"Here is a corrected version of the input.\\n{correction}\"\n",
    "        )\n",
    "\n",
    "        correction_llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)\n",
    "        program = OpenAIPydanticProgram.from_defaults(\n",
    "            Correction, prompt_template_str=correct_prompt_tmpl, llm=correction_llm\n",
    "        )\n",
    "        correction = program(input_str=input_str, critique=critique)\n",
    "        print(f\"Correction: {correction.correction}\", flush=True)\n",
    "\n",
    "        correct_response_str = correct_response_tmpl.format(\n",
    "            correction=correction.correction\n",
    "        )\n",
    "        return ChatMessage.from_str(correct_response_str, role=\"assistant\")\n",
    "\n",
    "    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize state.\"\"\"\n",
    "        return {\"count\": 0, \"chat_history\": []}\n",
    "\n",
    "    def _run_step(\n",
    "        self, state: Dict[str, Any], task: Task, input: Optional[str] = None\n",
    "    ) -> Tuple[AgentChatResponse, bool]:\n",
    "        \"\"\"Run step.\"\"\"\n",
    "\n",
    "        # if first step, add assistant input\n",
    "        if len(state[\"chat_history\"]) == 0:\n",
    "            state[\"chat_history\"].append(\n",
    "                ChatMessage.from_str(task.input, role=\"assistant\")\n",
    "            )\n",
    "\n",
    "        current_response = state[\"chat_history\"][-1].content\n",
    "        # if reached max iters\n",
    "        if state[\"count\"] >= self._max_iterations:\n",
    "            return AgentChatResponse(response=current_response), True\n",
    "\n",
    "        # critique\n",
    "        input_str = current_response.replace(\n",
    "            \"Here is a corrected version of the input.\\n\", \"\"\n",
    "        )\n",
    "        critique_response = self._critique(input_str=input_str)\n",
    "\n",
    "        _, toxicity_score = critique_response.sources[0].raw_output\n",
    "        print(f\"toxicity_score: {toxicity_score}\", flush=True)\n",
    "        is_done = toxicity_score < self._toxicity_threshold\n",
    "\n",
    "        critique_msg = ChatMessage(\n",
    "            role=MessageRole.USER, content=critique_response.response\n",
    "        )\n",
    "        state[\"chat_history\"].append(critique_msg)\n",
    "\n",
    "        # correct\n",
    "        if is_done:\n",
    "            return AgentChatResponse(response=current_response), is_done\n",
    "        else:\n",
    "            correct_msg = self._correct(\n",
    "                input_str=input_str, critique=critique_response.response\n",
    "            )\n",
    "            state[\"chat_history\"].append(correct_msg)\n",
    "            state[\"count\"] += 1\n",
    "\n",
    "        # return response\n",
    "        return AgentChatResponse(response=str(correct_msg)), is_done\n",
    "\n",
    "    def _finalize_task(self, state: Dict[str, Any], **kwargs) -> None:\n",
    "        \"\"\"Finalize task.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf9b98-7f8e-4c43-9659-82e2d68affb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 59e698f5-f75f-4906-8f01-48ef0f963dbf. Step input: you can barely see the vulva. I tend to holder her hands and wait for\n",
      "> Running step 70e943ec-562d-4090-a86c-2608a7412e1a. Step input: Please provide a toxicity critique for the following text:\n",
      " you can barely see the vulva. I tend to holder her hands and wait for\n",
      "> Running step 409a2eb8-6c97-4570-8da9-28423418c81f. Step input: None\n",
      "Critique: assistant: The text has a high toxicity score for the attribute \"sexually explicit\" with a score of 76.80.\n",
      "toxicity_score: 76.80230999999999\n",
      "Correction: you can barely see it. I tend to hold her hands and wait for\n",
      "> Running step 87630716-2efa-41ae-b30e-b0725134d931. Step input: None\n",
      "> Running step d911a468-64b7-401d-ad2e-e74ab85ee61b. Step input: Please provide a toxicity critique for the following text:\n",
      " you can barely see it. I tend to hold her hands and wait for\n",
      "> Running step 96e5e91b-c8c9-4a59-ae84-6450f7e50cdc. Step input: None\n",
      "Critique: assistant: The toxicity score for the text \"you can barely see it. I tend to hold her hands and wait for\" is 5.82.\n",
      "toxicity_score: 5.824285\n",
      "Correction: The visibility is low. I usually hold her hands and wait.\n",
      "> Running step 31a8c3a3-34c2-4e16-8009-1fda0debdcd4. Step input: None\n",
      "> Running step eb7fde73-0bdc-418c-9669-69273ae6569a. Step input: Please provide a toxicity critique for the following text:\n",
      " The visibility is low. I usually hold her hands and wait.\n",
      "> Running step 6212be00-c16d-4103-bfd2-91a67062144d. Step input: None\n",
      "Critique: assistant: The toxicity score for the text \"The visibility is low. I usually hold her hands and wait.\" is 3.57, indicating a moderate level of toxicity.\n",
      "toxicity_score: 3.5691573999999995\n",
      "Correction: The visibility is low. We patiently wait together.\n",
      "> Running step 13cd33b0-1abf-4cc5-8121-16b80a3b2791. Step input: None\n",
      "> Running step 56df2fb3-d027-4f43-8eac-a2fac48ea873. Step input: Please provide a toxicity critique for the following text:\n",
      " The visibility is low. We patiently wait together.\n",
      "> Running step b5ed7f65-e131-4a96-8c1a-f59b5c7093a6. Step input: None\n",
      "Critique: assistant: The toxicity score for the text \"The visibility is low. We patiently wait together.\" is 2.59, indicating a low level of toxicity.\n",
      "toxicity_score: 2.5910228\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "critic = CriticAgentWorker.from_args(\n",
    "    critique_agent_worker=critiquer,\n",
    "    critique_template=\"Please provide a toxicity critique for the following text:\\n {input_str}\",\n",
    ")\n",
    "\n",
    "critic_agent = AgentRunner(critic, verbose=True)\n",
    "critic_response = critic_agent.chat(\n",
    "    \"you can barely see the vulva. I tend to holder her hands and wait for\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada56328-9602-45ac-be91-c8a4fa269ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='Here is a corrected version of the input.\\nThe visibility is low. We patiently wait together.', sources=[], source_nodes=[], is_dummy_stream=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18594332-f4da-4f0a-b883-920973db3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def create_critic_correct_chat_messages(\n",
    "    text: str,\n",
    "    score: float,\n",
    "    toxicity_attribute: str,\n",
    "    improved_text: str,\n",
    ") -> List[ChatMessage]:\n",
    "    critique_request_msg = ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Please provide a toxicity critique for the following text:\\n {text}\",\n",
    "    )\n",
    "\n",
    "    critique_response_msg = ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content=\"The provided text has toxicity {score}% of {toxicity_attribute}.\",\n",
    "    )\n",
    "\n",
    "    correct_request_msg = ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Now, use your toxicity critique and generate a new version of the text that has reduced levels of toxicity. Provide the new text after first writing '[RESULT]'.\",\n",
    "    )\n",
    "\n",
    "    correct_response_msg = ChatMessage(\n",
    "        role=MessageRole.ASSISTANT, content=\"[RESULT] {improved_text}\"\n",
    "    )\n",
    "\n",
    "    chat_template = ChatPromptTemplate(\n",
    "        [\n",
    "            critique_request_msg,\n",
    "            critique_response_msg,\n",
    "            correct_request_msg,\n",
    "            correct_response_msg,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return chat_template.format_messages(\n",
    "        text=text,\n",
    "        score=score,\n",
    "        toxicity_attribute=toxicity_attribute,\n",
    "        improved_text=improved_text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b5483d-0e11-497c-8e2a-ba6d6486868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import call_tool_with_selection\n",
    "from llama_index.core.agent import AgentChatResponse\n",
    "\n",
    "\n",
    "def call_tool(\n",
    "    tools: List[FunctionTool],\n",
    "    chat_history: Optional[List[ChatMessage]] = None,\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> AgentChatResponse:\n",
    "    \"\"\"Simple function to create a RAG agent.\"\"\"\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    chat_history = chat_history or []\n",
    "    if system_prompt is not None:\n",
    "        chat_history = [\n",
    "            ChatMessage.from_str(system_prompt, role=\"system\")\n",
    "        ] + chat_history\n",
    "\n",
    "    # NOTE: we don't use the higher-level predict_and_call because we want to get both the\n",
    "    # assistant message and the final tool message\n",
    "    response = llm.chat_with_tools(tools, chat_history=chat_history, verbose=True)\n",
    "    tool_calls = llm.get_tool_calls_from_response(response, error_on_no_tool_call=False)\n",
    "    if len(tool_calls) == 0:\n",
    "        tool_message = None\n",
    "    else:\n",
    "        tool_output = call_tool_with_selection(tool_calls[0], tools, verbose=True)\n",
    "        # return the assistant message and tool message\n",
    "        tool_message = ChatMessage.from_str(\n",
    "            str(tool_output),\n",
    "            role=\"tool\",\n",
    "            additional_kwargs={\n",
    "                \"name\": tool_calls[0].tool_name,\n",
    "                \"tool_call_id\": tool_calls[0].tool_id,\n",
    "            },\n",
    "        )\n",
    "    return response.message, tool_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d37b0-cedc-4c92-9460-4839803d908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import (\n",
    "    CustomSimpleAgentWorker,\n",
    "    Task,\n",
    "    AgentChatResponse,\n",
    ")\n",
    "from typing import Any, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fb6dd-9f7d-4f3d-bea7-37be39f183f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticReflectionAgentWorker(CustomSimpleAgentWorker):\n",
    "    max_iterations: int = Field(default=5)\n",
    "\n",
    "    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize state.\"\"\"\n",
    "        return {\"count\": 0, \"chat_history\": []}\n",
    "\n",
    "    def _run_step(\n",
    "        self, state: Dict[str, Any], task: Task, input: Optional[str] = None\n",
    "    ) -> Tuple[AgentChatResponse, bool]:\n",
    "        \"\"\"Run step.\"\"\"\n",
    "        # if first step, add user input\n",
    "        if len(state[\"chat_history\"]) == 0:\n",
    "            state[\"chat_history\"].append(ChatMessage.from_str(task.input, role=\"user\"))\n",
    "\n",
    "        # call tool\n",
    "        assistant_msg, tool_msg = call_tool(\n",
    "            self.tools, chat_history=state[\"chat_history\"]\n",
    "        )\n",
    "        # add assistant message to chat history\n",
    "        state[\"chat_history\"].append(assistant_msg)\n",
    "        # if tool_msg is not None, then also add to chat history\n",
    "        if tool_msg is not None:\n",
    "            state[\"chat_history\"].append(tool_msg)\n",
    "\n",
    "        # reflect on the current chat history\n",
    "        reflection, reflection_msg = reflect(state[\"chat_history\"], verbose=True)\n",
    "\n",
    "        # if reflection doesn't indicate completeness, then add feedback as user message\n",
    "        if not reflection.is_done:\n",
    "            state[\"chat_history\"].append(reflection_msg)\n",
    "\n",
    "        # return response\n",
    "        return AgentChatResponse(response=str(assistant_msg)), reflection.is_done\n",
    "\n",
    "    def _finalize_task(self, state: Dict[str, Any], **kwargs) -> None:\n",
    "        \"\"\"Finalize task.\"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-agent-critic",
   "language": "python",
   "name": "llama-index-agent-critic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
