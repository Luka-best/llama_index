{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Initiate llama-index OCI client"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7356951c0a79f014"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from llama_index.llms.oci_genai import OCIGenAI\n",
    "\n",
    "llm = OCIGenAI(\n",
    "    model=\"meta.llama-3.1-70b-instruct\", # Switch to different models for testing\n",
    "    service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\",\n",
    "    compartment_id=\"ocid1.tenancy.oc1..aaaaaaaaumuuscymm6yb3wsbaicfx3mjhesghplvrvamvbypyehh5pgaasna\",\n",
    "    auth_type=\"SECURITY_TOKEN\",\n",
    "    auth_profile=\"BoatOc1\", # Please update here with your own profile name\n",
    "    additional_kwargs={\"max_tokens\": 1000}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:05:15.206733Z",
     "start_time": "2024-11-04T10:05:14.184963Z"
    }
   },
   "id": "29f1ea43785173a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple Chat"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7df8b4644fcb1eb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Arrrr, settle yerself down with a pint o' grog and listen close, me hearty! I've got a tale for ye that'll make yer timbers shiver.\n",
      "\n",
      "It were a dark and stormy night, and me and me crew, the \"Maverick's Revenge,\" were sailin' through treacherous waters. The winds were howlin' like a pack o' wolves, and the lightnin' were flashin' like a thousand torches in the sky. Me and me mate, Barnaby Blackheart, were at the helm, tryin' to keep the ship on course.\n",
      "\n",
      "Suddenly, a mighty wave crashed over the bow, and we were swept off our feet. I found meself clingin' to the riggin' for dear life, me boots fillin' up with seawater. Barnaby were clingin' to me leg, his eyes wide with fear.\n",
      "\n",
      "\"Cap'n, we're goin' down!\" he shouted above the din o' the storm.\n",
      "\n",
      "But I weren't havin' it. I've been sailin' these seas for nigh on 20 years, and I knew me ship like the back o' me hand. I gave a mighty roar and shouted, \"Not on me watch, matey!\"\n",
      "\n",
      "I grabbed hold o' the wheel and started to turn it, fightin' against the wind and the waves. The ship creaked and groaned, but I knew she were a sturdy galleon, built for battles like this.\n",
      "\n",
      "Slowly but surely, we started to make headway, the ship risin' up out o' the trough and into the sunlight. The storm were still ragin' on, but we were sailin' through it like a hot knife through butter.\n",
      "\n",
      "As we emerged on the other side, the sun broke through the clouds, and we saw it: a great, golden island risin' up out o' the sea. The sand were white, the palm trees were swayin' in the breeze, and a great, glitterin' waterfall cascaded down the side o' a mountain.\n",
      "\n",
      "\"Shiver me timbers!\" I exclaimed. \"That be the fabled Isla del Cielo, the Island o' Heaven!\"\n",
      "\n",
      "Barnaby's eyes went wide. \"Cap'n, I've heard tales o' that place. They say it be cursed, that anyone who sets foot on its shores will be doomed to sail the seas forever.\"\n",
      "\n",
      "But I weren't havin' it. I've always been a man o' adventure, and the thought o' findin' treasure on a cursed island were too great to resist. I gave the order to drop anchor and take the longboats ashore.\n",
      "\n",
      "And that, me hearty, be where the real adventure began...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a story\"),\n",
    "]\n",
    "response = llm.chat(messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:05:36.066559Z",
     "start_time": "2024-11-04T10:05:17.023760Z"
    }
   },
   "id": "bb2ab1f196c87f21"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 200, 'headers': {'content-type': 'application/json', 'opc-request-id': 'C9E9A28AD1F1481C9D8332D47ED271F5/4287D9DCD289D6FDB46A9095BC7210FA/B45847257D96B1D26B11E802A2AB6F53', 'content-encoding': 'gzip', 'content-length': '1730'}, 'data': {\n",
      "  \"chat_response\": {\n",
      "    \"api_format\": \"GENERIC\",\n",
      "    \"choices\": [\n",
      "      {\n",
      "        \"finish_reason\": \"stop\",\n",
      "        \"index\": 0,\n",
      "        \"logprobs\": {\n",
      "          \"text_offset\": null,\n",
      "          \"token_logprobs\": null,\n",
      "          \"tokens\": null,\n",
      "          \"top_logprobs\": null\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"text\": \"Arrrr, settle yerself down with a pint o' grog and listen close, me hearty! I've got a tale for ye that'll make yer timbers shiver.\\n\\nIt were a dark and stormy night, and me and me crew, the \\\"Maverick's Revenge,\\\" were sailin' through treacherous waters. The winds were howlin' like a pack o' wolves, and the lightnin' were flashin' like a thousand tiny lanterns in the sky. Me and me mate, Barnaby Blackheart, were at the helm, tryin' to keep the ship on course.\\n\\nSuddenly, a mighty gust o' wind hit us, and the ship were tossed about like a toy. The crew were scramblin' to keep their footing, and I were clingin' to the wheel for dear life. That's when I saw it: a ghost ship, sailin' straight for us!\\n\\nNow, I know what ye be thinkin': \\\"Ghost ships? Arrr, that be just a myth!\\\" But I'm here to tell ye, matey, I saw it with me own two eyes. It were a ship like no other, with sails as white as snow and a hull that seemed to glow with an otherworldly light.\\n\\nAs it drew closer, I could see the crew o' the ghost ship, their eyes glowin' like lanterns in the dark. They were a motley crew, with scars and tattoos and a look o' pure malevolence in their eyes. I knew we were in for a fight.\\n\\nBut I weren't afraid, no sir! I be Captain Blackbeak Billy, the most feared pirate on the seven seas! I rallied me crew, and we prepared to defend ourselves against the ghostly invaders.\\n\\nThe battle were fierce, with cannons blazin' and swords clashin' in the darkness. But I had a trick up me sleeve, see. I had a magical compass, passed down from me great-granddaddy, the infamous pirate, Captain Redbeard. It were said to have the power to navigate through treacherous waters and avoid danger.\\n\\nI held the compass aloft, and it began to glow with a fierce blue light. The ghost ship were repelled by the light, and it began to back away. But I weren't havin' it! I gave the order to pursue, and we chased that ghost ship through the stormy night, cannons blazin'!\\n\\nWe finally caught up to it, and I boarded the ship with me trusty cutlass. The ghostly crew were waitin' for me, but I were ready. I fought me way through them, swingin' me sword and dodgin' their spectral attacks.\\n\\nFinally, I reached the captain's quarters, where I found the ghostly captain himself, a towering figure with eyes that glowed like embers. He were a fierce opponent, but I were determined to emerge victorious.\\n\\nThe battle were intense, with sparks flyin' and the sound o' clashin' steel echoin' through the ship. But in the end, it were just me and the ghost captain, facin' off in the darkness.\\n\\nI raised me sword, and the ghost captain raised his. We clashed, and the sound o' our blades meetin' were like thunder in the night. But I were the better swordsman, and I emerged victorious.\\n\\nThe ghost captain dissipated into thin air, and the ghost ship began to fade away. I were left standin' on the deck, me sword still shakin' with the force o' the battle.\\n\\nAs the storm passed, the sun began to rise, and I saw that we were sailin' through calm waters, the sea as smooth as glass. Me crew cheered, and we set sail for the next adventure, ready to face whatever dangers lay ahead.\\n\\nAnd that, me hearty, be the tale o' how I, Captain Blackbeak Billy, defeated the ghost ship and saved me crew from certain doom! Arrr!\",\n",
      "              \"type\": \"TEXT\"\n",
      "            }\n",
      "          ],\n",
      "          \"name\": null,\n",
      "          \"role\": \"ASSISTANT\"\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"time_created\": \"2024-11-01T01:20:56.126000+00:00\"\n",
      "  },\n",
      "  \"model_id\": \"meta.llama-3.1-70b-instruct\",\n",
      "  \"model_version\": \"1.0.0\"\n",
      "}, 'request': <oci.request.Request object at 0x1687b0d00>, 'next_page': None, 'request_id': 'C9E9A28AD1F1481C9D8332D47ED271F5/4287D9DCD289D6FDB46A9095BC7210FA/B45847257D96B1D26B11E802A2AB6F53'}\n"
     ]
    }
   ],
   "source": [
    "print(response.raw)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-01T01:21:38.444168Z",
     "start_time": "2024-11-01T01:21:38.436626Z"
    }
   },
   "id": "2e03c8934545e7f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chat Streaming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c080eb754c4867c8"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, settle yerself down with a pint o' grog and listen close, me hearty! I've got a tale for ye that'll make yer timbers shiver.\n",
      "\n",
      "It were a dark and stormy night, and me and me crew, the \"Maverick's Revenge,\" were sailin' through treacherous waters. We'd been at sea for weeks, searchin' for the treasure of the infamous Captain Blackbeak. Legend had it that Blackbeak had hidden his loot on a remote island, guarded by sea monsters and cursed spirits.\n",
      "\n",
      "As we navigated through the choppy waters, our lookout, Barnaby Blackheart, spied somethin' on the horizon. \"Land ho!\" he cried, pointin' to a tiny island risin' up out o' the sea like a giant's fist.\n",
      "\n",
      "We dropped anchor and set off in the longboats, ready to face whatever dangers lay ahead. The island were a right proper jungle, with vines as thick as me arm and trees that seemed to stretch up to the clouds. We hacked our way through the underbrush, our cutlasses slicin' through the air like hot knives through butter.\n",
      "\n",
      "As we reached the center o' the island, we stumbled upon a great stone temple, covered in carvings o' skulls and snakes. I could feel the weight o' history bearin' down on me, and I knew we were gettin' close to the treasure.\n",
      "\n",
      "But we weren't alone. A rival pirate ship, the \"Sea Dragon,\" had followed us to the island, and their scurvy dogs were hot on our heels. We fought 'em off with cannons blazin' and swords clashin', but they were a fierce crew, and we took some heavy losses.\n",
      "\n",
      "Just when it seemed like all were lost, I remembered a trick I'd learned from me old mate, Captain Cutlass. I grabbed a barrel o' gunpowder and lit the fuse, creatin' a diversion that sent the Sea Dragon's crew runnin' for cover.\n",
      "\n",
      "With the coast clear, we made our way into the temple, where we found a chest overflowin' with gold doubloons and glitterin' jewels. We loaded up our treasure and set sail for home, with the Sea Dragon's crew cursin' our names as we disappeared over the horizon.\n",
      "\n",
      "Now, I know what ye be thinkin', matey: that's a tale fit for a pirate legend. But I'll let ye in on a little secret: it's all true, every last bit o' it. And if ye don't believe me, just ask me parrot, Polly. She were there, and she'll tell ye the same story, word for word.\n",
      "\n",
      "So hoist the colors, me hearties, and raise a toast to the Maverick's Revenge! We may not be the most feared pirates on the seven seas, but we've got the treasure to prove we're the bravest! Arrr!"
     ]
    }
   ],
   "source": [
    "resp = llm.stream_chat(messages)\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:05:52.343165Z",
     "start_time": "2024-11-04T10:05:39.089082Z"
    }
   },
   "id": "ae769aecf046976a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chat Chaining (Chat Prompt with a Template)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5a434473ee1c511"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the dog go to the vet?\n",
      "\n",
      "Because he was feeling ruff!\n",
      "\n",
      "(Sorry, I know it's a paws-itively terrible pun, but I hope it made you howl with laughter!)\n"
     ]
    }
   ],
   "source": [
    "# Set up a chat prompt with a template\n",
    "prompt_text = \"Tell me a joke about {topic}\"\n",
    "formatted_prompt = prompt_text.format(topic=\"dogs\")\n",
    "\n",
    "# Send the prompt to OCIGenAI\n",
    "response = llm.chat([ChatMessage(role=\"user\", content=formatted_prompt)])\n",
    "print(response.message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:06:00.362432Z",
     "start_time": "2024-11-04T10:05:57.141837Z"
    }
   },
   "id": "c9c6522e7b03f161"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chat with Documents \n",
    "\n",
    "* Note: requires embed model set up for llama index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba78d178325ad6c8"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "# Define documents as instances of the Document class\n",
    "documents = [\n",
    "    Document(text=\"Oracle database services provide high-performance versions of Oracle Database, including Oracle Autonomous Database to simplify relational database environments.\"),\n",
    "    Document(text=\"Oracle Cloud Infrastructure (OCI) offers secure, scalable cloud services across compute, storage, and networking, supporting enterprise applications.\"),\n",
    "    Document(text=\"Oracle Autonomous Database is a self-driving, self-securing, and self-repairing database that automates database maintenance tasks.\")\n",
    "]\n",
    "\n",
    "# Create an index directly from the list of Document instances\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Define a custom prompt template for condensing follow-up questions\n",
    "custom_prompt = PromptTemplate(\n",
    "    \"\"\"\n",
    "    Given a conversation (between Human and Assistant) and a follow-up message from Human,\n",
    "    rewrite the message to be a standalone question that captures all relevant context\n",
    "    from the conversation.\n",
    "    <Chat History>\n",
    "    {chat_history}\n",
    "    <Follow-Up Message>\n",
    "    {question}\n",
    "    <Standalone Question>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Initialize chat history with prior messages\n",
    "chat_history = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=\"You are an AI assistant.\"),\n",
    "    ChatMessage(role=MessageRole.USER, content=\"Tell me something about Oracle.\"),\n",
    "    ChatMessage(role=MessageRole.ASSISTANT, content=\"Oracle is a leading provider of database solutions.\"),\n",
    "]\n",
    "\n",
    "# Create a query engine for document retrieval\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    condense_question_prompt=custom_prompt,\n",
    "    chat_history=chat_history,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "follow_up_question = \"What are its main products?\"\n",
    "\n",
    "response = chat_engine.chat(follow_up_question)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-01T15:16:47.955274Z",
     "start_time": "2024-11-01T15:16:47.952476Z"
    }
   },
   "id": "60834ebd6814307c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic tool calling in llamaindex \n",
    "\n",
    "only Cohere supports tool calling for now"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "682c1e174c6ca7a6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from llama_index.llms.oci_genai import OCIGenAI\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "llm = OCIGenAI(\n",
    "    model=\"cohere.command-r-plus\",\n",
    "    service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\",\n",
    "    compartment_id=\"ocid1.tenancy.oc1..aaaaaaaaumuuscymm6yb3wsbaicfx3mjhesghplvrvamvbypyehh5pgaasna\",  \n",
    "    auth_type=\"SECURITY_TOKEN\",\n",
    "    auth_profile=\"BoatOc1\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:06:12.155690Z",
     "start_time": "2024-11-04T10:06:12.078304Z"
    }
   },
   "id": "4ac5510e464e4e80"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: I will use the multiply and add tools to answer the question.\n"
     ]
    }
   ],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Addition function on two integers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "\n",
    "response = llm.chat_with_tools(\n",
    "    tools=[add_tool, multiply_tool],\n",
    "    user_msg=\"What is 3 * 12? Also, what is 11 + 49?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:06:16.813438Z",
     "start_time": "2024-11-04T10:06:13.030388Z"
    }
   },
   "id": "794e947c64d2ea6c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='I will use the multiply and add tools to answer the question.', additional_kwargs={'finish_reason': 'COMPLETE', 'documents': None, 'citations': None, 'search_queries': None, 'is_search_required': None, 'tool_calls': [{'toolUseId': 'a387ebd8eb1346c296e0732d611ff981', 'name': 'multiply', 'input': '{\"a\": 3, \"b\": 12}'}, {'toolUseId': 'a0d0589161ae4b02a7b3028f4c23c60c', 'name': 'add', 'input': '{\"a\": 11, \"b\": 49}'}]}), 'raw': {'status': 200, 'headers': {'content-type': 'application/json', 'opc-request-id': '4C8C0553B2B44ED18FE7CF48021654F5/088D8729A525795E77C1E6A2A3D9AEB3/CBBD3C8D8C90E13D6EED0E1A9144B83C', 'content-encoding': 'gzip', 'content-length': '304'}, 'data': {\n",
      "  \"chat_response\": {\n",
      "    \"api_format\": \"COHERE\",\n",
      "    \"chat_history\": [\n",
      "      {\n",
      "        \"message\": \"What is 3 * 12? Also, what is 11 + 49?\",\n",
      "        \"role\": \"USER\"\n",
      "      },\n",
      "      {\n",
      "        \"message\": \"I will use the multiply and add tools to answer the question.\",\n",
      "        \"role\": \"CHATBOT\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"name\": \"multiply\",\n",
      "            \"parameters\": {\n",
      "              \"a\": 3,\n",
      "              \"b\": 12\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"add\",\n",
      "            \"parameters\": {\n",
      "              \"a\": 11,\n",
      "              \"b\": 49\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"citations\": null,\n",
      "    \"documents\": null,\n",
      "    \"error_message\": null,\n",
      "    \"finish_reason\": \"COMPLETE\",\n",
      "    \"is_search_required\": null,\n",
      "    \"prompt\": null,\n",
      "    \"search_queries\": null,\n",
      "    \"text\": \"I will use the multiply and add tools to answer the question.\",\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"name\": \"multiply\",\n",
      "        \"parameters\": {\n",
      "          \"a\": 3,\n",
      "          \"b\": 12\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"add\",\n",
      "        \"parameters\": {\n",
      "          \"a\": 11,\n",
      "          \"b\": 49\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"model_id\": \"cohere.command-r-plus\",\n",
      "  \"model_version\": \"1.2\"\n",
      "}, 'request': <oci.request.Request object at 0x137c6a970>, 'next_page': None, 'request_id': '4C8C0553B2B44ED18FE7CF48021654F5/088D8729A525795E77C1E6A2A3D9AEB3/CBBD3C8D8C90E13D6EED0E1A9144B83C'}, 'delta': None, 'logprobs': None, 'additional_kwargs': {'model_id': 'cohere.command-r-plus', 'model_version': '1.2', 'request_id': '4C8C0553B2B44ED18FE7CF48021654F5/088D8729A525795E77C1E6A2A3D9AEB3/CBBD3C8D8C90E13D6EED0E1A9144B83C', 'content-length': '304'}}\n"
     ]
    }
   ],
   "source": [
    "print(dict(response))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:06:18.022657Z",
     "start_time": "2024-11-04T10:06:18.019444Z"
    }
   },
   "id": "e268eea0e6ef3326"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'toolUseId': 'a387ebd8eb1346c296e0732d611ff981', 'name': 'multiply', 'input': '{\"a\": 3, \"b\": 12}'}, {'toolUseId': 'a0d0589161ae4b02a7b3028f4c23c60c', 'name': 'add', 'input': '{\"a\": 11, \"b\": 49}'}]\n"
     ]
    }
   ],
   "source": [
    "tool_calls = response.message.additional_kwargs.get('tool_calls', [])\n",
    "print(tool_calls)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:06:19.470806Z",
     "start_time": "2024-11-04T10:06:19.464217Z"
    }
   },
   "id": "86b8dc6006f2218c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chat streaming with tools"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "125cba17b0d7ff32"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: I\n",
      "Content: I will\n",
      "Content: I will use\n",
      "Content: I will use the\n",
      "Content: I will use the multiplication\n",
      "Content: I will use the multiplication tool\n",
      "Content: I will use the multiplication tool to\n",
      "Content: I will use the multiplication tool to find\n",
      "Content: I will use the multiplication tool to find the\n",
      "Content: I will use the multiplication tool to find the answer\n",
      "Content: I will use the multiplication tool to find the answer.\n",
      "Content: I will use the multiplication tool to find the answer.I will use the multiplication tool to find the answer.\n",
      "Tool Calls: [{'toolUseId': 'a2ef38fed43a4813979b0d3f136e5d86', 'name': 'multiply', 'input': '{\"a\": 23, \"b\": 45}'}]\n",
      "Content: I will use the multiplication tool to find the answer.I will use the multiplication tool to find the answer.\n",
      "Tool Calls: [{'toolUseId': '7e51be5c36d245389f54b50b21515dc8', 'name': 'multiply', 'input': '{\"a\": 23, \"b\": 45}'}]\n"
     ]
    }
   ],
   "source": [
    "# Use streaming with tools\n",
    "gen = llm.stream_chat_with_tools(\n",
    "    tools=[multiply_tool, add_tool],\n",
    "    user_msg=\"What is 23 * 45?\",\n",
    ")\n",
    "\n",
    "# Process the stream\n",
    "try:\n",
    "    for response in gen:\n",
    "        # Print content as it arrives\n",
    "        if response.message.content:\n",
    "            print(f\"Content: {response.message.content}\")\n",
    "\n",
    "        # Check for tool calls\n",
    "        if \"tool_calls\" in response.message.additional_kwargs:\n",
    "            tool_calls = response.message.additional_kwargs[\"tool_calls\"]\n",
    "            print(f\"Tool Calls: {tool_calls}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in stream processing: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:06:27.689806Z",
     "start_time": "2024-11-04T10:06:22.937951Z"
    }
   },
   "id": "6b865f544a43b6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Invoking tools and passing tool outputs back to model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2daf408eebe8f866"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will use the multiply tool to calculate 3 * 12 and the add tool to calculate 11 + 49.\n",
      "assistant: 3 * 12 is **36** and 11 + 49 is **60**.\n"
     ]
    }
   ],
   "source": [
    "# Initial user query\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "user_message = ChatMessage(role=MessageRole.USER, content=query)\n",
    "\n",
    "# Pass initial message to the LLM with tool calling enabled\n",
    "response = llm.chat_with_tools(tools=[add_tool, multiply_tool], user_msg=user_message.content)\n",
    "print(response.message.content)  # Initial assistant response about using tools\n",
    "\n",
    "# Extract tool calls and invoke each tool based on the response\n",
    "messages = [user_message, response.message]  # Start with user message and initial assistant response\n",
    "\n",
    "for tool_call in response.message.additional_kwargs.get(\"tool_calls\", []):\n",
    "    # Select the tool based on the tool call name\n",
    "    selected_tool = {\"add\": add_tool, \"multiply\": multiply_tool}[tool_call[\"name\"].lower()]\n",
    "    tool_input = eval(tool_call[\"input\"])  # Convert input JSON string to dict\n",
    "    tool_output = selected_tool(**tool_input)  # Invoke tool\n",
    "\n",
    "    # Add the tool's output as a new message in the conversation\n",
    "    tool_output_message = ChatMessage(\n",
    "        role=MessageRole.TOOL,\n",
    "        content=str(tool_output),  # Format tool output as string\n",
    "        additional_kwargs={\"tool_call_id\": tool_call.get(\"toolUseId\")}\n",
    "    )\n",
    "    messages.append(tool_output_message)\n",
    "\n",
    "# Pass the messages back to the LLM for a final response using the tool outputs\n",
    "final_response = llm.chat(messages)\n",
    "print(final_response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:08:17.111634Z",
     "start_time": "2024-11-04T10:08:10.727457Z"
    }
   },
   "id": "9e9c1d54094babdc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Few-shot tool prompting & basic agent-like tool looping"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cec514e673f6897a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will use the multiply tool to multiply 119 and 7, then use the add tool to add 30 to the result.\n",
      "119 x 7 = 833. Now I will add 30 to this.\n",
      "The answer is: 863\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Define initial few-shot messages\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"What's the product of 317253 and 128472 plus four\"\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content=\"\",\n",
    "        additional_kwargs={\n",
    "            \"tool_calls\": [{\"name\": \"multiply\", \"input\": {\"x\": 317253, \"y\": 128472}, \"toolUseId\": \"1\"}]\n",
    "        }\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.TOOL,\n",
    "        content=\"16505054784\",\n",
    "        additional_kwargs={\"tool_call_id\": \"1\"}\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content=\"\",\n",
    "        additional_kwargs={\n",
    "            \"tool_calls\": [{\"name\": \"add\", \"input\": {\"x\": 16505054784, \"y\": 4}, \"toolUseId\": \"2\"}]\n",
    "        }\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.TOOL,\n",
    "        content=\"16505054788\",\n",
    "        additional_kwargs={\"tool_call_id\": \"2\"}\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content=\"The product of 317253 and 128472 plus four is 16505054788\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Define the query\n",
    "query = \"Whats 119 times 7 plus 30. Don't do any math yourself, only use tools for math. Respect order of operations. when you have the answer output 'the answer is: <answer>'\"\n",
    "messages.append(ChatMessage(role=MessageRole.USER, content=query))\n",
    "\n",
    "# Start agent-like tool looping\n",
    "while True:\n",
    "    ai_response = llm.chat_with_tools(messages=messages, tools=[multiply_tool, add_tool])\n",
    "    print(ai_response.message.content)\n",
    "\n",
    "    if \"the answer is:\" in ai_response.message.content.lower():\n",
    "        break\n",
    "\n",
    "    messages.append(ai_response.message)\n",
    "\n",
    "    # Execute tools based on tool calls and append tool responses\n",
    "    if ai_response.message.additional_kwargs.get(\"tool_calls\"):\n",
    "        for tool_call in ai_response.message.additional_kwargs[\"tool_calls\"]:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_input = tool_call[\"input\"]\n",
    "\n",
    "            # If tool_input is a string, parse it as JSON\n",
    "            if isinstance(tool_input, str):\n",
    "                tool_input = json.loads(tool_input)\n",
    "\n",
    "            # Select tool and invoke it\n",
    "            if tool_name == \"multiply\":\n",
    "                result = multiply(**tool_input)\n",
    "            elif tool_name == \"add\":\n",
    "                result = add(**tool_input)\n",
    "\n",
    "            # Append the tool result as a tool message\n",
    "            messages.append(\n",
    "                ChatMessage(\n",
    "                    role=MessageRole.TOOL,\n",
    "                    content=str(result),\n",
    "                    additional_kwargs={\"tool_call_id\": tool_call[\"toolUseId\"]}\n",
    "                )\n",
    "            )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:08:26.172868Z",
     "start_time": "2024-11-04T10:08:19.223098Z"
    }
   },
   "id": "4b9a59c4a783c025"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a llama-index agent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c245da335beb23f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 69ad5aaa-c63d-4d2d-a06d-ed4df7907ebb. Step input: Whats 119 times 7 plus 30. Don't do any math yourself, only use tools for math. Respect order of operations.\n",
      "\u001B[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: multiply\n",
      "Action Input: {'a': 119, 'b': 7}\n",
      "\u001B[0m\u001B[1;3;34mObservation: 833\n",
      "\u001B[0m> Running step 58e39756-5bba-4bf5-9734-02cc9b342288. Step input: None\n",
      "\u001B[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The answer is **863**.\n",
      "\u001B[0mThe answer is **863**.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Set the LLM in the global settings\n",
    "Settings.llm = llm\n",
    "\n",
    "# Define your tools\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and return the result.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and return the result.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Create FunctionTool instances\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "tools = [multiply_tool, add_tool]\n",
    "\n",
    "# Create the ReActAgent\n",
    "agent = ReActAgent.from_tools(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Start the conversation\n",
    "response = agent.chat(\n",
    "    \"Whats 119 times 7 plus 30. Don't do any math yourself, only use tools for math. Respect order of operations.\"\n",
    ")\n",
    "print(response)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-04T10:08:35.025758Z",
     "start_time": "2024-11-04T10:08:27.957147Z"
    }
   },
   "id": "6b4e7d9c12f15c00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "967734e1f7aec641"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
