# Evaluating With `LlamaDatasets`

We have already gone through the core abstractions within the Evaluation module that
enable various kinds of evaluation methodologies of LLM-based applications or systems, including RAG systems. Of course, to evaluate the system one needs an
evaluation method, the system itself, as well as evaluation datasets. It is
considered best practice to test the LLM application on several distinct datasets
emanating from different sources and domains. Doing so, helps to ensure the overall
robustness (that is, the level in which the system will work in unseen, new cases) of
the system.

To this end, we've included the `BaseLlamaDataset` (referred to as `LlamaDataset`
for short) abstraction in our library. Their core purpose is to facilitate the
evaluations of systems on various datasets, by making these easy to create, easy
to use, and widely available.

The rest of this page focuses on a particular kind of `BaseLlamaDataset`, namely
the `LabelledRagDataset`. This dataset consists of examples, where an example
carries a `query`, a `reference_answer`, as well as `reference_contexts`. The main
reason for using a `LabelledRagDataset` is then to test a RAG system's performance
by first predicting a response to the given `query` and comparing that predicted
(or generated) response to the `reference_answer`.

```python
from llama_index.llama_dataset import (
    LabelledRagDataset,
    CreatedBy,
    CreatedByType,
    LabelledRagDataExample,
)

example1 = LabelledRagDataExample(
    query="This is some user query.",
    query_by=CreatedBy(type=CreateByType.HUMAN),
    reference_answer="This is a reference answer. Otherwise known as ground-truth answer.",
    reference_contexts=[
        "This is a list",
        "of contexts used to",
        "generate the reference_answer",
    ],
    reference_by=CreatedBy(type=CreateByType.HUMAN),
)

# a sad dataset consisting of one measely example
rag_dataset = LabelledRagDataset(examples=[example1])
```

## Building A `LabelledRagDataset`

As we just saw at the end of the previous section, we can build a `LabelledRagDataset`
manually by constructing `LabelledRagDataExample`'s one by one. However, this is
a bit tedious, and while human-annoted datasets are extremely valuable, datasets
that are generated by strong LLMs are also very useful.

As such, the `llama_dataset` module is equipped with the `RagDatasetGenerator` that
is able to generate a `LabelledRagDataset` over a set of source `Document`'s.

```python
from llama_index.llama_dataset.generator import RagDatasetGenerator
from llama_index import ServiceContext
from llama_index.llm import OpenAI
import nest_asyncio

nest_asyncio.apply()

documents = ...  # a set of documents loaded by using for example a Reader

service_context = ServiceContext.from_defaults(llm=OpenAI(model="gpt-4"))

dataset_generator = RagDatasetGenerator.from_documents(
    documents=documents,
    service_context=service_context,
    num_questions_per_chunk=10,  # set the number of questions per nodes
)

rag_dataset = dataset_generator.generate_dataset_from_nodes()
```

## Using A `LabelledRagDataset`

As mentioned before, we want to use a `LabelledRagDataset` to evaluate a RAG
system, built on the same source `Document`'s, performance with it. Doing so would
require performing two steps: (1) making predictions on the dataset (i.e. generating
responses to the query of each individual example), and (2) evaluating the predicted
response by comparing it to the reference answer. In step (2) we also evaluate the
RAG system's retrieved contexts and compare it to the reference contexts, to gain
an assessment on the retrieval component of the RAG system.

## Where To Find `LlamaDatasets`

## Notebooks

```{toctree}
---
maxdepth: 1
---

/examples/llama_dataset/labelled-rag-datasets.ipynb
/examples/llama_dataset/downloading_llama_datasets.ipynb
/examples/llama_dataset/uploading_llama_dataset.ipynb
```
