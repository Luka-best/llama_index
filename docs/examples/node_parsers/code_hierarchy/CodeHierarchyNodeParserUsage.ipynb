{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Hierarchy Node Parser\n",
    "\n",
    "The `CodeHierarchyNodeParser` is useful to split long code files into more reasonable chunks. What this will do is create a \"Hierarchy\" of sorts, where sections of the code are made more reasonable by replacing the scope body with short comments telling the LLM to search for a referenced node if it wants to read that context body. This is called skeletonization, and is toggled by setting `skeleton` to `True` which it is by default. Nodes in this hierarchy will be split based on scope, like function, class, or method scope, and will have links to their children and parents so the LLM can traverse the tree.\n",
    "\n",
    "This kind of node structure will work very well with indexes like the [Tree Index](https://gpt-index.readthedocs.io/en/stable/core_modules/data_modules/index/index_guide.html#tree-index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Import\n",
    "\n",
    "First be sure to install the necessary [tree-sitter](https://tree-sitter.github.io/tree-sitter/) libraries.\n",
    "\n",
    "`pip install tree-sitter tree-sitter-languages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser.code_hierarchy import CodeHierarchyNodeParser\n",
    "from llama_index.text_splitter.code_splitter import CodeSplitter\n",
    "from llama_index.readers import SimpleDirectoryReader\n",
    "from pathlib import Path\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, choose a directory you want to scan, and glob for all the code files you want to import.\n",
    "\n",
    "In this case I'm going to glob all \"*.py\" files in the `llama_index/node_parser` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(\n",
    "    input_files=Path(\"../../../../llama_index/node_parser\").glob(\"*.py\"),\n",
    "    file_metadata=lambda x: {\"filepath\": x}\n",
    ")\n",
    "nodes = reader.load_data()\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we got 8 files. Lets examine one of these nodes.\n",
    "We see here that the second one is 28756 characters long. That's way too long for most LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28958\n",
      "('from enum import Enum\\n'\n",
      " 'from typing import (Any, Callable, Dict, Iterable, List, Optional, '\n",
      " 'Sequence,\\n'\n",
      " '                    Set, Tuple)\\n'\n",
      " '\\n'\n",
      " 'try:\\n'\n",
      " '    from pydantic.v1 import BaseModel, Field\\n'\n",
      " 'except ImportError:\\n'\n",
      " '    from pydantic import BaseModel, Field\\n'\n",
      " '\\n'\n",
      " 'from tree_sitter import Node\\n'\n",
      " '\\n'\n",
      " 'from llama_index.callbacks.base import CallbackManager\\n'\n",
      " 'from llama_index.callbacks.schema import CBEventType, EventPayload\\n'\n",
      " 'from llama_index.node_parser.extractors.metadata_extractors import \\\\\\n'\n",
      " '    MetadataExtractor\\n'\n",
      " 'from llama_index.node_parser.interface import NodeParser\\n'\n",
      " 'from llama_index.node_parser.node_utils import get_nodes_from_node\\n'\n",
      " 'from llama_index.node_parser.simple import SimpleNodeParser\\n'\n",
      " 'from llama_index.schema import BaseNode, Document, NodeRelationship, '\n",
      " 'TextNode\\n'\n",
      " 'from llama_index.text_splitter.code_splitter import CodeSplitter\\n'\n",
      " 'from llama_index.utils import get_tqdm_iterable\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _SignatureCaptureType(BaseModel):\\n'\n",
      " '    \"\"\"\\n'\n",
      " '    Unfortunately some languages need special options for how to make a '\n",
      " 'signature.\\n'\n",
      " '\\n'\n",
      " '    For example, html element signatures should include their closing >, '\n",
      " 'there is no\\n'\n",
      " '    easy way to include this using an always-exclusive system.\\n'\n",
      " '\\n'\n",
      " \"    However, using an always-inclusive system, python decorators don't work, \"\n",
      " \"as there isn't\\n\"\n",
      " '    an easy to define terminator for decorators that is inclusive to their '\n",
      " 'signature.\\n'\n",
      " '    \"\"\"\\n'\n",
      " '\\n'\n",
      " '    type: str = Field(description=\"The type string to match on.\")\\n'\n",
      " '    inclusive: bool = Field(\\n'\n",
      " '        description=\"Whether to include the text of the node matched by this '\n",
      " 'type or not.\",\\n'\n",
      " '    )\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _SignatureCaptureOptions(BaseModel):\\n'\n",
      " '    start_signature_types: Optional[List[_SignatureCaptureType]] = Field(\\n'\n",
      " '        None,\\n'\n",
      " '        description=\"A list of node types any of which indicate the '\n",
      " 'beginning of the signature.\"\\n'\n",
      " '        \"If this is none or empty, use the start_byte of the node.\",\\n'\n",
      " '    )\\n'\n",
      " '    end_signature_types: Optional[List[_SignatureCaptureType]] = Field(\\n'\n",
      " '        None,\\n'\n",
      " '        description=\"A list of node types any of which indicate the end of '\n",
      " 'the signature.\"\\n'\n",
      " '        \"If this is none or empty, use the end_byte of the node.\",\\n'\n",
      " '    )\\n'\n",
      " '    name_identifier: str = Field(\\n'\n",
      " '        description=\"The node type to use for the signatures \\'name\\'.\"\\n'\n",
      " '        \"If retrieving the name is more complicated than a simple type '\n",
      " 'match, use a function which \"\\n'\n",
      " '        \"takes a node and returns true or false as to whether its the name '\n",
      " 'or not. \"\\n'\n",
      " '        \"The first match is returned.\"\\n'\n",
      " '    )\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\"\"\"\\n'\n",
      " 'Maps language -> Node Type -> SignatureCaptureOptions\\n'\n",
      " '\"\"\"\\n'\n",
      " '_DEFAULT_SIGNATURE_IDENTIFIERS: Dict[str, Dict[str, '\n",
      " '_SignatureCaptureOptions]] = {\\n'\n",
      " '    \"python\": {\\n'\n",
      " '        \"function_definition\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"block\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"class_definition\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"block\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '    },\\n'\n",
      " '    \"html\": {\\n'\n",
      " '        \"element\": _SignatureCaptureOptions(\\n'\n",
      " '            start_signature_types=[_SignatureCaptureType(type=\"<\", '\n",
      " 'inclusive=True)],\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\">\", '\n",
      " 'inclusive=True)],\\n'\n",
      " '            name_identifier=\"tag_name\",\\n'\n",
      " '        )\\n'\n",
      " '    },\\n'\n",
      " '    \"cpp\": {\\n'\n",
      " '        \"class_specifier\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"type_identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"function_definition\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"function_declarator\",\\n'\n",
      " '        ),\\n'\n",
      " '    },\\n'\n",
      " '    \"typescript\": {\\n'\n",
      " '        \"interface_declaration\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"type_identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"lexical_declaration\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"function_declaration\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"class_declaration\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"type_identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"method_definition\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"property_identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '    },\\n'\n",
      " '}\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _ScopeMethod(Enum):\\n'\n",
      " '    INDENTATION = \"INDENTATION\"\\n'\n",
      " '    BRACKETS = \"BRACKETS\"\\n'\n",
      " '    HTML_END_TAGS = \"HTML_END_TAGS\"\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _CommentOptions(BaseModel):\\n'\n",
      " '    comment_template: str\\n'\n",
      " '    scope_method: _ScopeMethod\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '_COMMENT_OPTIONS: Dict[str, _CommentOptions] = {\\n'\n",
      " '    \"cpp\": _CommentOptions(\\n'\n",
      " '        comment_template=\"// {}\", scope_method=_ScopeMethod.BRACKETS\\n'\n",
      " '    ),\\n'\n",
      " '    \"html\": _CommentOptions(\\n'\n",
      " '        comment_template=\"<!-- {} -->\", '\n",
      " 'scope_method=_ScopeMethod.HTML_END_TAGS\\n'\n",
      " '    ),\\n'\n",
      " '    \"python\": _CommentOptions(\\n'\n",
      " '        comment_template=\"# {}\", scope_method=_ScopeMethod.INDENTATION\\n'\n",
      " '    ),\\n'\n",
      " '    \"typescript\": _CommentOptions(\\n'\n",
      " '        comment_template=\"// {}\", scope_method=_ScopeMethod.BRACKETS\\n'\n",
      " '    ),\\n'\n",
      " '}\\n'\n",
      " '\\n'\n",
      " 'assert all(\\n'\n",
      " '    language in _DEFAULT_SIGNATURE_IDENTIFIERS for language in '\n",
      " '_COMMENT_OPTIONS\\n'\n",
      " '), \"Not all languages in _COMMENT_OPTIONS are in '\n",
      " '_DEFAULT_SIGNATURE_IDENTIFIERS\"\\n'\n",
      " 'assert all(\\n'\n",
      " '    language in _COMMENT_OPTIONS for language in '\n",
      " '_DEFAULT_SIGNATURE_IDENTIFIERS\\n'\n",
      " '), \"Not all languages in _DEFAULT_SIGNATURE_IDENTIFIERS are in '\n",
      " '_COMMENT_OPTIONS\"\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _ScopeItem(BaseModel):\\n'\n",
      " '    \"\"\"Like a Node from tree_sitter, but with only the str information we '\n",
      " 'need.\"\"\"\\n'\n",
      " '\\n'\n",
      " '    name: str\\n'\n",
      " '    type: str\\n'\n",
      " '    signature: str\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _ChunkNodeOutput(BaseModel):\\n'\n",
      " '    \"\"\"The output of a chunk_node call.\"\"\"\\n'\n",
      " '\\n'\n",
      " '    this_document: Optional[TextNode]\\n'\n",
      " '    upstream_children_documents: List[TextNode]\\n'\n",
      " '    all_documents: List[TextNode]\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class CodeHierarchyNodeParser(NodeParser):\\n'\n",
      " '    \"\"\"Split code using a AST parser.\\n'\n",
      " '\\n'\n",
      " '    Add metadata about the scope of the code block and relationships '\n",
      " 'between\\n'\n",
      " '    code blocks.\\n'\n",
      " '    \"\"\"\\n'\n",
      " '\\n'\n",
      " '    @classmethod\\n'\n",
      " '    def class_name(cls) -> str:\\n'\n",
      " '        \"\"\"Get class name.\"\"\"\\n'\n",
      " '        return cls.__name__\\n'\n",
      " '\\n'\n",
      " '    language: str = Field(\\n'\n",
      " '        description=\"The programming languge of the code being split.\"\\n'\n",
      " '    )\\n'\n",
      " '    signature_identifiers: Dict[str, _SignatureCaptureOptions] = Field(\\n'\n",
      " '        description=(\\n'\n",
      " '            \"A dictionary mapping the type of a split mapped to the first '\n",
      " 'and last type of its\"\\n'\n",
      " '            \"children which identify its signature.\"\\n'\n",
      " '        )\\n'\n",
      " '    )\\n'\n",
      " '    min_characters: int = Field(\\n'\n",
      " '        default=80,\\n'\n",
      " '        description=\"Minimum number of characters per chunk.\"\\n'\n",
      " '        \"Defaults to 80 because that\\'s about how long a replacement comment '\n",
      " 'is in skeleton mode.\",\\n'\n",
      " '    )\\n'\n",
      " '    code_splitter: Optional[CodeSplitter] = Field(\\n'\n",
      " '        description=\"The text splitter to use when splitting documents.\"\\n'\n",
      " '    )\\n'\n",
      " '    metadata_extractor: Optional[MetadataExtractor] = Field(\\n'\n",
      " '        default=None, description=\"Metadata extraction pipeline to apply to '\n",
      " 'nodes.\"\\n'\n",
      " '    )\\n'\n",
      " '    callback_manager: CallbackManager = Field(\\n'\n",
      " '        default_factory=CallbackManager, exclude=True\\n'\n",
      " '    )\\n'\n",
      " '    skeleton: bool = Field(\\n'\n",
      " '        True,\\n'\n",
      " '        description=\"Parent nodes have the text of their child nodes '\n",
      " 'replaced with a signature and a comment \"\\n'\n",
      " '        \"instructing the language model to visit the child node for the full '\n",
      " 'text of the scope.\",\\n'\n",
      " '    )\\n'\n",
      " '\\n'\n",
      " '    def __init__(\\n'\n",
      " '        self,\\n'\n",
      " '        language: str,\\n'\n",
      " '        skeleton: bool = True,\\n'\n",
      " '        signature_identifiers: Optional[Dict[str, _SignatureCaptureOptions]] '\n",
      " '= None,\\n'\n",
      " '        code_splitter: Optional[CodeSplitter] = None,\\n'\n",
      " '        callback_manager: Optional[CallbackManager] = None,\\n'\n",
      " '        metadata_extractor: Optional[MetadataExtractor] = None,\\n'\n",
      " '        min_characters: int = 80,\\n'\n",
      " '    ):\\n'\n",
      " '        callback_manager = callback_manager or CallbackManager([])\\n'\n",
      " '\\n'\n",
      " '        if signature_identifiers is None:\\n'\n",
      " '            try:\\n'\n",
      " '                signature_identifiers = '\n",
      " '_DEFAULT_SIGNATURE_IDENTIFIERS[language]\\n'\n",
      " '            except KeyError:\\n'\n",
      " '                raise ValueError(\\n'\n",
      " '                    f\"Must provide signature_identifiers for language '\n",
      " '{language}.\"\\n'\n",
      " '                )\\n'\n",
      " '\\n'\n",
      " '        super().__init__(\\n'\n",
      " '            language=language,\\n'\n",
      " '            callback_manager=callback_manager,\\n'\n",
      " '            metadata_extractor=metadata_extractor,\\n'\n",
      " '            code_splitter=code_splitter,\\n'\n",
      " '            signature_identifiers=signature_identifiers,\\n'\n",
      " '            min_characters=min_characters,\\n'\n",
      " '            skeleton=skeleton,\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '    def _get_node_name(self, node: Node) -> str:\\n'\n",
      " '        \"\"\"Get the name of a node.\"\"\"\\n'\n",
      " '        signature_identifier = self.signature_identifiers[node.type]\\n'\n",
      " '\\n'\n",
      " '        def recur(node: Node) -> str:\\n'\n",
      " '            for child in node.children:\\n'\n",
      " '                if child.type == signature_identifier.name_identifier:\\n'\n",
      " '                    return child.text.decode()\\n'\n",
      " '                if child.children:\\n'\n",
      " '                    out = recur(child)\\n'\n",
      " '                    if out:\\n'\n",
      " '                        return out\\n'\n",
      " '            return \"\"\\n'\n",
      " '\\n'\n",
      " '        return recur(node).strip()\\n'\n",
      " '\\n'\n",
      " '    def _get_node_signature(self, text: str, node: Node) -> str:\\n'\n",
      " '        \"\"\"Get the signature of a node.\"\"\"\\n'\n",
      " '        signature_identifier = self.signature_identifiers[node.type]\\n'\n",
      " '\\n'\n",
      " '        def find_start(node: Node) -> Optional[int]:\\n'\n",
      " '            if not signature_identifier.start_signature_types:\\n'\n",
      " '                signature_identifier.start_signature_types = []\\n'\n",
      " '\\n'\n",
      " '            for st in signature_identifier.start_signature_types:\\n'\n",
      " '                if node.type == st.type:\\n'\n",
      " '                    if st.inclusive:\\n'\n",
      " '                        return node.start_byte\\n'\n",
      " '                    return node.end_byte\\n'\n",
      " '\\n'\n",
      " '            for child in node.children:\\n'\n",
      " '                out = find_start(child)\\n'\n",
      " '                if out is not None:\\n'\n",
      " '                    return out\\n'\n",
      " '\\n'\n",
      " '            return None\\n'\n",
      " '\\n'\n",
      " '        def find_end(node: Node) -> Optional[int]:\\n'\n",
      " '            if not signature_identifier.end_signature_types:\\n'\n",
      " '                signature_identifier.end_signature_types = []\\n'\n",
      " '\\n'\n",
      " '            for st in signature_identifier.end_signature_types:\\n'\n",
      " '                if node.type == st.type:\\n'\n",
      " '                    if st.inclusive:\\n'\n",
      " '                        return node.end_byte\\n'\n",
      " '                    return node.start_byte\\n'\n",
      " '\\n'\n",
      " '            for child in node.children:\\n'\n",
      " '                out = find_end(child)\\n'\n",
      " '                if out is not None:\\n'\n",
      " '                    return out\\n'\n",
      " '\\n'\n",
      " '            return None\\n'\n",
      " '\\n'\n",
      " '        start_byte, end_byte = find_start(node), find_end(node)\\n'\n",
      " '        if start_byte is None:\\n'\n",
      " '            start_byte = node.start_byte\\n'\n",
      " '        if end_byte is None:\\n'\n",
      " '            end_byte = node.end_byte\\n'\n",
      " '        return text[start_byte:end_byte].strip()\\n'\n",
      " '\\n'\n",
      " '    def _chunk_node(\\n'\n",
      " '        self,\\n'\n",
      " '        parent: Node,\\n'\n",
      " '        text: str,\\n'\n",
      " '        _context_list: Optional[List[_ScopeItem]] = None,\\n'\n",
      " '        _root: bool = True,\\n'\n",
      " '    ) -> _ChunkNodeOutput:\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        This is really the \"main\" method of this class. It is recursive and '\n",
      " 'recursively\\n'\n",
      " '        chunks the text by the options identified in '\n",
      " 'self.signature_identifiers.\\n'\n",
      " '\\n'\n",
      " '        It is ran by get_nodes_from_documents.\\n'\n",
      " '\\n'\n",
      " '        Args:\\n'\n",
      " '            parent (Node): The parent node to chunk\\n'\n",
      " '            text (str): The text of the entire document\\n'\n",
      " '            _context_list (Optional[List[_ScopeItem]]): The scope context of '\n",
      " 'the parent node\\n'\n",
      " '            _root (bool): Whether or not this is the root node\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        if _context_list is None:\\n'\n",
      " '            _context_list = []\\n'\n",
      " '\\n'\n",
      " '        upstream_children_documents: List[TextNode] = []\\n'\n",
      " '        all_documents: List[TextNode] = []\\n'\n",
      " '\\n'\n",
      " '        # Capture any whitespace before parent.start_byte\\n'\n",
      " '        # Very important for space sensitive languages like python\\n'\n",
      " '        start_byte = parent.start_byte\\n'\n",
      " '        while start_byte > 0 and text[start_byte - 1] in (\" \", \"\\\\t\"):\\n'\n",
      " '            start_byte -= 1\\n'\n",
      " '\\n'\n",
      " '        # Create this node\\n'\n",
      " '        current_chunk = text[start_byte : parent.end_byte]\\n'\n",
      " '\\n'\n",
      " '        # Return early if the chunk is too small\\n'\n",
      " '        if len(current_chunk) < self.min_characters and not _root:\\n'\n",
      " '            return _ChunkNodeOutput(\\n'\n",
      " '                this_document=None, all_documents=[], '\n",
      " 'upstream_children_documents=[]\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        # TIP: This is a wonderful place to put a debug breakpoint when\\n'\n",
      " '        #      Trying to integrate a new language. Pay attention to '\n",
      " 'parent.type to learn\\n'\n",
      " '        #      all the availible node types and their hierarchy.\\n'\n",
      " '        if parent.type in self.signature_identifiers or _root:\\n'\n",
      " '            # Get the new context\\n'\n",
      " '            if not _root:\\n'\n",
      " '                new_context = _ScopeItem(\\n'\n",
      " '                    name=self._get_node_name(parent),\\n'\n",
      " '                    type=parent.type,\\n'\n",
      " '                    signature=self._get_node_signature(text=text, '\n",
      " 'node=parent),\\n'\n",
      " '                )\\n'\n",
      " '                _context_list.append(new_context)\\n'\n",
      " '            this_document = TextNode(\\n'\n",
      " '                text=current_chunk,\\n'\n",
      " '                metadata={\\n'\n",
      " '                    \"inclusive_scopes\": [cl.dict() for cl in '\n",
      " '_context_list],\\n'\n",
      " '                },\\n'\n",
      " '                relationships={\\n'\n",
      " '                    NodeRelationship.CHILD: [],\\n'\n",
      " '                },\\n'\n",
      " '            )\\n'\n",
      " '            all_documents.append(this_document)\\n'\n",
      " '        else:\\n'\n",
      " '            this_document = None\\n'\n",
      " '\\n'\n",
      " '        # Iterate over children\\n'\n",
      " '        for child in parent.children:\\n'\n",
      " '            if child.children:\\n'\n",
      " '                # Recurse on the child\\n'\n",
      " '                next_chunks = self._chunk_node(\\n'\n",
      " '                    child, text, _context_list=_context_list.copy(), '\n",
      " '_root=False\\n'\n",
      " '                )\\n'\n",
      " '\\n'\n",
      " '                # If there is a this_document, then we need to add the '\n",
      " 'children to this_document\\n'\n",
      " '                # and flush upstream_children_documents\\n'\n",
      " '                if this_document is not None:\\n'\n",
      " \"                    # If we have been given a document, that means it's \"\n",
      " 'children are already set, so it needs to become a child of this node\\n'\n",
      " '                    if next_chunks.this_document is not None:\\n'\n",
      " '                        assert (\\n'\n",
      " '                            not next_chunks.upstream_children_documents\\n'\n",
      " '                        ), \"next_chunks.this_document and '\n",
      " 'next_chunks.upstream_children_documents are exclusive.\"\\n'\n",
      " '                        '\n",
      " 'this_document.relationships[NodeRelationship.CHILD].append(  # type: ignore\\n'\n",
      " '                            '\n",
      " 'next_chunks.this_document.as_related_node_info()\\n'\n",
      " '                        )\\n'\n",
      " '                        next_chunks.this_document.relationships[\\n'\n",
      " '                            NodeRelationship.PARENT\\n'\n",
      " '                        ] = this_document.as_related_node_info()\\n'\n",
      " '                    # Otherwise, we have been given a list of '\n",
      " 'upstream_children_documents. We need to make them a child of this node\\n'\n",
      " '                    else:\\n'\n",
      " '                        for d in next_chunks.upstream_children_documents:\\n'\n",
      " '                            '\n",
      " 'this_document.relationships[NodeRelationship.CHILD].append(  # type: ignore\\n'\n",
      " '                                d.as_related_node_info()\\n'\n",
      " '                            )\\n'\n",
      " '                            d.relationships[\\n'\n",
      " '                                NodeRelationship.PARENT\\n'\n",
      " '                            ] = this_document.as_related_node_info()\\n'\n",
      " '                # Otherwise we pass the children upstream\\n'\n",
      " '                else:\\n'\n",
      " \"                    # If we have been given a document, that means it's \"\n",
      " 'children are already set, so it needs to become a child of the next node\\n'\n",
      " '                    if next_chunks.this_document is not None:\\n'\n",
      " '                        assert (\\n'\n",
      " '                            not next_chunks.upstream_children_documents\\n'\n",
      " '                        ), \"next_chunks.this_document and '\n",
      " 'next_chunks.upstream_children_documents are exclusive.\"\\n'\n",
      " '                        '\n",
      " 'upstream_children_documents.append(next_chunks.this_document)\\n'\n",
      " '                    # Otherwise, we have leftover children, they need to '\n",
      " 'become children of the next node\\n'\n",
      " '                    else:\\n'\n",
      " '                        upstream_children_documents.extend(\\n'\n",
      " '                            next_chunks.upstream_children_documents\\n'\n",
      " '                        )\\n'\n",
      " '\\n'\n",
      " '                # Lastly we need to maintain all documents\\n'\n",
      " '                all_documents.extend(next_chunks.all_documents)\\n'\n",
      " '\\n'\n",
      " '        return _ChunkNodeOutput(\\n'\n",
      " '            this_document=this_document,\\n'\n",
      " '            upstream_children_documents=upstream_children_documents,\\n'\n",
      " '            all_documents=all_documents,\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '    def get_nodes_from_documents(\\n'\n",
      " '        self,\\n'\n",
      " '        documents: Sequence[Document],\\n'\n",
      " '        show_progress: bool = False,\\n'\n",
      " '    ) -> List[BaseNode]:\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        The main public method of this class.\\n'\n",
      " '\\n'\n",
      " '        Parse documents into nodes.\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        out: List[BaseNode] = []\\n'\n",
      " '        with self.callback_manager.event(\\n'\n",
      " '            CBEventType.CHUNKING,\\n'\n",
      " '            payload={EventPayload.CHUNKS: [document.text for document in '\n",
      " 'documents]},\\n'\n",
      " '        ) as event:\\n'\n",
      " '            try:\\n'\n",
      " '                import tree_sitter_languages\\n'\n",
      " '            except ImportError:\\n'\n",
      " '                raise ImportError(\\n'\n",
      " '                    \"Please install tree_sitter_languages to use '\n",
      " 'CodeSplitter.\"\\n'\n",
      " '                )\\n'\n",
      " '\\n'\n",
      " '            try:\\n'\n",
      " '                parser = tree_sitter_languages.get_parser(self.language)\\n'\n",
      " '            except Exception as e:\\n'\n",
      " '                print(\\n'\n",
      " '                    f\"Could not get parser for language {self.language}. '\n",
      " 'Check \"\\n'\n",
      " '                    '\n",
      " '\"https://github.com/grantjenks/py-tree-sitter-languages#license \"\\n'\n",
      " '                    \"for a list of valid languages.\"\\n'\n",
      " '                )\\n'\n",
      " '                raise e\\n'\n",
      " '\\n'\n",
      " '            documents_with_progress = get_tqdm_iterable(\\n'\n",
      " '                documents, show_progress, \"Parsing documents into nodes\"\\n'\n",
      " '            )\\n'\n",
      " '            for document in documents_with_progress:\\n'\n",
      " '                text = document.text\\n'\n",
      " '                tree = parser.parse(bytes(text, \"utf-8\"))\\n'\n",
      " '\\n'\n",
      " '                if (\\n'\n",
      " '                    not tree.root_node.children\\n'\n",
      " '                    or tree.root_node.children[0].type != \"ERROR\"\\n'\n",
      " '                ):\\n'\n",
      " '                    # Chunk the code\\n'\n",
      " '                    _chunks = self._chunk_node(tree.root_node, '\n",
      " 'document.text)\\n'\n",
      " '                    assert (\\n'\n",
      " '                        _chunks.this_document is not None\\n'\n",
      " '                    ), \"Root node must be a chunk\"\\n'\n",
      " '                    chunks = _chunks.all_documents\\n'\n",
      " '\\n'\n",
      " '                    # Add your metadata to the chunks here\\n'\n",
      " '                    for chunk in chunks:\\n'\n",
      " '                        chunk.metadata = {\\n'\n",
      " '                            \"language\": self.language,\\n'\n",
      " '                            **chunk.metadata,\\n'\n",
      " '                            **document.metadata,\\n'\n",
      " '                        }\\n'\n",
      " '                        chunk.relationships[\\n'\n",
      " '                            NodeRelationship.SOURCE\\n'\n",
      " '                        ] = document.as_related_node_info()\\n'\n",
      " '\\n'\n",
      " '                    if self.skeleton:\\n'\n",
      " '                        self._skeletonize_list(chunks)\\n'\n",
      " '\\n'\n",
      " '                    # Now further split the code by lines and characters\\n'\n",
      " '                    # TODO: Test this and the relationships it creates\\n'\n",
      " '                    if self.code_splitter:\\n'\n",
      " '                        new_nodes = []\\n'\n",
      " '                        for original_node in chunks:\\n'\n",
      " '                            new_split_nodes = get_nodes_from_node(\\n'\n",
      " '                                original_node,\\n'\n",
      " '                                text_splitter=self.code_splitter,\\n'\n",
      " '                                include_metadata=True,\\n'\n",
      " '                                include_prev_next_rel=True,\\n'\n",
      " '                            )\\n'\n",
      " '\\n'\n",
      " '                            # Force the first new_split_node to have the '\n",
      " 'same id as the original_node\\n'\n",
      " '                            new_split_nodes[0].id_ = original_node.id_\\n'\n",
      " '\\n'\n",
      " '                            # Add the parent child info to all the '\n",
      " 'new_nodes_ derived from node\\n'\n",
      " '                            for new_split_node in new_split_nodes:\\n'\n",
      " '                                '\n",
      " 'new_split_node.relationships[NodeRelationship.CHILD] = '\n",
      " 'original_node.child_nodes  # type: ignore\\n'\n",
      " '                                '\n",
      " 'new_split_node.relationships[NodeRelationship.PARENT] = '\n",
      " 'original_node.parent_node  # type: ignore\\n'\n",
      " '\\n'\n",
      " '                            # Go through chunks and replace all instances of '\n",
      " 'node.node_id in relationships with new_nodes_[0].node_id\\n'\n",
      " '                            for old_node in chunks:\\n'\n",
      " '                                # Handle child nodes, which are a list\\n'\n",
      " '                                new_children = []\\n'\n",
      " '                                for old_nodes_child in old_node.child_nodes '\n",
      " 'or []:\\n'\n",
      " '                                    if old_nodes_child.node_id == '\n",
      " 'original_node.node_id:\\n'\n",
      " '                                        new_children.append(\\n'\n",
      " '                                            '\n",
      " 'new_split_nodes[0].as_related_node_info()\\n'\n",
      " '                                        )\\n'\n",
      " '                                    new_children.append(old_nodes_child)\\n'\n",
      " '                                old_node.relationships[\\n'\n",
      " '                                    NodeRelationship.CHILD\\n'\n",
      " '                                ] = new_children\\n'\n",
      " '\\n'\n",
      " '                                # Handle parent node\\n'\n",
      " '                                if (\\n'\n",
      " '                                    old_node.parent_node\\n'\n",
      " '                                    and old_node.parent_node.node_id\\n'\n",
      " '                                    == original_node.node_id\\n'\n",
      " '                                ):\\n'\n",
      " '                                    old_node.relationships[\\n'\n",
      " '                                        NodeRelationship.PARENT\\n'\n",
      " '                                    ] = '\n",
      " 'new_split_nodes[0].as_related_node_info()\\n'\n",
      " '\\n'\n",
      " '                            # Now save new_nodes_\\n'\n",
      " '                            new_nodes += new_split_nodes\\n'\n",
      " '\\n'\n",
      " '                        chunks = new_nodes\\n'\n",
      " '\\n'\n",
      " '                    # Or just extract metadata\\n'\n",
      " '                    if self.metadata_extractor:\\n'\n",
      " '                        chunks = '\n",
      " 'self.metadata_extractor.process_nodes(chunks)  # type: ignore\\n'\n",
      " '\\n'\n",
      " '                    event.on_end(\\n'\n",
      " '                        payload={EventPayload.CHUNKS: chunks},\\n'\n",
      " '                    )\\n'\n",
      " '\\n'\n",
      " '                    out += chunks\\n'\n",
      " '                else:\\n'\n",
      " '                    raise ValueError(\\n'\n",
      " '                        f\"Could not parse code with language '\n",
      " '{self.language}.\"\\n'\n",
      " '                    )\\n'\n",
      " '\\n'\n",
      " '        return out\\n'\n",
      " '\\n'\n",
      " '    @staticmethod\\n'\n",
      " '    def _get_indentation(text: str) -> Tuple[str, int, int]:\\n'\n",
      " '        indent_char = None\\n'\n",
      " '        minimum_chain = None\\n'\n",
      " '\\n'\n",
      " '        # Check that text is at least 1 line long\\n'\n",
      " '        text_split = text.splitlines()\\n'\n",
      " '        if len(text_split) == 0:\\n'\n",
      " '            raise ValueError(\"Text should be at least one line long.\")\\n'\n",
      " '\\n'\n",
      " '        for line in text_split:\\n'\n",
      " '            stripped_line = line.lstrip()\\n'\n",
      " '\\n'\n",
      " '            if stripped_line:\\n'\n",
      " \"                # Get whether it's tabs or spaces\\n\"\n",
      " '                spaces_count = line.count(\" \", 0, len(line) - '\n",
      " 'len(stripped_line))\\n'\n",
      " '                tabs_count = line.count(\"\\\\t\", 0, len(line) - '\n",
      " 'len(stripped_line))\\n'\n",
      " '\\n'\n",
      " '                if not indent_char:\\n'\n",
      " '                    if spaces_count:\\n'\n",
      " '                        indent_char = \" \"\\n'\n",
      " '                    if tabs_count:\\n'\n",
      " '                        indent_char = \"\\\\t\"\\n'\n",
      " '\\n'\n",
      " '                # Detect mixed indentation.\\n'\n",
      " '                if spaces_count > 0 and tabs_count > 0:\\n'\n",
      " '                    raise ValueError(\"Mixed indentation found.\")\\n'\n",
      " '                if indent_char == \" \" and tabs_count > 0:\\n'\n",
      " '                    raise ValueError(\"Mixed indentation found.\")\\n'\n",
      " '                if indent_char == \"\\\\t\" and spaces_count > 0:\\n'\n",
      " '                    raise ValueError(\"Mixed indentation found.\")\\n'\n",
      " '\\n'\n",
      " '                # Get the minimum chain of indent_char\\n'\n",
      " '                if indent_char:\\n'\n",
      " '                    char_count = line.count(\\n'\n",
      " '                        indent_char, 0, len(line) - len(stripped_line)\\n'\n",
      " '                    )\\n'\n",
      " '                    if minimum_chain is not None:\\n'\n",
      " '                        if char_count > 0:\\n'\n",
      " '                            minimum_chain = min(char_count, minimum_chain)\\n'\n",
      " '                    else:\\n'\n",
      " '                        if char_count > 0:\\n'\n",
      " '                            minimum_chain = char_count\\n'\n",
      " '\\n'\n",
      " '        # Handle edge case\\n'\n",
      " '        if indent_char is None:\\n'\n",
      " '            indent_char = \" \"\\n'\n",
      " '        if minimum_chain is None:\\n'\n",
      " '            minimum_chain = 4\\n'\n",
      " '\\n'\n",
      " '        # Get the first indent count\\n'\n",
      " '        first_line = text_split[0]\\n'\n",
      " '        first_indent_count = 0\\n'\n",
      " '        for char in first_line:\\n'\n",
      " '            if char == indent_char:\\n'\n",
      " '                first_indent_count += 1\\n'\n",
      " '            else:\\n'\n",
      " '                break\\n'\n",
      " '\\n'\n",
      " '        # Return the default indent level if only one indentation level was '\n",
      " 'found.\\n'\n",
      " '        return indent_char, minimum_chain, first_indent_count // '\n",
      " 'minimum_chain\\n'\n",
      " '\\n'\n",
      " '    @staticmethod\\n'\n",
      " '    def _get_comment_text(node: TextNode) -> str:\\n'\n",
      " '        \"\"\"Gets just the natural language text for a skeletonize '\n",
      " 'comment.\"\"\"\\n'\n",
      " '        return f\"Code replaced for brevity. See node_id {node.node_id}\"\\n'\n",
      " '\\n'\n",
      " '    @classmethod\\n'\n",
      " '    def _get_replacement_text(cls, child_node: TextNode) -> str:\\n'\n",
      " '        \"\"\"Manufactures a the replacement text to use to skeletonize a given '\n",
      " 'child node.\"\"\"\\n'\n",
      " '        signature = '\n",
      " 'child_node.metadata[\"inclusive_scopes\"][-1][\"signature\"]\\n'\n",
      " '        language = child_node.metadata[\"language\"]\\n'\n",
      " '        if language not in _COMMENT_OPTIONS:\\n'\n",
      " '            # TODO: Create a contribution message\\n'\n",
      " '            raise KeyError(\"Language not yet supported. Please '\n",
      " 'contribute!\")\\n'\n",
      " '        comment_options = _COMMENT_OPTIONS[language]\\n'\n",
      " '\\n'\n",
      " '        # Create the text to replace the child_node.text with\\n'\n",
      " '        (\\n'\n",
      " '            indentation_char,\\n'\n",
      " '            indentation_count_per_lvl,\\n'\n",
      " '            first_indentation_lvl,\\n'\n",
      " '        ) = cls._get_indentation(child_node.text)\\n'\n",
      " '\\n'\n",
      " '        # Start with a properly indented signature\\n'\n",
      " '        replacement_txt = (\\n'\n",
      " '            indentation_char * indentation_count_per_lvl * '\n",
      " 'first_indentation_lvl\\n'\n",
      " '            + signature\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '        # Add brackets if necessary. Expandable in the future to other '\n",
      " 'methods of scoping.\\n'\n",
      " '        if comment_options.scope_method == _ScopeMethod.BRACKETS:\\n'\n",
      " '            replacement_txt += \" {\\\\n\"\\n'\n",
      " '            replacement_txt += (\\n'\n",
      " '                indentation_char\\n'\n",
      " '                * indentation_count_per_lvl\\n'\n",
      " '                * (first_indentation_lvl + 1)\\n'\n",
      " '                + comment_options.comment_template.format(\\n'\n",
      " '                    cls._get_comment_text(child_node)\\n'\n",
      " '                )\\n'\n",
      " '                + \"\\\\n\"\\n'\n",
      " '            )\\n'\n",
      " '            replacement_txt += (\\n'\n",
      " '                indentation_char * indentation_count_per_lvl * '\n",
      " 'first_indentation_lvl\\n'\n",
      " '                + \"}\"\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        elif comment_options.scope_method == _ScopeMethod.INDENTATION:\\n'\n",
      " '            replacement_txt += \"\\\\n\"\\n'\n",
      " '            replacement_txt += indentation_char * indentation_count_per_lvl '\n",
      " '* (\\n'\n",
      " '                first_indentation_lvl + 1\\n'\n",
      " '            ) + comment_options.comment_template.format(\\n'\n",
      " '                cls._get_comment_text(child_node)\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        elif comment_options.scope_method == _ScopeMethod.HTML_END_TAGS:\\n'\n",
      " '            tag_name = child_node.metadata[\"inclusive_scopes\"][-1][\"name\"]\\n'\n",
      " '            end_tag = f\"</{tag_name}>\"\\n'\n",
      " '            replacement_txt += \"\\\\n\"\\n'\n",
      " '            replacement_txt += (\\n'\n",
      " '                indentation_char\\n'\n",
      " '                * indentation_count_per_lvl\\n'\n",
      " '                * (first_indentation_lvl + 1)\\n'\n",
      " '                + comment_options.comment_template.format(\\n'\n",
      " '                    cls._get_comment_text(child_node)\\n'\n",
      " '                )\\n'\n",
      " '                + \"\\\\n\"\\n'\n",
      " '            )\\n'\n",
      " '            replacement_txt += (\\n'\n",
      " '                indentation_char * indentation_count_per_lvl * '\n",
      " 'first_indentation_lvl\\n'\n",
      " '                + end_tag\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        else:\\n'\n",
      " '            raise KeyError(f\"Unrecognized enum value '\n",
      " '{comment_options.scope_method}\")\\n'\n",
      " '\\n'\n",
      " '        return replacement_txt\\n'\n",
      " '\\n'\n",
      " '    @classmethod\\n'\n",
      " '    def _skeletonize(cls, parent_node: TextNode, child_node: TextNode) -> '\n",
      " 'None:\\n'\n",
      " '        \"\"\"WARNING: In Place Operation\"\"\"\\n'\n",
      " '        # Simple protection clauses\\n'\n",
      " '        if child_node.text not in parent_node.text:\\n'\n",
      " '            raise ValueError(\"The child text is not contained inside the '\n",
      " 'parent text.\")\\n'\n",
      " '        if child_node.node_id not in (c.node_id for c in '\n",
      " 'parent_node.child_nodes or []):\\n'\n",
      " '            raise ValueError(\"The child node is not a child of the parent '\n",
      " 'node.\")\\n'\n",
      " '\\n'\n",
      " '        # Now do the replacement\\n'\n",
      " '        replacement_text = cls._get_replacement_text(child_node=child_node)\\n'\n",
      " '        parent_node.text = parent_node.text.replace(child_node.text, '\n",
      " 'replacement_text)\\n'\n",
      " '\\n'\n",
      " '    @classmethod\\n'\n",
      " '    def _skeletonize_list(cls, nodes: List[TextNode]) -> None:\\n'\n",
      " \"        # Create a convienient map for mapping node id's to nodes\\n\"\n",
      " '        node_id_map = {n.node_id: n for n in nodes}\\n'\n",
      " '\\n'\n",
      " '        def recur(node: TextNode) -> None:\\n'\n",
      " '            # If any children exist, skeletonize ourselves, starting at the '\n",
      " 'root DFS\\n'\n",
      " '            for child in node.child_nodes or []:\\n'\n",
      " '                child_node = node_id_map[child.node_id]\\n'\n",
      " '                cls._skeletonize(parent_node=node, child_node=child_node)\\n'\n",
      " '                recur(child_node)\\n'\n",
      " '\\n'\n",
      " '        # Iterate over root nodes and recur\\n'\n",
      " '        for n in nodes:\\n'\n",
      " '            if n.parent_node is None:\\n'\n",
      " '                recur(n)\\n')\n"
     ]
    }
   ],
   "source": [
    "print(len(nodes[1].text))\n",
    "pprint(nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are we to do? Well lets try splitting it. We are going to use the `CodeHierarchyNodeParser` to split the nodes into more reasonable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_nodes = CodeHierarchyNodeParser(\n",
    "    language=\"python\",\n",
    "\n",
    "    # You can further parameterize the CodeSplitter to split the code\n",
    "    # into \"chunks\" that match your context window size using\n",
    "    # chunck_lines and max_chars parameters, here we just use the defaults\n",
    "    code_splitter=CodeSplitter(\n",
    "        language=\"python\"\n",
    "    )\n",
    ").get_nodes_from_documents(nodes)\n",
    "len(split_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So that split up our data from 8 nodes into 112 nodes! Whats the max length of any of these nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(n.text) for n in split_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much shorter than before! Let's look at a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"\"\"Simple node parser.\"\"\"\\n'\n",
      " 'from typing import Callable, List, Optional, Sequence\\n'\n",
      " '\\n'\n",
      " 'from llama_index.bridge.pydantic import Field\\n'\n",
      " '\\n'\n",
      " 'from llama_index.callbacks.base import CallbackManager\\n'\n",
      " 'from llama_index.callbacks.schema import CBEventType, EventPayload\\n'\n",
      " 'from llama_index.node_parser.extractors.metadata_extractors import '\n",
      " 'MetadataExtractor\\n'\n",
      " 'from llama_index.node_parser.interface import NodeParser\\n'\n",
      " 'from llama_index.node_parser.node_utils import build_nodes_from_splits\\n'\n",
      " 'from llama_index.schema import BaseNode, Document\\n'\n",
      " 'from llama_index.text_splitter.utils import split_by_sentence_tokenizer\\n'\n",
      " 'from llama_index.utils import get_tqdm_iterable\\n'\n",
      " '\\n'\n",
      " 'DEFAULT_WINDOW_SIZE = 3\\n'\n",
      " 'DEFAULT_WINDOW_METADATA_KEY = \"window\"\\n'\n",
      " 'DEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class SentenceWindowNodeParser(NodeParser):\\n'\n",
      " '    # Code replaced for brevity. See node_id '\n",
      " '350a4aa6-d388-467c-9abb-d206fe9dacef')\n"
     ]
    }
   ],
   "source": [
    "pprint(split_nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without even needing a long printout we can see everything this module imported in the first document (which is at the module level) and the single class it defines. However, now instead of the class body, we see a comment: \n",
    "\n",
    "`# Code replaced for brevity. See node_id {node_id}`\n",
    "\n",
    "What if we go to that node_id?\n",
    "\n",
    "Notice below, that node_id is also in it's `child_nodes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'class SentenceWindowNodeParser(NodeParser):'\n"
     ]
    }
   ],
   "source": [
    "split_nodes_by_id = {n.node_id: n for n in split_nodes}\n",
    "pprint(split_nodes_by_id[\"350a4aa6-d388-467c-9abb-d206fe9dacef\"].text)\n",
    "\n",
    "assert \"350a4aa6-d388-467c-9abb-d206fe9dacef\" in (n.node_id for n in split_nodes[0].child_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an artefact of the `CodeSplitter`. This must have been a big class! But lets look at it's relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='350a4aa6-d388-467c-9abb-d206fe9dacef', node_type=None, metadata={'language': 'python', 'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}], 'filepath': '../../../../llama_index/node_parser/sentence_window.py'}, hash='1ba0da3da31ea95d61bd16676fcc192faf404aec007794c0007179424ceb9ea5'),\n",
      " <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ef755e26-a66a-4ec9-9144-d1a3f1f26ffc', node_type=None, metadata={'language': 'python', 'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}], 'filepath': '../../../../llama_index/node_parser/sentence_window.py'}, hash='29a4be710fef2a2fe594805008b2dfeafacd7cb5f19e8b10eef65802c3251128'),\n",
      " <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='c3aed921-d35a-4257-9fcd-a76a9ff5d12d', node_type=None, metadata={'language': 'python', 'inclusive_scopes': [], 'filepath': '../../../../llama_index/node_parser/sentence_window.py'}, hash='24b75e42cb2a1940f8f003a9457bf789568d122bb42bce28f98282b14fdb68e5'),\n",
      " <NodeRelationship.CHILD: '5'>: [RelatedNodeInfo(node_id='cf755401-4628-4d12-9973-5f6086612b2b', node_type=None, metadata={'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}, {'name': '__init__', 'type': 'function_definition', 'signature': 'def __init__(\\n        self,\\n        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\\n        window_size: int = DEFAULT_WINDOW_SIZE,\\n        window_metadata_key: str = DEFAULT_WINDOW_METADATA_KEY,\\n        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\\n        include_metadata: bool = True,\\n        include_prev_next_rel: bool = True,\\n        callback_manager: Optional[CallbackManager] = None,\\n        metadata_extractor: Optional[MetadataExtractor] = None,\\n    ) -> None:'}]}, hash='2cc7e93cb979cf389d47f92be0e3dae337ebd7ff6d463769a53821db80f85412'),\n",
      "                                 RelatedNodeInfo(node_id='cf877fbe-1a9b-4d0d-8071-a8d42fadac2e', node_type=None, metadata={'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}, {'name': 'class_name', 'type': 'function_definition', 'signature': 'def class_name(cls) -> str:'}]}, hash='04a447bf290bdb095af2b57f3545409e4ada1bf038c597b35cd6456710658c66'),\n",
      "                                 RelatedNodeInfo(node_id='2de319f5-bb4e-4727-9ebf-27bdac4fe93a', node_type=None, metadata={'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}, {'name': 'text_splitter', 'type': 'function_definition', 'signature': 'def text_splitter(self) -> Callable[[str], List[str]]:'}]}, hash='bc867bd67606b314632070c946c41fa2f2b123a182ee2e89ae541e9c428464bd'),\n",
      "                                 RelatedNodeInfo(node_id='737a47cf-20ca-4805-9101-08a43483141c', node_type=None, metadata={'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}, {'name': 'from_defaults', 'type': 'function_definition', 'signature': 'def from_defaults(\\n        cls,\\n        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\\n        window_size: int = DEFAULT_WINDOW_SIZE,\\n        window_metadata_key: str = DEFAULT_WINDOW_METADATA_KEY,\\n        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\\n        include_metadata: bool = True,\\n        include_prev_next_rel: bool = True,\\n        callback_manager: Optional[CallbackManager] = None,\\n        metadata_extractor: Optional[MetadataExtractor] = None,\\n    ) -> \"SentenceWindowNodeParser\":'}]}, hash='6601ea2c560ee8ee2e30815275afed3937d2ca55f735075ca3c03735012fa477'),\n",
      "                                 RelatedNodeInfo(node_id='bc9dc950-dc92-4acf-916a-65ddf6e9a4a7', node_type=None, metadata={'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}, {'name': 'get_nodes_from_documents', 'type': 'function_definition', 'signature': 'def get_nodes_from_documents(\\n        self,\\n        documents: Sequence[Document],\\n        show_progress: bool = False,\\n    ) -> List[BaseNode]:'}]}, hash='d0ee8fec1d89a4de0624b1bb8984f34543683dc8d5c0200f0ad1a56d55f01840'),\n",
      "                                 RelatedNodeInfo(node_id='6d80f4cd-40fa-4fa9-9fab-ad75b791c618', node_type=None, metadata={'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}, {'name': 'build_window_nodes_from_documents', 'type': 'function_definition', 'signature': 'def build_window_nodes_from_documents(\\n        self, documents: Sequence[Document]\\n    ) -> List[BaseNode]:'}]}, hash='b47c5fb94581ce6682a8a4a75c800904e5a3963966050e97684e72669cbee210')]}\n"
     ]
    }
   ],
   "source": [
    "pprint(split_nodes_by_id[\"350a4aa6-d388-467c-9abb-d206fe9dacef\"].relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that this node has a `NEXT` relationship, and many children.\n",
    "\n",
    "If we view it's `NEXT` relationship we will see things as the `CodeSplitter` sees things. Cutting up the node into chunks that are a certain character length. For more information about the `CodeSplitter` read this:\n",
    "https://docs.sweep.dev/blogs/chunking-2m-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the next node was split from this one because of a big docstring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"\"\"Sentence window node parser.\\n'\n",
      " '\\n'\n",
      " '    Splits a document into Nodes, with each node being a sentence.\\n'\n",
      " '    Each node contains a window from the surrounding sentences in the '\n",
      " 'metadata.\\n'\n",
      " '\\n'\n",
      " '    Args:\\n'\n",
      " '        sentence_splitter (Optional[Callable]): splits text into sentences\\n'\n",
      " '        include_metadata (bool): whether to include metadata in nodes\\n'\n",
      " '        include_prev_next_rel (bool): whether to include prev/next '\n",
      " 'relationships\\n'\n",
      " '    \"\"\"\\n'\n",
      " '\\n'\n",
      " '    sentence_splitter: Callable[[str], List[str]] = Field(\\n'\n",
      " '        default_factory=split_by_sentence_tokenizer,\\n'\n",
      " '        description=\"The text splitter to use when splitting documents.\",\\n'\n",
      " '        exclude=True,\\n'\n",
      " '    )\\n'\n",
      " '    window_size: int = Field(\\n'\n",
      " '        default=DEFAULT_WINDOW_SIZE,\\n'\n",
      " '        description=\"The number of sentences on each side of a sentence to '\n",
      " 'capture.\",\\n'\n",
      " '    )\\n'\n",
      " '    window_metadata_key: str = Field(\\n'\n",
      " '        default=DEFAULT_WINDOW_METADATA_KEY,\\n'\n",
      " '        description=\"The metadata key to store the sentence window under.\",\\n'\n",
      " '    )\\n'\n",
      " '    original_text_metadata_key: str = Field(\\n'\n",
      " '        default=DEFAULT_OG_TEXT_METADATA_KEY,\\n'\n",
      " '        description=\"The metadata key to store the original sentence in.\",\\n'\n",
      " '    )\\n'\n",
      " '    include_metadata: bool = Field(\\n'\n",
      " '        default=True, description=\"Whether or not to consider metadata when '\n",
      " 'splitting.\"\\n'\n",
      " '    )\\n'\n",
      " '    include_prev_next_rel: bool = Field(\\n'\n",
      " '        default=True, description=\"Include prev/next node relationships.\"\\n'\n",
      " '    )')\n"
     ]
    }
   ],
   "source": [
    "from llama_index.schema import NodeRelationship\n",
    "next_node_relationship_info = split_nodes_by_id[\"350a4aa6-d388-467c-9abb-d206fe9dacef\"].relationships[NodeRelationship.NEXT]\n",
    "next_node = split_nodes_by_id[next_node_relationship_info.node_id]\n",
    "pprint(next_node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the difference between `NodeRelationship.CHILD`/`NodeRelationship.PARENT` and `NodeRelationship.NEXT`/`NodeRelationship.PREVIOUS` as different dimensions.\n",
    "\n",
    "`CodeHierarchyNodeParser` creates `NodeRelationship.CHILD`/`NodeRelationship.PARENT` between code blocks based on scope hierarchys.\n",
    "\n",
    "Nodes which are then additionally split by `CodeSplitter` based on context length get an additional `NodeRelationship.NEXT`/`NodeRelationship.PREVIOUS`, and the first node in this chain maintains the `NodeRelationship.CHILD`/`NodeRelationship.PARENT` relationships given to it by `CodeHierarchyNodeParser`\n",
    "\n",
    "Now what if we explore that first nodes children (`NodeRelationship.CHILD`) are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('def __init__(\\n'\n",
      " '        self,\\n'\n",
      " '        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\\n'\n",
      " '        window_size: int = DEFAULT_WINDOW_SIZE,\\n'\n",
      " '        window_metadata_key: str = DEFAULT_WINDOW_METADATA_KEY,\\n'\n",
      " '        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\\n'\n",
      " '        include_metadata: bool = True,\\n'\n",
      " '        include_prev_next_rel: bool = True,\\n'\n",
      " '        callback_manager: Optional[CallbackManager] = None,\\n'\n",
      " '        metadata_extractor: Optional[MetadataExtractor] = None,\\n'\n",
      " '    ) -> None:\\n'\n",
      " '        \"\"\"Init params.\"\"\"\\n'\n",
      " '        callback_manager = callback_manager or CallbackManager([])\\n'\n",
      " '        sentence_splitter = sentence_splitter or '\n",
      " 'split_by_sentence_tokenizer()\\n'\n",
      " '        super().__init__(\\n'\n",
      " '            sentence_splitter=sentence_splitter,\\n'\n",
      " '            window_size=window_size,\\n'\n",
      " '            window_metadata_key=window_metadata_key,\\n'\n",
      " '            original_text_metadata_key=original_text_metadata_key,\\n'\n",
      " '            include_metadata=include_metadata,\\n'\n",
      " '            include_prev_next_rel=include_prev_next_rel,\\n'\n",
      " '            callback_manager=callback_manager,\\n'\n",
      " '            metadata_extractor=metadata_extractor,\\n'\n",
      " '        )')\n"
     ]
    }
   ],
   "source": [
    "next_node_relationship_info = split_nodes_by_id[\"350a4aa6-d388-467c-9abb-d206fe9dacef\"].relationships[NodeRelationship.CHILD][0]\n",
    "next_node = split_nodes_by_id[next_node_relationship_info.node_id]\n",
    "pprint(next_node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first child of the class is the `__init__` statement! That makes sense.\n",
    "\n",
    "I hope this has been a good demo of the class. Feel free to explore it further!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
