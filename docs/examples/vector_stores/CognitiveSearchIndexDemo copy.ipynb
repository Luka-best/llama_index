{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Example\n",
    "\n",
    "In this basic example, we take the a Paul Graham essay, split it into chunks, embed it using an OpenAI embedding model, load it into an Azure Cognitive Search index, and then query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install llama-index\n",
    "#!{sys.executable} -m pip install azure-search-documents==11.4.0b8\n",
    "#!{sys.executable} -m pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    Document,\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.vector_stores import CognitiveSearchVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up OpenAI\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Azure Cognitive Search\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "search_service_name = getpass.getpass(\n",
    "    \"Azure Cognitive Search Service Name\"\n",
    ")  \n",
    "\n",
    "key = getpass.getpass(\n",
    "    \"Azure Cognitive Search Key\"\n",
    ")  \n",
    "\n",
    "cognitive_search_credential = AzureKeyCredential(key)\n",
    "\n",
    "service_endpoint = f\"https://{search_service_name}.search.windows.net\"\n",
    "\n",
    "index_name = \"quickstart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchField,\n",
    "    SemanticSettings,\n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    ")\n",
    "\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_and_create_index(index_name: str, service_endpoint: str, credential: Any):\n",
    "    index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
    "    if index_name in index_client.list_index_names():\n",
    "        print(f\"Index {index_name} exists, dropping index\")\n",
    "        index_client.delete_index(index_name)\n",
    "\n",
    "    create_search_index(index_name, service_endpoint, credential)\n",
    "\n",
    "\n",
    "def create_search_index(index_name: str, service_endpoint: str, credential: Any):\n",
    "    # if args.verbose: print(f\"Ensuring search index {args.index} exists\")\n",
    "    index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
    "    if index_name not in index_client.list_index_names():\n",
    "        index = SearchIndex(\n",
    "            name=index_name,\n",
    "            fields=[\n",
    "                SimpleField(name=\"id\", type=\"Edm.String\", key=True),\n",
    "                SearchableField(\n",
    "                    name=\"content\", type=\"Edm.String\", analyzer_name=\"en.microsoft\"\n",
    "                ),\n",
    "                SearchField(\n",
    "                    name=\"embedding\",\n",
    "                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    hidden=False,\n",
    "                    searchable=True,\n",
    "                    filterable=False,\n",
    "                    sortable=False,\n",
    "                    facetable=False,\n",
    "                    vector_search_dimensions=1536,\n",
    "                    vector_search_configuration=\"default\",\n",
    "                ),\n",
    "                SimpleField(name=\"li_jsonMetadata\", type=\"Edm.String\"),\n",
    "                SimpleField(name=\"li_doc_id\", type=\"Edm.String\", filterable=True),\n",
    "            ],\n",
    "            semantic_settings=SemanticSettings(\n",
    "                configurations=[\n",
    "                    SemanticConfiguration(\n",
    "                        name=\"default\",\n",
    "                        prioritized_fields=PrioritizedFields(\n",
    "                            title_field=None,\n",
    "                            prioritized_content_fields=[\n",
    "                                SemanticField(field_name=\"content\")\n",
    "                            ],\n",
    "                        ),\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            vector_search=VectorSearch(\n",
    "                algorithm_configurations=[\n",
    "                    HnswVectorSearchAlgorithmConfiguration(\n",
    "                        name=\"default\",\n",
    "                        kind=\"hnsw\",\n",
    "                        parameters={\n",
    "                            \"m\": 4,\n",
    "                            \"efConstruction\": 400,\n",
    "                            \"efSearch\": 1000,\n",
    "                            \"metric\": \"cosine\",\n",
    "                        },\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        print(f\"Creating {index_name} search index\")\n",
    "        index_client.create_index(index)\n",
    "    else:\n",
    "        print(f\"Search index {index_name} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_and_create_index(index_name=index_name, service_endpoint=service_endpoint, credential=cognitive_search_credential)\n",
    "#create_search_index(\n",
    "#    index_name=index_name,\n",
    "#    service_endpoint=service_endpoint,\n",
    "#    credential=cognitive_search_credential,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Azure Cognitive Search\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "search_service_name = \"llmdevcog001\"  \n",
    "\n",
    "key = \"UWTIPqJwmA03Cjew8KmP9OkxidP4whkfxVN0EwzmJXAzSeAMib7T\"  \n",
    "\n",
    "cognitive_search_credential = AzureKeyCredential(key)\n",
    "\n",
    "service_endpoint = f\"https://{search_service_name}.search.windows.net\"\n",
    "\n",
    "index_name = \"quickstart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://llmdevcog001.search.windows.net/indexes?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': 'de0a519f-48cc-11ee-96f5-bc091bdb04fb'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.4.0b8 Python/3.11.4 (Windows-10-10.0.22621-SP0)'\n",
      "No body was attached to the request\n",
      "Request URL: 'https://llmdevcog001.search.windows.net/indexes?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': 'de0a519f-48cc-11ee-96f5-bc091bdb04fb'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.4.0b8 Python/3.11.4 (Windows-10-10.0.22621-SP0)'\n",
      "No body was attached to the request\n",
      "Request URL: 'https://llmdevcog001.search.windows.net/indexes?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': 'de0a519f-48cc-11ee-96f5-bc091bdb04fb'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.4.0b8 Python/3.11.4 (Windows-10-10.0.22621-SP0)'\n",
      "No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=minimal; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'de0a519f-48cc-11ee-96f5-bc091bdb04fb'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 01 Sep 2023 13:38:46 GMT'\n",
      "Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=minimal; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'de0a519f-48cc-11ee-96f5-bc091bdb04fb'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 01 Sep 2023 13:38:46 GMT'\n",
      "Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=minimal; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'de0a519f-48cc-11ee-96f5-bc091bdb04fb'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 01 Sep 2023 13:38:46 GMT'\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from llama_index.vector_stores import CognitiveSearchVectorStore\n",
    "from llama_index.vector_stores.cogsearch import IndexManagement, MetadataIndexFieldType\n",
    "\n",
    "# set up Azure Cognitive Search vector store and load in data\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint,\n",
    "    credential=cognitive_search_credential,\n",
    ")\n",
    "\n",
    "metadata_fields = [\"author\", \"theme\", \"director\"]\n",
    "# metadata_fields = {\"Subject\": (\"SchoolSubject\", MetadataIndexFieldType.STRING), \"Class\": (\"RoomNumber\", MetadataIndexFieldType.INT32), \"Building\": (\"Block\", MetadataIndexFieldType.DOUBLE)}\n",
    "\n",
    "vector_store = CognitiveSearchVectorStore(\n",
    "    search_or_index_client=index_client,\n",
    "    index_name = \"quickstart01\",\n",
    "    filterable_metadata_field_keys = metadata_fields,\n",
    "    index_management = IndexManagement.CREATE_IF_NOT_EXISTS,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"content\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    metadata_string_field_key=\"li_jsonMetadata\",\n",
    "    doc_id_field_key=\"li_doc_id\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://llmdevcog001.search.windows.net/indexes?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': 'fbc3e51a-48cc-11ee-9a1d-bc091bdb04fb'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.4.0b8 Python/3.11.4 (Windows-10-10.0.22621-SP0)'\n",
      "No body was attached to the request\n",
      "Request URL: 'https://llmdevcog001.search.windows.net/indexes?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': 'fbc3e51a-48cc-11ee-9a1d-bc091bdb04fb'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.4.0b8 Python/3.11.4 (Windows-10-10.0.22621-SP0)'\n",
      "No body was attached to the request\n",
      "Request URL: 'https://llmdevcog001.search.windows.net/indexes?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': 'fbc3e51a-48cc-11ee-9a1d-bc091bdb04fb'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.4.0b8 Python/3.11.4 (Windows-10-10.0.22621-SP0)'\n",
      "No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=minimal; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'fbc3e51a-48cc-11ee-9a1d-bc091bdb04fb'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 01 Sep 2023 13:39:33 GMT'\n",
      "Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=minimal; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'fbc3e51a-48cc-11ee-9a1d-bc091bdb04fb'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 01 Sep 2023 13:39:33 GMT'\n",
      "Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=minimal; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'fbc3e51a-48cc-11ee-9a1d-bc091bdb04fb'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 01 Sep 2023 13:39:33 GMT'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Search client not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m storage_context \u001b[39m=\u001b[39m StorageContext\u001b[39m.\u001b[39mfrom_defaults(vector_store\u001b[39m=\u001b[39mvector_store)\n\u001b[0;32m     31\u001b[0m service_context \u001b[39m=\u001b[39m ServiceContext\u001b[39m.\u001b[39mfrom_defaults(embed_model\u001b[39m=\u001b[39membed_model)\n\u001b[1;32m---> 32\u001b[0m index \u001b[39m=\u001b[39m VectorStoreIndex\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[0;32m     33\u001b[0m     documents, storage_context\u001b[39m=\u001b[39;49mstorage_context, service_context\u001b[39m=\u001b[39;49mservice_context\n\u001b[0;32m     34\u001b[0m )\n",
      "File \u001b[1;32mC:\\source_code\\github.com\\rivms\\llama_index\\llama_index\\indices\\base.py:102\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     docstore\u001b[39m.\u001b[39mset_document_hash(doc\u001b[39m.\u001b[39mget_doc_id(), doc\u001b[39m.\u001b[39mhash)\n\u001b[0;32m     98\u001b[0m nodes \u001b[39m=\u001b[39m service_context\u001b[39m.\u001b[39mnode_parser\u001b[39m.\u001b[39mget_nodes_from_documents(\n\u001b[0;32m     99\u001b[0m     documents, show_progress\u001b[39m=\u001b[39mshow_progress\n\u001b[0;32m    100\u001b[0m )\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m    103\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[0;32m    104\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[0;32m    105\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[0;32m    106\u001b[0m     show_progress\u001b[39m=\u001b[39;49mshow_progress,\n\u001b[0;32m    107\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    108\u001b[0m )\n",
      "File \u001b[1;32mC:\\source_code\\github.com\\rivms\\llama_index\\llama_index\\indices\\vector_store\\base.py:46\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async \u001b[39m=\u001b[39m use_async\n\u001b[0;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store_nodes_override \u001b[39m=\u001b[39m store_nodes_override\n\u001b[1;32m---> 46\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     47\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[0;32m     48\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[0;32m     49\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[0;32m     50\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[0;32m     51\u001b[0m     show_progress\u001b[39m=\u001b[39;49mshow_progress,\n\u001b[0;32m     52\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m     53\u001b[0m )\n",
      "File \u001b[1;32mC:\\source_code\\github.com\\rivms\\llama_index\\llama_index\\indices\\base.py:71\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[1;34m(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m index_struct \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[39massert\u001b[39;00m nodes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     index_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_index_from_nodes(nodes)\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct \u001b[39m=\u001b[39m index_struct\n\u001b[0;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context\u001b[39m.\u001b[39mindex_store\u001b[39m.\u001b[39madd_index_struct(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct)\n",
      "File \u001b[1;32mC:\\source_code\\github.com\\rivms\\llama_index\\llama_index\\indices\\vector_store\\base.py:241\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_index_from_nodes\u001b[39m(\u001b[39mself\u001b[39m, nodes: Sequence[BaseNode]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m IndexDict:\n\u001b[0;32m    235\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build the index from nodes.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m \u001b[39m    NOTE: Overrides BaseIndex.build_index_from_nodes.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m        VectorStoreIndex only stores nodes in document store\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m        if vector store does not store text\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_index_from_nodes(nodes)\n",
      "File \u001b[1;32mC:\\source_code\\github.com\\rivms\\llama_index\\llama_index\\indices\\vector_store\\base.py:229\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m    227\u001b[0m     run_async_tasks(tasks)\n\u001b[0;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 229\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_nodes_to_index(\n\u001b[0;32m    230\u001b[0m         index_struct, nodes, show_progress\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_show_progress\n\u001b[0;32m    231\u001b[0m     )\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m index_struct\n",
      "File \u001b[1;32mC:\\source_code\\github.com\\rivms\\llama_index\\llama_index\\indices\\vector_store\\base.py:202\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[1;34m(self, index_struct, nodes, show_progress)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    201\u001b[0m embedding_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_node_embedding_results(nodes, show_progress)\n\u001b[1;32m--> 202\u001b[0m new_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vector_store\u001b[39m.\u001b[39;49madd(embedding_results)\n\u001b[0;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store\u001b[39m.\u001b[39mstores_text \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store_nodes_override:\n\u001b[0;32m    205\u001b[0m     \u001b[39m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     \u001b[39m# we need to add the nodes to the index struct and document store\u001b[39;00m\n\u001b[0;32m    207\u001b[0m     \u001b[39mfor\u001b[39;00m result, new_id \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(embedding_results, new_ids):\n",
      "File \u001b[1;32mC:\\source_code\\github.com\\rivms\\llama_index\\llama_index\\vector_stores\\cogsearch.py:461\u001b[0m, in \u001b[0;36mCognitiveSearchVectorStore.add\u001b[1;34m(self, embedding_results)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Add embedding results to index associated with the configured search client.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[39mArgs\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[39m    embedding_results: List[NodeWithEmbedding]: list of embedding results\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \n\u001b[0;32m    458\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_search_client:\n\u001b[1;32m--> 461\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSearch client not initialized\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    463\u001b[0m documents \u001b[39m=\u001b[39m []\n\u001b[0;32m    464\u001b[0m ids \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: Search client not initialized"
     ]
    }
   ],
   "source": [
    "# define embedding function\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"../../../examples/paul_graham_essay/data\"\n",
    ").load_data()\n",
    "\n",
    "# set up Azure Cognitive Search vector store and load in data\n",
    "search_client = SearchClient(\n",
    "    endpoint=service_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=cognitive_search_credential,\n",
    ")\n",
    "\n",
    "vector_store = CognitiveSearchVectorStore(\n",
    "    search_or_index_client=index_client,\n",
    "    index_name = \"quickstart01\",\n",
    "    filterable_metadata_field_keys = metadata_fields,\n",
    "    index_management = IndexManagement.VALIDATE_INDEX,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"content\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    metadata_string_field_key=\"li_jsonMetadata\",\n",
    "    doc_id_field_key=\"li_doc_id\",\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Data\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"What did the author learn?\",\n",
    ")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What was a hard moment for the author?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Who is the author?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "query_engine = index.as_query_engine(streaming=True)\n",
    "response = query_engine.query(\"What happened at interleaf?\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "token_count = 0\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")\n",
    "    token_count += 1\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "tokens_per_second = token_count / time_elapsed\n",
    "\n",
    "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What colour is the sky?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.insert_nodes([Document(text=\"The sky is indigo today\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What colour is the sky?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import TextNode\n",
    "\n",
    "nodes = [\n",
    "    TextNode(\n",
    "        text=\"The Shawshank Redemption\",\n",
    "        metadata={\n",
    "            \"author\": \"Stephen King\",\n",
    "            \"theme\": \"Friendship\",\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"The Godfather\",\n",
    "        metadata={\n",
    "            \"director\": \"Francis Ford Coppola\",\n",
    "            \"theme\": \"Mafia\",\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Inception\",\n",
    "        metadata={\n",
    "            \"director\": \"Christopher Nolan\",\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.insert_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\n",
    "\n",
    "\n",
    "filters = MetadataFilters(filters=[ExactMatchFilter(key=\"theme\", value=\"Mafia\")])\n",
    "\n",
    "retriever = index.as_retriever(filters=filters)\n",
    "retriever.retrieve(\"What is inception about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, cast, Dict, Callable, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum \n",
    "class MetadataIndexFieldType(str, enum.Enum):\n",
    "    \"\"\"Enumeration representing the supported types for metadata fields in an Azure Cognitive Search Index, corresponds with types supported in a flat metadata dictionary\"\"\"\n",
    "\n",
    "    STRING = \"Edm.String\"\n",
    "    BOOLEAN = \"Edm.Boolean\"\n",
    "    INT32 = \"Edm.Int32\"\n",
    "    INT64 = \"Edm.Int64\"\n",
    "    DOUBLE = \"Edm.Double\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = MetadataIndexFieldType.BOOLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_metadata_to_index_field_dict(filterable_metadata_field_keys: Union[\n",
    "                List[str] | Dict[str, str],\n",
    "                Dict[str, tuple[str, MetadataIndexFieldType]]\n",
    "        ] = None) -> Dict[str, tuple[str, MetadataIndexFieldType]]:\n",
    "    \"\"\"\n",
    "    Normalises the supported forms for specifying metadata field names and their\n",
    "    corresponding field types and field names in the Azure Cognitive Search index \n",
    "    \"\"\"\n",
    "    index_field_spec: Dict[str, tuple[str, MetadataIndexFieldType]] = {}\n",
    "\n",
    "    if isinstance(filterable_metadata_field_keys, List):\n",
    "        \n",
    "        for metadata_field in filterable_metadata_field_keys:\n",
    "            # Index field name and the metadata field name are the same\n",
    "            # Use String as the default index field type\n",
    "            index_field_spec[metadata_field] = (metadata_field, MetadataIndexFieldType.STRING)\n",
    "\n",
    "    elif isinstance(filterable_metadata_field_keys, Dict):      \n",
    "        for metadata_field,v in filterable_metadata_field_keys.items():\n",
    "            if isinstance(v, tuple):\n",
    "                # Index field name and metadata field name may differ\n",
    "                # The index field type used is as supplied\n",
    "                index_field_spec[metadata_field] = (v[0], v[1])\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                # Index field name and metadata field name may differ\n",
    "                # Use String as the default index field type\n",
    "                index_field_spec[metadata_field] = (v, MetadataIndexFieldType.STRING)\n",
    "    return index_field_spec\n",
    "\n",
    "\n",
    "def create_metadata_index_fields(filterable_metadata_field_keys: Union[\n",
    "                List[str] | Dict[str, str],\n",
    "                Dict[str, tuple[str, MetadataIndexFieldType]]\n",
    "        ] = None) -> List[SearchableField]:\n",
    "    index_fields = []\n",
    "    index_field_spec = get_metadata_to_index_field_dict(filterable_metadata_field_keys)\n",
    "\n",
    "    print(\"Mappings\")\n",
    "\n",
    "    # create search fields\n",
    "    for k,v in index_field_spec.items():\n",
    "        field_name, field_type = v\n",
    "        index_field_type = MetadataIndexFieldType(field_type).name\n",
    "        field = SimpleField(name=field_name, type=index_field_type, filterable=True)\n",
    "        index_fields.append(field)\n",
    "\n",
    "    return index_fields\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_field(s: SearchField):\n",
    "    print(f\"{s.name},{s.type},{s.filterable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = test_param({\"content\": (\"chunk\", MetadataIndexFieldType.INT32), \"id\": (\"id\", MetadataIndexFieldType.DOUBLE)})\n",
    "print(\"Search Fields\")\n",
    "for i in index:\n",
    "    print_search_field(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_param([\"content\", \"doc_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = test_param({\"content\": \"chunk\", \"id\": \"id\"})\n",
    "for i in index:\n",
    "    print_search_field(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = test_param({\"content\": (\"chunk\", MetadataIndexFieldType.INT32), \"id\": (\"id\", MetadataIndexFieldType.DOUBLE)})\n",
    "for i in index:\n",
    "    print_search_field(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindextest01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
