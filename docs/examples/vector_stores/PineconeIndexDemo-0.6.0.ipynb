{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307804a3-c02b-4a57-ac0d-172c30ddc851",
   "metadata": {},
   "source": [
    "# LlamaIndex + Pinecone "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba1864",
   "metadata": {},
   "source": [
    "In this tutorial, we show how to use LlamaIndex with Pinecone to answer complex queries over multiple data sources.  \n",
    "* While Pinecone provides a powerful and efficient retrieval engine,\n",
    "it remains challenging to answer complex questions that require multi-step reasoning and synthesis over many data sources.\n",
    "* With LlamaIndex, we combine the power of vector similiarty search and multi-step reasoning to delivery higher quality and richer responses.\n",
    "\n",
    "\n",
    "Here, we show 2 specific use-cases:\n",
    "1. compare and contrast queries over Wikipedia articles about different cities.\n",
    "2. temporal queries that require reasoning over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d48af8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "#### Creating a Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce3143d-198c-4dd2-8e5a-c5cdf94f017a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suo/miniconda3/envs/llama/lib/python3.9/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81fc463e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinecone.init(environment=\"eu-west1-gcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index if it does not already exist\n",
    "# dimensions are for text-embedding-ada-002\n",
    "pinecone.create_index(\"quickstart-index\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1544b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinecone_index = pinecone.Index(\"quickstart-index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b8cae",
   "metadata": {},
   "source": [
    "# Use-Case 1: Compare and Contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d56590",
   "metadata": {},
   "source": [
    "#### Load Dataset\n",
    "\n",
    "Fetch and load Wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410eec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suo/miniconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583afcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_titles = [\"Toronto\", \"Seattle\", \"San Francisco\", \"Chicago\", \"Boston\", \"Washington, D.C.\", \"Cambridge, Massachusetts\", \"Houston\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "data_path = Path('data_wiki')\n",
    "\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'titles': title,\n",
    "            'prop': 'extracts',\n",
    "            # 'exintro': True,\n",
    "            'explaintext': True,\n",
    "        }\n",
    "    ).json()\n",
    "    page = next(iter(response['query']['pages'].values()))\n",
    "    wiki_text = page['extract']\n",
    "\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", 'w') as fp:\n",
    "        fp.write(wiki_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b802084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all wiki documents\n",
    "city_docs = {}\n",
    "all_docs = []\n",
    "for wiki_title in wiki_titles:\n",
    "    city_docs[wiki_title] = SimpleDirectoryReader(input_files=[data_path / f\"{wiki_title}.txt\"]).load_data()\n",
    "    all_docs.extend(city_docs[wiki_title])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "#### Build Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a2bcc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba1558b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index for Toronto\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 20744 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 20744 tokens\n",
      "Building index for Seattle\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 16942 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 16942 tokens\n",
      "Building index for San Francisco\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 23433 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 23433 tokens\n",
      "Building index for Chicago\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 26082 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 26082 tokens\n",
      "Building index for Boston\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 18648 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 18648 tokens\n",
      "Building index for Washington, D.C.\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 21649 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 21649 tokens\n",
      "Building index for Cambridge, Massachusetts\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 12855 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 12855 tokens\n",
      "Building index for Houston\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 21844 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 21844 tokens\n"
     ]
    }
   ],
   "source": [
    "# Build index for each city document\n",
    "city_indices = {}\n",
    "index_summaries = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    print(f\"Building index for {wiki_title}\")\n",
    "    # create storage context\n",
    "    vector_store = PineconeVectorStore(pinecone_index=pinecone_index, namespace=wiki_title)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    # build index\n",
    "    city_indices[wiki_title] = GPTVectorStoreIndex.from_documents(city_docs[wiki_title], storage_context=storage_context)\n",
    "\n",
    "    # set summary text for city\n",
    "    index_summaries[wiki_title] = f\"Wikipedia articles about {wiki_title}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04304299-fc3e-40a0-8600-f50c3292767e",
   "metadata": {},
   "source": [
    "#### Build Graph Query Engine for Compare & Contrast Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35369eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.composability import ComposableGraph\n",
    "from llama_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bedbb693-725f-478f-be26-fa7180ea38b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "graph = ComposableGraph.from_indices(\n",
    "    GPTSimpleKeywordTableIndex,\n",
    "    [index for _, index in city_indices.items()], \n",
    "    [summary for _, summary in index_summaries.items()],\n",
    "    max_keywords_per_chunk=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8ddcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.query.query_transform.base import DecomposeQueryTransform\n",
    "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
    "\n",
    "decompose_transform = DecomposeQueryTransform(verbose=True)\n",
    "\n",
    "custom_query_engines = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    index = city_indices[wiki_title]\n",
    "    query_engine = index.as_query_engine()\n",
    "    query_engine = TransformQueryEngine(\n",
    "        query_engine,\n",
    "        query_transform=decompose_transform,\n",
    "        transform_extra_info={'index_summary': index_summaries[wiki_title]},\n",
    "    )\n",
    "    custom_query_engines[index.index_id] = query_engine\n",
    "\n",
    "custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n",
    "    retriever_mode='simple',\n",
    "    response_mode='tree_summarize',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78235fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with query decomposition in subindices\n",
    "query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3616c",
   "metadata": {},
   "source": [
    "#### Run Compare & Contrast Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f210a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.keyword_table.retrievers:> Starting query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
      "> Starting query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
      "INFO:llama_index.indices.keyword_table.retrievers:query keywords: ['contrast', 'demographics', 'seattle', 'toronto', 'compare', 'houston']\n",
      "query keywords: ['contrast', 'demographics', 'seattle', 'toronto', 'compare', 'houston']\n",
      "INFO:llama_index.indices.keyword_table.retrievers:> Extracted keywords: ['seattle', 'toronto', 'houston']\n",
      "> Extracted keywords: ['seattle', 'toronto', 'houston']\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Seattle?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 7 tokens\n",
      "> [retrieve] Total embedding token usage: 7 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Seattle?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1789 tokens\n",
      "> [get_response] Total LLM token usage: 1789 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Toronto?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 7 tokens\n",
      "> [retrieve] Total embedding token usage: 7 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Toronto?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1884 tokens\n",
      "> [get_response] Total LLM token usage: 1884 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Houston?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 7 tokens\n",
      "> [retrieve] Total embedding token usage: 7 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Houston?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1780 tokens\n",
      "> [get_response] Total LLM token usage: 1780 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 226 tokens\n",
      "> [get_response] Total LLM token usage: 226 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 226 tokens\n",
      "> [get_response] Total LLM token usage: 226 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Compare and contrast the demographics in Seattle, Houston, and Toronto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d24db9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Seattle, Houston, and Toronto are all large cities with diverse populations. Seattle has the smallest population of the three cities, with 753,675 people as of 2021. Houston has the second largest population, with 2,304,580 people according to the 2020 U.S. census. Toronto has the largest population of the three cities, with 6,202,225 people in 2021. All three cities have a mix of different ethnicities and cultures, with Seattle having the most diverse population. Houston and Toronto have larger immigrant populations than Seattle. All three cities have a mix of different economic classes, with Toronto having the highest median household income."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "display_response(response, show_source=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a23412f",
   "metadata": {},
   "source": [
    "# Use-Case 2: Temporal Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce276271",
   "metadata": {},
   "source": [
    "Temporal queries such as \"what happened after X\" is intuitive to humans, but can often confuse vector databases.  \n",
    "\n",
    "This is because the vector embedding will focus on the subject \"X\" rather than the imporant temporal cue. This results in irrelevant and misleading context that harms the final answer.  \n",
    "\n",
    "LlamaIndex solves this by explicitly maintainging node relationships and leverage LLM to automatically perform query expansion to find more relevant context.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3150f9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader, StorageContext, GPTVectorStoreIndex\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader('../data/paul_graham').load_data()\n",
    "\n",
    "# define storage context\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index, namespace='pg_essay')\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# build index \n",
    "index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context, store_nodes_override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe01aa",
   "metadata": {},
   "source": [
    "We can define an auto prev/next node postprocessor to leverage LLM reasoning to help query expansion (with relevant additional nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56f2d0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor.node import AutoPrevNextNodePostprocessor\n",
    "\n",
    "# define postprocessor\n",
    "node_postprocessor = AutoPrevNextNodePostprocessor(\n",
    "    docstore=index.storage_context.docstore, \n",
    "    service_context=index.service_context,\n",
    "    num_nodes=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# define query engine\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=1,\n",
    "    node_postprocessors=[node_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56896a41",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffd6c81a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 17 tokens\n",
      "> [retrieve] Total embedding token usage: 17 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1153 tokens\n",
      "> [get_response] Total LLM token usage: 1153 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1153 tokens\n",
      "> [get_response] Total LLM token usage: 1153 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> Postprocessor Predicted mode: next\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 4480 tokens\n",
      "> [get_response] Total LLM token usage: 4480 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# Infer that we need to search nodes after current one\n",
    "response = query_engine.query(\n",
    "    \"What did the author do after handing off Y Combinator to Sam Altman?\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f91a50f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** After handing off Y Combinator to Sam Altman, the author decided to take a break and focus on painting. He spent most of the rest of 2014 painting, but eventually ran out of steam and stopped working on it. He then started writing essays again and wrote a bunch of new ones over the next few months. In March 2015, he started working on Lisp again and spent the next four years writing a new Lisp called Bel. During this time, he had to ban himself from writing essays in order to focus on the project. In the summer of 2016, he and his family moved to England and he wrote most of Bel there. In the fall of 2019, Bel was finally finished and he wrote a bunch of essays about topics he had stacked up. He then wrote an essay for himself to answer the question of how he should choose what to do next. After taking advice from Robert Morris, he decided to move on from Y Combinator and focus on other projects. He has since been involved in a variety of projects, including writing essays, working on Lisp, and investing in startups."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22434b2e",
   "metadata": {},
   "source": [
    "In comparison, naive top-k retrieval results in irrelevant context and hallucinated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93d2432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 17 tokens\n",
      "> [retrieve] Total embedding token usage: 17 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1028 tokens\n",
      "> [get_response] Total LLM token usage: 1028 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** After handing off Y Combinator to Sam Altman, the author went on to found OpenAI, a research laboratory dedicated to artificial intelligence. He also wrote a book, \"The Launch Pad: Inside Y Combinator, Silicon Valley's Most Exclusive School for Startups,\" and became a partner at Founders Fund, a venture capital firm."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/1`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Document ID:** b06bb792-633e-498a-84ce-84ec68308a82<br>**Similarity:** 0.83942616<br>**Text:** in. We also noticed that the startups were becoming one another's customers. We used to refer jokingly to the \"YC GDP,\" but as YC grows this becomes less and less of a joke. Now lots of startups get their initial set of customers almost entirely from among their batchmates.\n",
       "\n",
       "I had not originally intended YC to be a full-time job. I was going to do three things: hack, write essays, and work on YC. As YC grew, and I grew more excited about it, it started to take up a lot more than a third of my attention. But for the first few years I was still able to work on other things.\n",
       "\n",
       "In the summer of 2006, Robert and I started working on a new version of Arc. This one was reasonably fast, because it was compiled into Scheme. To test this new Arc, I wrote Hacker News in it. It was originally meant to be a news aggregator for startup founders and was called Startup News, but after a few months I got tired of reading about nothing but startups. Plus it wasn't startup founders we wanted to reach. ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define query engine\n",
    "naive_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=1,\n",
    ")\n",
    "\n",
    "response = naive_query_engine.query(\n",
    "    \"What did the author do after handing off Y Combinator to Sam Altman?\", \n",
    ")\n",
    "\n",
    "display_response(response, show_source=True, source_length=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64117464",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "221001e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 17 tokens\n",
      "> [retrieve] Total embedding token usage: 17 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1131 tokens\n",
      "> [get_response] Total LLM token usage: 1131 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1131 tokens\n",
      "> [get_response] Total LLM token usage: 1131 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> Postprocessor Predicted mode: previous\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 4277 tokens\n",
      "> [get_response] Total LLM token usage: 4277 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# Infer that we need to search nodes before current one\n",
    "response = query_engine.query(\n",
    "    \"What did the author do before handing off Y Combinator to Sam Altman?\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6bf950f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Before handing off Y Combinator to Sam Altman, the author wrote essays, worked on spam filters, painted, cooked for groups, bought a building in Cambridge, went on dates with Jessica Livingston, gave talks about how to start a startup, worked on a new version of Arc, and wrote a book called Hackers & Painters. He also schemed with Robert and Trevor about projects they could work on together, and worked on a subset of a new architecture that could be done as an open source project. He also wrote essays about a variety of topics, and started angel investing. He also recognized the potential of the internet to allow anyone to publish anything, and began to focus on writing essays online. He worked on projects that weren't prestigious, such as Lisp, Still Life painting, Viaweb, and Y Combinator, which he believed had the potential to be discovered."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7bbed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
