{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "307804a3-c02b-4a57-ac0d-172c30ddc851",
            "metadata": {},
            "source": [
                "# LlamaIndex + Pinecone "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "99ba1864",
            "metadata": {},
            "source": [
                "In this tutorial, we show how to use LlamaIndex with Pinecone to answer complex queries over multiple data sources.  \n",
                "* While Pinecone provides a powerful and efficient retrieval engine,\n",
                "it remains challenging to answer complex questions that require multi-step reasoning and synthesis over many data sources.\n",
                "* With LlamaIndex, we combine the power of vector similiarty search and multi-step reasoning to delivery higher quality and richer responses.\n",
                "\n",
                "\n",
                "Here, we show 2 specific use-cases:\n",
                "1. compare and contrast queries over Wikipedia articles about different cities.\n",
                "2. temporal queries that require reasoning over time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "d48af8e1",
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "import sys\n",
                "\n",
                "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
                "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
            "metadata": {},
            "source": [
                "#### Creating a Pinecone Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "0ce3143d-198c-4dd2-8e5a-c5cdf94f017a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/suo/miniconda3/envs/llama/lib/python3.9/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from tqdm.autonotebook import tqdm\n"
                    ]
                }
            ],
            "source": [
                "import pinecone"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "81fc463e",
            "metadata": {},
            "outputs": [],
            "source": [
                "pinecone.init(environment=\"eu-west1-gcp\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3500dd93",
            "metadata": {},
            "outputs": [],
            "source": [
                "# create index if it does not already exist\n",
                "# dimensions are for text-embedding-ada-002\n",
                "pinecone.create_index(\"quickstart-index\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "7b1544b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "pinecone_index = pinecone.Index(\"quickstart-index\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "027b8cae",
            "metadata": {},
            "source": [
                "# Use-Case 1: Compare and Contrast"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "26d56590",
            "metadata": {},
            "source": [
                "#### Load Dataset\n",
                "\n",
                "Fetch and load Wikipedia pages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c410eec0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
                        "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
                        "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
                        "NumExpr defaulting to 8 threads.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/suo/miniconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "from llama_index import SimpleDirectoryReader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "583afcf9",
            "metadata": {},
            "outputs": [],
            "source": [
                "wiki_titles = [\"Toronto\", \"Seattle\", \"San Francisco\", \"Chicago\", \"Boston\", \"Washington, D.C.\", \"Cambridge, Massachusetts\", \"Houston\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "540a39b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import requests\n",
                "\n",
                "data_path = Path('data_wiki')\n",
                "\n",
                "for title in wiki_titles:\n",
                "    response = requests.get(\n",
                "        'https://en.wikipedia.org/w/api.php',\n",
                "        params={\n",
                "            'action': 'query',\n",
                "            'format': 'json',\n",
                "            'titles': title,\n",
                "            'prop': 'extracts',\n",
                "            # 'exintro': True,\n",
                "            'explaintext': True,\n",
                "        }\n",
                "    ).json()\n",
                "    page = next(iter(response['query']['pages'].values()))\n",
                "    wiki_text = page['extract']\n",
                "\n",
                "    if not data_path.exists():\n",
                "        Path.mkdir(data_path)\n",
                "\n",
                "    with open(data_path / f\"{title}.txt\", 'w') as fp:\n",
                "        fp.write(wiki_text)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b802084",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load all wiki documents\n",
                "city_docs = {}\n",
                "all_docs = []\n",
                "for wiki_title in wiki_titles:\n",
                "    city_docs[wiki_title] = SimpleDirectoryReader(input_files=[data_path / f\"{wiki_title}.txt\"]).load_data()\n",
                "    all_docs.extend(city_docs[wiki_title])\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
            "metadata": {},
            "source": [
                "#### Build Indices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "0a2bcc07",
            "metadata": {},
            "outputs": [],
            "source": [
                "from llama_index import GPTVectorStoreIndex, StorageContext\n",
                "from llama_index.vector_stores import PineconeVectorStore\n",
                "from IPython.display import Markdown, display"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "ba1558b3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Building index for Toronto\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 20744 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 20744 tokens\n",
                        "Building index for Seattle\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 16942 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 16942 tokens\n",
                        "Building index for San Francisco\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 23433 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 23433 tokens\n",
                        "Building index for Chicago\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 26082 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 26082 tokens\n",
                        "Building index for Boston\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 18648 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 18648 tokens\n",
                        "Building index for Washington, D.C.\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 21649 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 21649 tokens\n",
                        "Building index for Cambridge, Massachusetts\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 12855 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 12855 tokens\n",
                        "Building index for Houston\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 21844 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 21844 tokens\n"
                    ]
                }
            ],
            "source": [
                "# Build index for each city document\n",
                "city_indices = {}\n",
                "index_summaries = {}\n",
                "for wiki_title in wiki_titles:\n",
                "    print(f\"Building index for {wiki_title}\")\n",
                "    # create storage context\n",
                "    vector_store = PineconeVectorStore(pinecone_index=pinecone_index, namespace=wiki_title)\n",
                "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
                "    \n",
                "    # build index\n",
                "    city_indices[wiki_title] = GPTVectorStoreIndex.from_documents(city_docs[wiki_title], storage_context=storage_context)\n",
                "\n",
                "    # set summary text for city\n",
                "    index_summaries[wiki_title] = f\"Wikipedia articles about {wiki_title}\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "04304299-fc3e-40a0-8600-f50c3292767e",
            "metadata": {},
            "source": [
                "#### Build Graph Query Engine for Compare & Contrast Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "35369eda",
            "metadata": {},
            "outputs": [],
            "source": [
                "from llama_index.indices.composability import ComposableGraph\n",
                "from llama_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "bedbb693-725f-478f-be26-fa7180ea38b2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
                    ]
                }
            ],
            "source": [
                "graph = ComposableGraph.from_indices(\n",
                "    GPTSimpleKeywordTableIndex,\n",
                "    [index for _, index in city_indices.items()], \n",
                "    [summary for _, summary in index_summaries.items()],\n",
                "    max_keywords_per_chunk=50\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "ba8ddcf0",
            "metadata": {},
            "outputs": [],
            "source": [
                "from llama_index.indices.query.query_transform.base import DecomposeQueryTransform\n",
                "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
                "\n",
                "decompose_transform = DecomposeQueryTransform(verbose=True)\n",
                "\n",
                "custom_query_engines = {}\n",
                "for wiki_title in wiki_titles:\n",
                "    index = city_indices[wiki_title]\n",
                "    query_engine = index.as_query_engine()\n",
                "    query_engine = TransformQueryEngine(\n",
                "        query_engine,\n",
                "        query_transform=decompose_transform,\n",
                "        transform_extra_info={'index_summary': index_summaries[wiki_title]},\n",
                "    )\n",
                "    custom_query_engines[index.index_id] = query_engine\n",
                "\n",
                "custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n",
                "    retriever_mode='simple',\n",
                "    response_mode='tree_summarize',\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "78235fbb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# with query decomposition in subindices\n",
                "query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "7dd3616c",
            "metadata": {},
            "source": [
                "#### Run Compare & Contrast Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "3f210a02",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.indices.keyword_table.retrievers:> Starting query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
                        "> Starting query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
                        "INFO:llama_index.indices.keyword_table.retrievers:query keywords: ['contrast', 'demographics', 'seattle', 'toronto', 'compare', 'houston']\n",
                        "query keywords: ['contrast', 'demographics', 'seattle', 'toronto', 'compare', 'houston']\n",
                        "INFO:llama_index.indices.keyword_table.retrievers:> Extracted keywords: ['seattle', 'toronto', 'houston']\n",
                        "> Extracted keywords: ['seattle', 'toronto', 'houston']\n",
                        "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
                        "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Seattle?\n",
                        "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                        "> [retrieve] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 7 tokens\n",
                        "> [retrieve] Total embedding token usage: 7 tokens\n",
                        "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
                        "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Seattle?\n",
                        "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1789 tokens\n",
                        "> [get_response] Total LLM token usage: 1789 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n",
                        "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
                        "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Toronto?\n",
                        "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                        "> [retrieve] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 7 tokens\n",
                        "> [retrieve] Total embedding token usage: 7 tokens\n",
                        "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
                        "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Toronto?\n",
                        "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1884 tokens\n",
                        "> [get_response] Total LLM token usage: 1884 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n",
                        "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
                        "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Houston?\n",
                        "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                        "> [retrieve] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 7 tokens\n",
                        "> [retrieve] Total embedding token usage: 7 tokens\n",
                        "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the demographics in Seattle, Houston, and Toronto.\n",
                        "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query:  What is the population of Houston?\n",
                        "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1780 tokens\n",
                        "> [get_response] Total LLM token usage: 1780 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 226 tokens\n",
                        "> [get_response] Total LLM token usage: 226 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 226 tokens\n",
                        "> [get_response] Total LLM token usage: 226 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n"
                    ]
                }
            ],
            "source": [
                "response = query_engine.query(\"Compare and contrast the demographics in Seattle, Houston, and Toronto.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "d24db9d4",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Seattle, Houston, and Toronto are all large cities with diverse populations. Seattle has the smallest population of the three cities, with 753,675 people as of 2021. Houston has the second largest population, with 2,304,580 people according to the 2020 U.S. census. Toronto has the largest population of the three cities, with 6,202,225 people in 2021. All three cities have a mix of different ethnicities and cultures, with Seattle having the most diverse population. Houston and Toronto have larger immigrant populations than Seattle. All three cities have a mix of different economic classes, with Toronto having the highest median household income."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from llama_index.response.notebook_utils import display_response\n",
                "\n",
                "display_response(response, show_source=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1a23412f",
            "metadata": {},
            "source": [
                "# Use-Case 2: Temporal Query"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ce276271",
            "metadata": {},
            "source": [
                "Temporal queries such as \"what happened after X\" is intuitive to humans, but can often confuse vector databases.  \n",
                "\n",
                "This is because the vector embedding will focus on the subject \"X\" rather than the imporant temporal cue. This results in irrelevant and misleading context that harms the final answer.  \n",
                "\n",
                "LlamaIndex solves this by explicitly maintainging node relationships and leverage LLM to automatically perform query expansion to find more relevant context.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "3150f9a5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n"
                    ]
                }
            ],
            "source": [
                "from llama_index import SimpleDirectoryReader, StorageContext, GPTVectorStoreIndex\n",
                "from llama_index.vector_stores import PineconeVectorStore\n",
                "\n",
                "\n",
                "# load documents\n",
                "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n",
                "\n",
                "# define storage context\n",
                "vector_store = PineconeVectorStore(pinecone_index=pinecone_index, namespace='pg_essay')\n",
                "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
                "\n",
                "# build index \n",
                "index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3dbe01aa",
            "metadata": {},
            "source": [
                "We can define an auto prev/next node postprocessor to leverage LLM reasoning to help query expansion (with relevant additional nodes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "56f2d0d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "from llama_index.indices.postprocessor.node import AutoPrevNextNodePostprocessor\n",
                "\n",
                "# define postprocessor\n",
                "node_postprocessor = AutoPrevNextNodePostprocessor(\n",
                "    docstore=index.storage_context.docstore, \n",
                "    service_context=index.service_context,\n",
                "    num_nodes=3,\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "# define query engine\n",
                "query_engine = index.as_query_engine(\n",
                "    similarity_top_k=1,\n",
                "    node_postprocessors=[node_postprocessor],\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "56896a41",
            "metadata": {},
            "source": [
                "#### Example 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "ffd6c81a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                        "> [retrieve] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 17 tokens\n",
                        "> [retrieve] Total embedding token usage: 17 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1153 tokens\n",
                        "> [get_response] Total LLM token usage: 1153 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1153 tokens\n",
                        "> [get_response] Total LLM token usage: 1153 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n",
                        "> Postprocessor Predicted mode: next\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 4413 tokens\n",
                        "> [get_response] Total LLM token usage: 4413 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n"
                    ]
                }
            ],
            "source": [
                "# Infer that we need to search nodes after current one\n",
                "response = query_engine.query(\n",
                "    \"What did the author do after handing off Y Combinator to Sam Altman?\", \n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "id": "f91a50f9",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** After handing off Y Combinator to Sam Altman, the author went on to found OpenAI, a research laboratory dedicated to artificial intelligence. He also wrote a book, \"The Launch Pad: Inside Y Combinator, Silicon Valley's Most Exclusive School for Startups,\" and became a partner at Founders Fund, a venture capital firm."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from llama_index.response.notebook_utils import display_response\n",
                "\n",
                "display_response(response)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "22434b2e",
            "metadata": {},
            "source": [
                "In comparison, naive top-k retrieval results in irrelevant context and hallucinated answer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "a93d2432",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                        "> [retrieve] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 17 tokens\n",
                        "> [retrieve] Total embedding token usage: 17 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1028 tokens\n",
                        "> [get_response] Total LLM token usage: 1028 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n"
                    ]
                },
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** After handing off Y Combinator to Sam Altman, the author went on to found OpenAI, a research laboratory dedicated to artificial intelligence. He also wrote a book, \"The Launch Pad: Inside Y Combinator, Silicon Valley's Most Exclusive School for Startups,\" and became a partner at Founders Fund, a venture capital firm."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/markdown": [
                            "---"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/markdown": [
                            "**`Source Node 1/1`**"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/markdown": [
                            "**Document ID:** dc9369a8-3012-45f3-baa0-6861e05bcda9<br>**Similarity:** 0.83942616<br>**Text:** in. We also noticed that the startups were becoming one another's customers. We used to refer jok...<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# define query engine\n",
                "naive_query_engine = index.as_query_engine(\n",
                "    similarity_top_k=1,\n",
                ")\n",
                "\n",
                "response = naive_query_engine.query(\n",
                "    \"What did the author do after handing off Y Combinator to Sam Altman?\", \n",
                ")\n",
                "\n",
                "display_response(response, show_source=True)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "64117464",
            "metadata": {},
            "source": [
                "#### Example 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "id": "221001e4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                        "> [retrieve] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 17 tokens\n",
                        "> [retrieve] Total embedding token usage: 17 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1131 tokens\n",
                        "> [get_response] Total LLM token usage: 1131 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1131 tokens\n",
                        "> [get_response] Total LLM token usage: 1131 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n",
                        "> Postprocessor Predicted mode: previous\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 4215 tokens\n",
                        "> [get_response] Total LLM token usage: 4215 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n"
                    ]
                }
            ],
            "source": [
                "# Infer that we need to search nodes before current one\n",
                "response = query_engine.query(\n",
                "    \"What did the author do before handing off Y Combinator to Sam Altman?\", \n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "id": "a6bf950f",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Before handing off Y Combinator to Sam Altman, the author wrote essays, worked on spam filters, painted, cooked for groups, worked on a new version of Arc, wrote a book, and worked on a new dialect of Lisp called Arc. He also gave talks, started angel investing, and founded Y Combinator. He also worked on projects with Robert and Trevor, and wrote essays online. He also gave a talk at a Lisp conference and wrote a postscript file of the talk online. He also organized a summer program for startups, funded Y Combinator with his own money, and used his building in Cambridge as the headquarters. He also held weekly dinners at the headquarters and tried to help the startups he funded."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "display_response(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7e7bbed0",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
