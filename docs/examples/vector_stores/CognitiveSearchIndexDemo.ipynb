{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Example\n",
    "\n",
    "In this basic example, we take the a Paul Graham essay, split it into chunks, embed it using an OpenAI embedding model, load it into an Azure Cognitive Search index, and then query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install llama-index\n",
    "#!{sys.executable} -m pip install azure-search-documents==11.4.0b8\n",
    "#!{sys.executable} -m pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    Document,\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.vector_stores import CognitiveSearchVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up OpenAI\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Azure Cognitive Search\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "search_service_name = getpass.getpass(\n",
    "    \"Azure Cognitive Search Service Name\"\n",
    ")  \n",
    "\n",
    "key = getpass.getpass(\n",
    "    \"Azure Cognitive Search Key\"\n",
    ")  \n",
    "\n",
    "cognitive_search_credential = AzureKeyCredential(key)\n",
    "\n",
    "service_endpoint = f\"https://{search_service_name}.search.windows.net\"\n",
    "\n",
    "index_name = \"quickstart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchField,\n",
    "    SemanticSettings,\n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    ")\n",
    "\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_and_create_index(index_name: str, service_endpoint: str, credential: Any):\n",
    "    index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
    "    if index_name in index_client.list_index_names():\n",
    "        print(f\"Index {index_name} exists, dropping index\")\n",
    "        index_client.delete_index(index_name)\n",
    "\n",
    "    create_search_index(index_name, service_endpoint, credential)\n",
    "\n",
    "\n",
    "def create_search_index(index_name: str, service_endpoint: str, credential: Any):\n",
    "    # if args.verbose: print(f\"Ensuring search index {args.index} exists\")\n",
    "    index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
    "    if index_name not in index_client.list_index_names():\n",
    "        index = SearchIndex(\n",
    "            name=index_name,\n",
    "            fields=[\n",
    "                SimpleField(name=\"id\", type=\"Edm.String\", key=True),\n",
    "                SearchableField(\n",
    "                    name=\"content\", type=\"Edm.String\", analyzer_name=\"en.microsoft\"\n",
    "                ),\n",
    "                SearchField(\n",
    "                    name=\"embedding\",\n",
    "                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    hidden=False,\n",
    "                    searchable=True,\n",
    "                    filterable=False,\n",
    "                    sortable=False,\n",
    "                    facetable=False,\n",
    "                    vector_search_dimensions=1536,\n",
    "                    vector_search_configuration=\"default\",\n",
    "                ),\n",
    "                SimpleField(name=\"li_jsonMetadata\", type=\"Edm.String\"),\n",
    "                SimpleField(name=\"li_doc_id\", type=\"Edm.String\", filterable=True),\n",
    "            ],\n",
    "            semantic_settings=SemanticSettings(\n",
    "                configurations=[\n",
    "                    SemanticConfiguration(\n",
    "                        name=\"default\",\n",
    "                        prioritized_fields=PrioritizedFields(\n",
    "                            title_field=None,\n",
    "                            prioritized_content_fields=[\n",
    "                                SemanticField(field_name=\"content\")\n",
    "                            ],\n",
    "                        ),\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            vector_search=VectorSearch(\n",
    "                algorithm_configurations=[\n",
    "                    HnswVectorSearchAlgorithmConfiguration(\n",
    "                        name=\"default\",\n",
    "                        kind=\"hnsw\",\n",
    "                        parameters={\n",
    "                            \"m\": 4,\n",
    "                            \"efConstruction\": 400,\n",
    "                            \"efSearch\": 1000,\n",
    "                            \"metric\": \"cosine\",\n",
    "                        },\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        print(f\"Creating {index_name} search index\")\n",
    "        index_client.create_index(index)\n",
    "    else:\n",
    "        print(f\"Search index {index_name} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index quickstart exists, dropping index\n",
      "Creating quickstart search index\n"
     ]
    }
   ],
   "source": [
    "# drop_and_create_index(index_name=index_name, service_endpoint=service_endpoint, credential=cognitive_search_credential)\n",
    "create_search_index(\n",
    "    index_name=index_name,\n",
    "    service_endpoint=service_endpoint,\n",
    "    credential=cognitive_search_credential,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding function\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"../../../examples/paul_graham_essay/data\"\n",
    ").load_data()\n",
    "\n",
    "# set up Azure Cognitive Search vector store and load in data\n",
    "search_client = SearchClient(\n",
    "    endpoint=service_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=cognitive_search_credential,\n",
    ")\n",
    "vector_store = CognitiveSearchVectorStore(\n",
    "    search_client,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"content\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    metadata_field_key=\"li_jsonMetadata\",\n",
    "    doc_id_field_key=\"li_doc_id\",\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer. They also built a microcomputer kit and started programming on it, writing simple games and a word processor.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query Data\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The author learned several things during their time at Interleaf. They learned that it's better for technology companies to be run by product people than sales people, that code edited by too many people leads to bugs, that cheap office space can be depressing, that planned meetings are inferior to corridor conversations, that big bureaucratic customers can be a dangerous source of money, and that there's not much overlap between conventional office hours and the optimal time for hacking.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What did the author learn?\",\n",
    ")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The author experienced a difficult moment when their mother had a stroke caused by colon cancer. They were determined to help their mother recover and spent a lot of time flying to Oregon to visit her. This event made the author reflect on their life and eventually led them to consider handing over Y Combinator to someone else.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"What was a hard moment for the author?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The author of the given context information is Paul Graham.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"Who is the author?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Interleaf, there was a group called Release Engineering that seemed to be as big as the group that actually wrote the software. The software at Interleaf had to be updated on the server, and there was a lot of emphasis on high production values to make the online store builders look legitimate.\n",
      "\n",
      "Streamed output at 20.981856143887274 tokens/s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "query_engine = index.as_query_engine(streaming=True)\n",
    "response = query_engine.query(\"What happened at interleaf?\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "token_count = 0\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")\n",
    "    token_count += 1\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "tokens_per_second = token_count / time_elapsed\n",
    "\n",
    "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The color of the sky can vary depending on various factors such as time of day, weather conditions, and location. It can range from shades of blue during the day to hues of orange, pink, and purple during sunrise or sunset.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"What colour is the sky?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.insert_nodes([Document(text=\"The sky is indigo today\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The colour of the sky is indigo.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"What colour is the sky?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filters are not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import TextNode\n",
    "\n",
    "nodes = [\n",
    "    TextNode(\n",
    "        text=\"The Shawshank Redemption\",\n",
    "        metadata={\n",
    "            \"author\": \"Stephen King\",\n",
    "            \"theme\": \"Friendship\",\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"The Godfather\",\n",
    "        metadata={\n",
    "            \"director\": \"Francis Ford Coppola\",\n",
    "            \"theme\": \"Mafia\",\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Inception\",\n",
    "        metadata={\n",
    "            \"director\": \"Christopher Nolan\",\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.insert_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\n",
    "\n",
    "\n",
    "filters = MetadataFilters(filters=[ExactMatchFilter(key=\"theme\", value=\"Mafia\")])\n",
    "\n",
    "retriever = index.as_retriever(filters=filters)\n",
    "retriever.retrieve(\"What is inception about?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindextest01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
