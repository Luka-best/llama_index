{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composable Objects\n",
    "\n",
    "In this notebook, we show how you can combine multiple objects into a single top-level index.\n",
    "\n",
    "This approach works by setting up `IndexNode` objects, with an `obj` field that points to a:\n",
    "- query engine\n",
    "- retriever\n",
    "- query pipeline\n",
    "- another node!\n",
    "\n",
    "```python\n",
    "object = IndexNode(index_id=\"my_object\", obj=query_engine, text=\"some text about this object\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"./llama2.pdf\"\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/1706.03762.pdf\" -O \"./attention.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "\n",
    "llama2_docs = PyMuPDFReader().load_data(\n",
    "    file_path=\"./llama2.pdf\", metadata=True\n",
    ")\n",
    "attention_docs = PyMuPDFReader().load_data(\n",
    "    file_path=\"./attention.pdf\", metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import TokenTextSplitter\n",
    "\n",
    "nodes = TokenTextSplitter(\n",
    "    chunk_size=1024, chunk_overlap=128\n",
    ").get_nodes_from_documents(llama2_docs + attention_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "\n",
    "index = VectorStoreIndex(nodes=nodes)\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing Objects\n",
    "\n",
    "Here, we construct the `IndexNodes`. Note that the text is what is used to index the node by the top-level index.\n",
    "\n",
    "For a vector index, the text is embedded, for a keyword index, the text is used for keywords.\n",
    "\n",
    "In this example, the `SummaryIndex` is used, which does not technically need the text for retrieval, since it always retrieves all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import IndexNode\n",
    "\n",
    "vector_obj = IndexNode(\n",
    "    index_id=\"vector\", obj=vector_retriever, text=\"Vector Retriever\"\n",
    ")\n",
    "bm25_obj = IndexNode(\n",
    "    index_id=\"bm25\", obj=bm25_retriever, text=\"BM25 Retriever\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryIndex\n",
    "\n",
    "summary_index = SummaryIndex(objects=[vector_obj, bm25_obj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying\n",
    "\n",
    "When we query, all objects will be retrieved and used to generate the nodes to get a final answer.\n",
    "\n",
    "Using `tree_summarize` with `aquery()` ensures concurrent execution and faster responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await query_engine.aquery(\n",
    "    \"How does attention work in transformers?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention in transformers works by mapping a query and a set of key-value pairs to an output. The output is computed as a weighted sum of the values, where the weights are determined by the similarity between the query and the keys. In the transformer model, attention is used in three different ways: \n",
      "\n",
      "1. Encoder-decoder attention: The queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.\n",
      "\n",
      "2. Self-attention in the encoder: Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "\n",
      "3. Self-attention in the decoder: Each position in the decoder can attend to all positions in the decoder up to and including that position. To preserve the auto-regressive property, leftward information flow in the decoder is prevented by masking out illegal connections.\n",
      "\n",
      "By using multi-head attention, the transformer model can jointly attend to information from different representation subspaces at different positions, improving its performance.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await query_engine.aquery(\n",
    "    \"What is the architecture of Llama2 based on?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The architecture of Llama 2 is based on the transformer model.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await query_engine.aquery(\n",
    "    \"What was used before attention in transformers?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recurrent neural networks, such as long short-term memory (LSTM) and gated recurrent neural networks (GRU), were commonly used before attention in transformers. These models were widely used for sequence modeling and transduction tasks like language modeling and machine translation.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
