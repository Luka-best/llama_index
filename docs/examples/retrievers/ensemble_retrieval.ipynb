{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf1de44-4047-46cf-a04c-dbf910d9e179",
   "metadata": {},
   "source": [
    "# Ensemble Query Engine Guide\n",
    "\n",
    "Oftentimes when building a RAG applications there are many retreival parameters/strategies to decide from (from chunk size to vector vs. keyword vs. hybrid search, for instance).\n",
    "\n",
    "Thought: what if we could try a bunch of strategies at once, and have any AI/reranker/LLM prune the results?\n",
    "\n",
    "This achieves two purposes:\n",
    "- Better (albeit more costly) retrieved results by pooling results from multiple strategies, assuming the reranker is good\n",
    "- A way to benchmark different retrieval strategies against each other (w.r.t reranker)\n",
    "\n",
    "This guide showcases this over the Llama 2 paper. We do ensemble retrieval over different chunk sizes and also different indices.\n",
    "\n",
    "**NOTE**: A closely related guide is our [Ensemble Retrievers Guide](https://gpt-index.readthedocs.io/en/stable/examples/retrievers/ensemble_retrieval.html) - make sure to check it out! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73fead-ec2c-4346-bd08-e183c13c7e29",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we define the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d59778-4cda-47b5-8cd0-b80fee91d1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c628448c-573c-4eeb-a7e1-707fe8cc575c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().handlers = []\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SummaryIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.response.notebook_utils import display_response\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787174ed-10ce-47d7-82fd-9ca7f891eea7",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "In this section we first load in the Llama 2 paper as a single document. We then chunk it multiple times, according to different chunk sizes. We build a separate vector index corresponding to each chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a003b8-1d4a-4faf-9402-46d5977bb28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dada361-4ac5-44a9-a29c-ae1aa8f5af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import Document\n",
    "from llama_hub.file.pymu_pdf.base import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed22d71-2dfb-4c77-9511-57166a3de6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "docs0 = loader.load(file_path=Path(\"./data/llama2.pdf\"))\n",
    "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "docs = [Document(text=doc_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e6b28-22ae-4af7-9a1d-b2dcd7fafa8f",
   "metadata": {},
   "source": [
    "Here we try out different chunk sizes: 128, 256, 512, and 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7081194a-ede7-478e-bff2-23e89e23ef16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 128\n",
      "Chunk Size: 256\n",
      "Chunk Size: 512\n",
      "Chunk Size: 1024\n"
     ]
    }
   ],
   "source": [
    "# initialize service context (set chunk size)\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "chunk_sizes = [128, 256, 512, 1024]\n",
    "service_contexts = []\n",
    "nodes_list = []\n",
    "vector_indices = []\n",
    "query_engines = []\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"Chunk Size: {chunk_size}\")\n",
    "    service_context = ServiceContext.from_defaults(chunk_size=chunk_size, llm=llm)\n",
    "    service_contexts.append(service_context)\n",
    "    nodes = service_context.node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "    # add chunk size to nodes to track later\n",
    "    for node in nodes:\n",
    "        node.metadata[\"chunk_size\"] = chunk_size\n",
    "        node.excluded_embed_metadata_keys = [\"chunk_size\"]\n",
    "        node.excluded_llm_metadata_keys = [\"chunk_size\"]\n",
    "\n",
    "    nodes_list.append(nodes)\n",
    "\n",
    "    # build vector index\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    vector_indices.append(vector_index)\n",
    "\n",
    "    # query engines\n",
    "    query_engines.append(vector_index.as_query_engine())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9ac92-0ab2-4306-88ca-48f7e06ee763",
   "metadata": {},
   "source": [
    "## Define Ensemble Retriever\n",
    "\n",
    "We setup an \"ensemble\" retriever primarily using our recursive retrieval abstraction. This works like the following:\n",
    "- Define a separate `IndexNode` corresponding to the vector retriever for each chunk size (retriever for chunk size 128, retriever for chunk size 256, and more)\n",
    "- Put all IndexNodes into a single `SummaryIndex` - when the corresponding retriever is called, *all* nodes are returned.\n",
    "- Define a Recursive Retriever, with the root node being the summary index retriever. This will first fetch all nodes from the summary index retriever, and then recursively call the vector retriever for each chunk size.\n",
    "- Rerank the final results.\n",
    "\n",
    "The end result is that all vector retrievers are called when a query is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fbca69b4-d8d5-4dcb-af33-f9ed4a91ec05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try ensemble retrieval\n",
    "\n",
    "from llama_index.tools import RetrieverTool\n",
    "from llama_index.schema import IndexNode\n",
    "\n",
    "# retriever_tools = []\n",
    "retriever_dict = {}\n",
    "retriever_nodes = []\n",
    "for chunk_size, vector_index in zip(chunk_sizes, vector_indices):\n",
    "    node_id = f\"chunk_{chunk_size}\"\n",
    "    node = IndexNode(\n",
    "        text=f\"Retrieves relevant context from the Llama 2 paper (chunk size {chunk_size})\",\n",
    "        index_id=node_id\n",
    "    )\n",
    "    retriever_nodes.append(node)\n",
    "    retriever_dict[node_id] = vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c1883-4a85-4908-872f-d32072bfe63a",
   "metadata": {},
   "source": [
    "Define recursive retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c9eaa6f-8f11-4380-b3c6-79092f17def3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.selectors.pydantic_selectors import PydanticMultiSelector\n",
    "# from llama_index.retrievers import RouterRetriever\n",
    "from llama_index.retrievers import RecursiveRetriever\n",
    "from llama_index import SummaryIndex\n",
    "\n",
    "# the derived retriever will just retrieve all nodes\n",
    "summary_index = SummaryIndex(retriever_nodes)\n",
    "\n",
    "retriever = RecursiveRetriever(\n",
    "    root_id=\"root\",\n",
    "    retriever_dict={\n",
    "        \"root\": summary_index.as_retriever(),\n",
    "        **retriever_dict\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8317e0-2249-4d83-9ff0-dbd6701e25ec",
   "metadata": {},
   "source": [
    "Let's test the retriever on a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c72c61c-d4f7-4159-bb80-1989468ab61c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes = await retriever.aretrieve(\n",
    "    \"Tell me about the main aspects of safety fine-tuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ed8bc-83ad-4851-9ec6-bfbbdf3ff38d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Number of nodes: {len(nodes)}')\n",
    "for node in nodes:\n",
    "    print(node.node.metadata[\"chunk_size\"])\n",
    "    print(node.node.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e39a9-b031-46bb-b835-e09ed7bec3ee",
   "metadata": {},
   "source": [
    "Define reranker to process the final retrieved set of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f26c527-17d2-4d4e-a6ee-8ea878ef8742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define reranker\n",
    "from llama_index.indices.postprocessor import (\n",
    "    LLMRerank,\n",
    "    SentenceTransformerRerank,\n",
    "    CohereRerank,\n",
    ")\n",
    "\n",
    "# reranker = LLMRerank()\n",
    "# reranker = SentenceTransformerRerank(top_n=10)\n",
    "reranker = CohereRerank(top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcbee6b-aa93-4144-95b7-f5542c9c689e",
   "metadata": {},
   "source": [
    "Define retriever query engine to integrate the recursive retriever + reranker together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "828589ef-d062-40dc-8a4b-245190769445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define RetrieverQueryEngine\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever, node_postprocessors=[reranker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53e3c341-e66d-4950-88d5-6411699d064b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Tell me about the main aspects of safety fine-tuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa680dd-03a0-4a76-b456-c4ef0136fdc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_response(\n",
    "    response, show_source=True, source_length=500, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850424f-84fc-4ea3-bb6a-18b4bc1d6dd5",
   "metadata": {},
   "source": [
    "### Analyzing the Relative Importance of each Chunk\n",
    "\n",
    "One interesting property of ensemble-based retrieval is that through reranking, we can actually use the ordering of chunks in the final retrieved set to determine the importance of each chunk size. For instance, if certain chunk sizes are always ranked near the top, then those are probably more relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a7a8303-be94-45c5-8bc5-13ec8c7f1694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute the average precision for each chunk size based on positioning in combined ranking\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def mrr_all(metadata_values, metadata_key, source_nodes):\n",
    "    # source nodes is a ranked list\n",
    "    # go through each value, find out positioning in source_nodes\n",
    "    value_to_mrr_dict = {}\n",
    "    for metadata_value in metadata_values:\n",
    "        mrr = 0\n",
    "        for idx, source_node in enumerate(source_nodes):\n",
    "            if source_node.node.metadata[metadata_key] == metadata_value:\n",
    "                mrr = 1 / (idx + 1)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # normalize AP, set in dict\n",
    "        value_to_mrr_dict[metadata_value] = mrr\n",
    "\n",
    "    df = pd.DataFrame(value_to_mrr_dict, index=[\"MRR\"])\n",
    "    df.style.set_caption(\"Mean Reciprocal Rank\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adebbb82-764e-4b45-933e-84bf4ad64d40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank for each Chunk Size\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>128</th>\n",
       "      <th>256</th>\n",
       "      <th>512</th>\n",
       "      <th>1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MRR</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         128   256   512   1024\n",
       "MRR  0.333333   1.0   0.5  0.25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the Mean Reciprocal Rank for each chunk size (higher is better)\n",
    "# we can see that chunk size of 256 has the highest ranked results.\n",
    "print(\"Mean Reciprocal Rank for each Chunk Size\")\n",
    "mrr_all(chunk_sizes, \"chunk_size\", response.source_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a2f3c-55ce-4fa6-a15a-be539723a967",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We more rigorously evaluate how well an ensemble retriever works compared to the \"baseline\" retriever.\n",
    "\n",
    "We define/load an eval benchmark dataset and then run different evaluations over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4d66b14-4f38-4b61-809c-f603d7e09ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    QueryResponseDataset,\n",
    ")\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b5a0d4e-7c0a-40f2-be5c-9dc1297483fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: run this if the dataset isn't already saved\n",
    "eval_service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-4\"))\n",
    "# generate questions from the largest chunks (1024)\n",
    "dataset_generator = DatasetGenerator(\n",
    "    nodes_list[-1],\n",
    "    service_context=eval_service_context,\n",
    "    show_progress=True,\n",
    "    num_questions_per_chunk=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b97355-de34-4840-a68f-4d137ab1b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = await dataset_generator.agenerate_dataset_from_nodes(num=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab72dc2-0d17-4925-83ca-a0630de28349",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.save_json(\"data/llama2_eval_qr_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7fd120e-36c2-4d20-8fca-d3e783756879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "eval_dataset = QueryResponseDataset.from_json(\"data/llama2_eval_qr_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f8b3a4-7824-4924-848f-fe8155291f80",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d4e2e1a-a7cf-471a-b786-2645cfb327c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0cbc18a-9cec-4c29-a5e7-c0ba3752118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    PairwiseComparisonEvaluator,\n",
    ")\n",
    "\n",
    "# NOTE: can uncomment other evaluators\n",
    "# evaluator_c = CorrectnessEvaluator(service_context=eval_service_context)\n",
    "evaluator_s = SemanticSimilarityEvaluator(service_context=eval_service_context)\n",
    "# evaluator_r = RelevancyEvaluator(service_context=eval_service_context)\n",
    "evaluator_f = FaithfulnessEvaluator(service_context=eval_service_context)\n",
    "\n",
    "pairwise_evaluator = PairwiseComparisonEvaluator(service_context=eval_service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a96ca32-9196-43e1-b82a-403becc2d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation.eval_utils import (\n",
    "    get_responses\n",
    ")\n",
    "from llama_index.evaluation import BatchEvalRunner\n",
    "\n",
    "eval_qs = eval_dataset.questions\n",
    "qr_pairs = eval_dataset.qr_pairs\n",
    "ref_response_strs = [r for (_, r) in qr_pairs]\n",
    "\n",
    "base_query_engine = query_engines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3a77d-b513-4df8-a0eb-8543d86eb8ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_pred_responses = get_responses(eval_qs[:max_samples], base_query_engine, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401e805-97d3-447a-8460-d23c664bbcb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_responses = get_responses(eval_qs[:max_samples], query_engine, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e00a7-d8cd-4f14-b5e3-d1e3a78303ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "023e8638-6d52-4d19-b0d8-99f43bfa5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pred_response_strs = [str(p) for p in pred_responses]\n",
    "base_pred_response_strs = [str(p) for p in base_pred_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "078f6459-c38d-4d3f-a53b-436b9d1d86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_dict = {\n",
    "    \"semantic_similarity\": evaluator_s,\n",
    "    \"faithfulness\": evaluator_f,\n",
    "}\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=1, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e88130b1-8938-4ba7-a91c-ee9c0e19a689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                | 0/60 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=998 request_id=f7af93706e3a6002ff0014375ba78555 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|█▏                                                                      | 1/60 [00:01<01:06,  1.13s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=50 request_id=7eadd9bb1920b956685e47c89e71f432 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=61 request_id=8ec6b09d9681fc88bc4ce846f4c70fdf response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  3%|██▍                                                                     | 2/60 [00:01<00:48,  1.20it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1118 request_id=13718b45400e9841812321c287e2bfd5 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  5%|███▌                                                                    | 3/60 [00:03<00:59,  1.04s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1004 request_id=bc0ec23b86b298d4d43486a164735653 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  7%|████▊                                                                   | 4/60 [00:04<01:01,  1.09s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1226 request_id=d75c481aa1df9c5ad15b4e1ddcde68ff response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  8%|██████                                                                  | 5/60 [00:05<01:09,  1.27s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=383f6e29973d8c524d2795d3efc79f28 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=47 request_id=c9fd347b916b1c8e54223adde19b63f4 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 10%|███████▏                                                                | 6/60 [00:06<01:00,  1.12s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=79 request_id=82412027958337c6dd49274429386dff response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=46 request_id=5641065649af247911a9028a23c1ecf1 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 12%|████████▍                                                               | 7/60 [00:07<00:47,  1.12it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=941 request_id=47885d9f8816b56089c95869ddead93c response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 13%|█████████▌                                                              | 8/60 [00:08<00:51,  1.02it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=34 request_id=3d5f455c84382c2eeca0842cc405564b response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=26 request_id=3845667fe8dcc50fc0c9fde65565ceaa response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 15%|██████████▊                                                             | 9/60 [00:08<00:40,  1.25it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=45 request_id=b5298c1648a8688670cf3c3b66af8b3e response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=33 request_id=8fa442cee8600b950536401956bd1a86 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 17%|███████████▊                                                           | 10/60 [00:08<00:33,  1.50it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=31 request_id=6a418d7c5cc3f099962a66fc3217f033 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=32 request_id=524f3cb446fe2e2b4021642f662e6194 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 18%|█████████████                                                          | 11/60 [00:09<00:28,  1.72it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=34 request_id=4371c989a5e5ae4d31846ea8fbe91674 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=44 request_id=614c6c5e65270983cd7c6df68515176d response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 20%|██████████████▏                                                        | 12/60 [00:09<00:24,  1.92it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=34 request_id=382a6aaffa5267d8e6831fe23e6d4ab2 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=56 request_id=50bcb6dae8a431dce0cea94bdf7ca07b response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 22%|███████████████▍                                                       | 13/60 [00:10<00:23,  1.98it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1032 request_id=e6cac7db17ff72d89b657335acee9994 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 23%|████████████████▌                                                      | 14/60 [00:11<00:34,  1.34it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1441 request_id=31919e13e72b082537790c658c03fd36 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 25%|█████████████████▊                                                     | 15/60 [00:13<00:45,  1.02s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=932 request_id=1dafba62c4f266333f3378d9a8376b29 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 27%|██████████████████▉                                                    | 16/60 [00:14<00:47,  1.07s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=26 request_id=29c31dae96f26558ff1646c320c3c87e response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=31 request_id=f61e33f4ef13b95c78dde5f3a5492b7d response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 28%|████████████████████                                                   | 17/60 [00:14<00:39,  1.08it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=64 request_id=a66b82f069a0048dcec93a99bbf9740a response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=68 request_id=6409a0fbe3910cc67bd7497d4ef5d0d4 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 30%|█████████████████████▎                                                 | 18/60 [00:15<00:33,  1.25it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=50 request_id=d400010b471f9534c55e10179af2a71f response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=36 request_id=59f3e5e42de4a369a59ecf6912caf13d response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 32%|██████████████████████▍                                                | 19/60 [00:15<00:28,  1.42it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=938 request_id=318601862133a8ce28c4515b512d4f33 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 33%|███████████████████████▋                                               | 20/60 [00:17<00:36,  1.09it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=857 request_id=4b8f40060c678f7fe75c3f84c62b1a26 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 35%|████████████████████████▊                                              | 21/60 [00:18<00:43,  1.13s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=38 request_id=810ad60bf437682844068253ffc29adf response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=32 request_id=12856f139dff50e2916a22cbb26b4b31 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 37%|██████████████████████████                                             | 22/60 [00:19<00:34,  1.09it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=50 request_id=0f259d59e0f9d4f4e5157fe3925779b6 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=39 request_id=c2bd822e6398effae0fee799dacc65bd response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 38%|███████████████████████████▏                                           | 23/60 [00:19<00:28,  1.29it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=983 request_id=2563d6e0874513b15fa727492e51f743 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 40%|████████████████████████████▍                                          | 24/60 [00:21<00:34,  1.06it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1233 request_id=d647479fca665aa9308846225b2076d2 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 42%|█████████████████████████████▌                                         | 25/60 [00:22<00:37,  1.07s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=46 request_id=8dab844464a881f883ebaa0b7cc45b89 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=77 request_id=0ae7c6cbd43de576fc142aa196ef9ea9 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████████████████▊                                        | 26/60 [00:22<00:29,  1.14it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=931 request_id=680b0590ab66d9b460642ba20b1ba901 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 45%|███████████████████████████████▉                                       | 27/60 [00:24<00:33,  1.03s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=921 request_id=874a4dbf19d8c83fdfc16bce6a485cd5 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████████████████████▏                                     | 28/60 [00:25<00:33,  1.05s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1016 request_id=09c4beb18e3d5d021b8857a10b72c2a4 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████████████████████▎                                    | 29/60 [00:26<00:34,  1.11s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=34 request_id=3eae089126e24d5eca63c7d08309631d response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=32 request_id=02fac29cf05c81c0d0614ae2c9ce6d38 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████████████████████▌                                   | 30/60 [00:27<00:26,  1.12it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=a17caf3120104af6173fe14bf6ef26e0 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=972 request_id=400bd696ca40573443703addd10bd769 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 52%|████████████████████████████████████▋                                  | 31/60 [00:32<01:03,  2.19s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=8f14ad93ae0c95712eb072a645067d70 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=26 request_id=8467172ab7fefd8156ce9a72282f1d8a response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 53%|█████████████████████████████████████▊                                 | 32/60 [00:32<00:45,  1.63s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=34 request_id=7490f22f4246600a17cb84eafd6b0fe3 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=f7cee4d6f93e6c4c0f04b866efae3c9f response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 55%|███████████████████████████████████████                                | 33/60 [00:33<00:33,  1.25s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=28c397ed08363a83ee83122556c944ed response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=cfb96145b2ec646181e830c40f352a49 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1567 request_id=2b859ed42e4fc2e24428cafd66488e4b response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 57%|████████████████████████████████████████▏                              | 34/60 [00:43<01:41,  3.89s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=42 request_id=bad240171a1d17066d8bee8d4ded6064 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=54 request_id=db5b0f53dcb803446f4d93142bd85b40 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 58%|█████████████████████████████████████████▍                             | 35/60 [00:43<01:11,  2.86s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=26 request_id=fb2e984a4f5075aabe6790371f69ca76 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=30 request_id=7c0c8cb69896346e2a0539ee8fb11694 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████████████████████████▌                            | 36/60 [00:43<00:50,  2.12s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=945 request_id=7f42073ed600d3a5be65370b8ca195d9 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████████████████████████▊                           | 37/60 [00:44<00:41,  1.81s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=76701281d4f006768898b1e45bf78b90 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=39c22a89c7baf4d9319eada0fe3a91c7 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=847 request_id=aa649018cb2db05e69983f797fb2e09e response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████████████████████████▉                          | 38/60 [00:54<01:29,  4.06s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=42 request_id=d9ff23d89add22db18a521289bd5be0d response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=39 request_id=4a0ed7638744f3c8ce3a56f62f8619c6 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 65%|██████████████████████████████████████████████▏                        | 39/60 [00:54<01:02,  2.96s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=f6161dcff7163926c177d658ed3e4ad9 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=869 request_id=2953a1fbd6b4fd74078eda00ceae1cd6 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 67%|███████████████████████████████████████████████▎                       | 40/60 [00:59<01:12,  3.63s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=f08113c27136b82248f9958f5d0620f2 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1007 request_id=6d31716dfb74f2fc741b92414092acf1 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 68%|████████████████████████████████████████████████▌                      | 41/60 [01:05<01:18,  4.12s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=44 request_id=73e8ccf79ae0eccff874c78dba823ef0 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=64 request_id=9d7aeac9320b50c7a6ac83e44278575b response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████████████████████████▋                     | 42/60 [01:05<00:54,  3.00s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=71 request_id=229f6261e823cabab7d607832aebcfda response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=36 request_id=e7058e23a4181d1d372178882ce8b972 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████████████████████████▉                    | 43/60 [01:06<00:38,  2.25s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=886 request_id=be8e8d5b83f55595846971059c20aad1 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████████████████████████████                   | 44/60 [01:07<00:30,  1.90s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=62a7d3ffda0bdc0b735269ad9a6df56b response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=60884e4a5dc9bac0d9e87a56e527e776 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1077 request_id=3e3c83d06a44df4c6a5ee18ffb9c1bb8 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████████████████████████████▎                 | 45/60 [01:16<01:02,  4.16s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=912f2c224f17f6617b8a90f8ff42f0b0 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=935 request_id=214bbd0e59a98ab1bf6410b9e326d487 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████████████████████████████▍                | 46/60 [01:21<01:03,  4.53s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=28 request_id=4471c16491e7cd8c8ea90c44003d848d response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=27 request_id=1a4636a76f230c9d73f725717c4a1846 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 78%|███████████████████████████████████████████████████████▌               | 47/60 [01:22<00:43,  3.31s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=89937c2b5f99b6b20981a20e3244260f response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=945 request_id=a47d60c4488e535750520daba1aa4051 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 80%|████████████████████████████████████████████████████████▊              | 48/60 [01:27<00:46,  3.89s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=6a553bbc899d047b8627fc0c1f9ac948 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=978 request_id=87eb44a01322289d692dfc4a96f75852 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 82%|█████████████████████████████████████████████████████████▉             | 49/60 [01:32<00:47,  4.32s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=c933d4e6a5826608effbf15cedd5eb89 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1016 request_id=2a4677729a2e25ccef7aeb044fa6492b response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████████████████████████████████▏           | 50/60 [01:38<00:46,  4.67s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=160e2db6de2b10fd81d0490d38ae7b53 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1322 request_id=0b09d474b7b8aa909058f413ee9baa90 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████████████████████████████████▎          | 51/60 [01:44<00:44,  4.95s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=29 request_id=4bd6655c7c3e198ca4547046825f8dcb response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=56 request_id=9f9e0e2c7ecac9c6b45664886f3a8dc2 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████████████████████████████████▌         | 52/60 [01:44<00:28,  3.58s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=39 request_id=bbef7d46f4b0d30214d1d2e5d8710b97 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=28 request_id=1b7b61b851d02aa59ea23ddeb5988e33 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 88%|██████████████████████████████████████████████████████████████▋        | 53/60 [01:44<00:18,  2.64s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=d66972daabee95da16fd6bb4301f6e99 response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=923 request_id=0d42c96883b585a6b624a0ce7841f0b2 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 90%|███████████████████████████████████████████████████████████████▉       | 54/60 [01:50<00:20,  3.40s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=d2b0fb9785d3b31143873a0b944755bd response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=33 request_id=db2d18ee0a07b932ec79afcda07dffc7 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 92%|█████████████████████████████████████████████████████████████████      | 55/60 [01:50<00:12,  2.49s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=e7e5b9f66aa34e30803f306fc3e523ab response_code=429\n",
      "error_code=rate_limit_exceeded error_message='Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues.' error_param=None error_type=tokens message='OpenAI API error received' stream_error=False\n",
      "Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-4 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min. Limit: 40000 / min. Please try again in 1ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=974 request_id=8a3b4f00e7982b82695fa5e1d4c2d293 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 93%|██████████████████████████████████████████████████████████████████▎    | 56/60 [01:56<00:13,  3.45s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=133 request_id=1b41f96be6939eb306114dbf1079117f response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=32 request_id=2c5620353a335dd2052da70bd5e0bed0 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████████████████████████████████████▍   | 57/60 [01:56<00:07,  2.56s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=29 request_id=94bc4197d472fdfecdfb7c7b10f60192 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=52 request_id=29ddb72c072b8d90844415e310a45fb9 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████████████████████████████████████▋  | 58/60 [01:56<00:03,  1.89s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=68 request_id=d29a5a64c290727c1b8c98ab216782f7 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=41 request_id=d43570eb0186f99f61be96643eba6f24 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████████████████████████████████████▊ | 59/60 [01:57<00:01,  1.44s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=50 request_id=7070d0da0a98488d4c65776fa219bed9 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=34 request_id=b9ca0382e269fbf347c21d9d684bdd72 response_code=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████| 60/60 [01:57<00:00,  1.96s/it]\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples], responses=pred_responses[:max_samples], reference=ref_response_strs[:max_samples]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8becc3de-2530-4e64-bafe-180fbd64a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs[:max_samples], responses=base_pred_responses[:max_samples], reference=ref_response_strs[:max_samples]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa47ef-df96-4e58-8960-92d60091d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_runner = BatchEvalRunner({\"pairwise\": pairwise_evaluator}, workers=3, show_progress=True)\n",
    "\n",
    "pairwise_eval_results = await batch_runner.aevaluate_response_strs(\n",
    "    queries=eval_qs[:max_samples], response_strs=pred_response_strs[:max_samples], reference=base_pred_response_strs[:max_samples]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "157949ac-be2f-4456-bac0-58654d691ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def display_results(eval_results_list, names, metric_keys):\n",
    "    metric_dict = defaultdict(list)\n",
    "    metric_dict[\"names\"] = names\n",
    "    for metric_key in metric_keys:\n",
    "        for eval_results in eval_results_list:\n",
    "            mean_score = np.array([r.score for r in eval_results[metric_key]]).mean()\n",
    "            metric_dict[metric_key].append(mean_score)\n",
    "    return pd.DataFrame(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9153a9f-109c-437e-a860-9e2346859659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ensemble Retriever</td>\n",
       "      <td>0.918733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Base Retriever</td>\n",
       "      <td>0.925547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                names  semantic_similarity\n",
       "0  Ensemble Retriever             0.918733\n",
       "1      Base Retriever             0.925547"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_results([eval_results, base_eval_results], [\"Ensemble Retriever\", \"Base Retriever\"], [\"semantic_similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f5f1c839-ac5e-4458-af05-c6ed4a2db7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>pairwise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pairwise Comparison</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 names  pairwise\n",
       "0  Pairwise Comparison  0.583333"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_results([pairwise_eval_results], [\"Pairwise Comparison\"], [\"pairwise\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
