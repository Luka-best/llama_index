{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf1de44-4047-46cf-a04c-dbf910d9e179",
   "metadata": {},
   "source": [
    "# Router Retriever\n",
    "In this guide, we define a custom router retriever that selects one or more candidate retrievers in order to execute a given query.\n",
    "\n",
    "The router (`BaseSelector`) module uses the LLM to dynamically make decisions on which underlying retrieval tools to use. This can be helpful to select one out of a diverse range of data sources. This can also be helpful to aggregate retrieval results across a variety of data sources (if a multi-selector module is used).\n",
    "\n",
    "This notebook is very similar to the RouterQueryEngine notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73fead-ec2c-4346-bd08-e183c13c7e29",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d59778-4cda-47b5-8cd0-b80fee91d1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c628448c-573c-4eeb-a7e1-707fe8cc575c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().handlers = []\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ListIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    SimpleKeywordTableIndex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787174ed-10ce-47d7-82fd-9ca7f891eea7",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "We first show how to convert a Document into a set of Nodes, and insert into a DocumentStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc1b8ac-bf55-4d60-841c-61698663322f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7081194a-ede7-478e-bff2-23e89e23ef16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize service context (set chunk size)\n",
    "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f61bca2-c3b4-4ef0-a8f1-367933aa6d05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f5c44f-11d2-47a2-a566-c6dc0fd5a1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define \n",
    "list_index = ListIndex(nodes, storage_context=storage_context)\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "keyword_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d6162df-9da7-4aad-a2ca-eb318f67daec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_retriever = list_index.as_retriever()\n",
    "vector_retriever = vector_index.as_retriever()\n",
    "keyword_retriever = keyword_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee3f7c3b-69b4-48d5-bf22-ac51a4e3179f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.tools import RetrieverTool\n",
    "\n",
    "list_tool = RetrieverTool.from_defaults(\n",
    "    retriever=list_retriever,\n",
    "    description=\"Useful for summarization questions related to Paul Graham eassy on What I Worked On.\",\n",
    ")\n",
    "vector_tool = RetrieverTool.from_defaults(\n",
    "    retriever=vector_retriever,\n",
    "    description=\"Useful for retrieving specific context from Paul Graham essay on What I Worked On.\",\n",
    ")\n",
    "keyword_tool = RetrieverTool.from_defaults(\n",
    "    retriever=keyword_retriever,\n",
    "    description=\"Useful for retrieving specific context using keywords from Paul Graham essay on What I Worked On.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba2d68-13f9-4519-87ec-40511da7abdd",
   "metadata": {},
   "source": [
    "### Define Selector Module for Routing\n",
    "\n",
    "There are several selectors available, each with some distinct attributes.\n",
    "\n",
    "The LLM selectors use the LLM to output a JSON that is parsed, and the corresponding indexes are queried.\n",
    "\n",
    "The Pydantic selectors (currently only supported by `gpt-4-0613` and `gpt-3.5-turbo-0613` (the default)) use the OpenAI Function Call API to produce pydantic selection objects, rather than parsing raw JSON.\n",
    "\n",
    "Here we use PydanticSingleSelector/PydanticMultiSelector but you can use the LLM-equivalents as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb64a55-05b7-4565-949b-025b8d19c375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.selectors.llm_selectors import LLMSingleSelector, LLMMultiSelector\n",
    "from llama_index.selectors.pydantic_selectors import (\n",
    "    PydanticMultiSelector,\n",
    "    PydanticSingleSelector,\n",
    ")\n",
    "from llama_index.retrievers import RouterRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513ca57-bef9-47d3-aa17-3cf72a6eb318",
   "metadata": {},
   "source": [
    "#### PydanticSingleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ecb1c95-0096-4036-ad32-2337d844bf68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = RouterRetriever(\n",
    "    selector=PydanticSingleSelector.from_defaults(),\n",
    "    retriever_tools=[\n",
    "        list_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b8c4c12-1a30-425e-8312-04be050b2101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the summary of the document?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/indices/base_retriever.py:22\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     21\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/retrievers/router_retriever.py:73\u001b[0m, in \u001b[0;36mRouterRetriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_retrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m     70\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[1;32m     71\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[1;32m     72\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m---> 73\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39minds) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     76\u001b[0m             results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/selectors/types.py:76\u001b[0m, in \u001b[0;36mBaseSelector.select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m     74\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [_wrap_choice(choice) \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m choices]\n\u001b[1;32m     75\u001b[0m query_bundle \u001b[38;5;241m=\u001b[39m _wrap_query(query)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/selectors/pydantic_selectors.py:66\u001b[0m, in \u001b[0;36mPydanticSingleSelector._select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m     63\u001b[0m choices_text \u001b[38;5;241m=\u001b[39m _build_choices_text(choices)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector_program\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_choices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchoices_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# parse output\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _pydantic_output_to_selector_result(prediction)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/program/openai_program.py:101\u001b[0m, in \u001b[0;36mOpenAIPydanticProgram.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     99\u001b[0m openai_fn_spec \u001b[38;5;241m=\u001b[39m to_openai_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_cls)\n\u001b[0;32m--> 101\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mChatMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessageRole\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUSER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatted_prompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mopenai_fn_spec\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m message \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39madditional_kwargs:\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/llms/base.py:134\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_llm_chat\u001b[39m(_self: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self) \u001b[38;5;28;01mas\u001b[39;00m callback_manager:\n\u001b[1;32m    133\u001b[0m         event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[0;32m--> 134\u001b[0m             CBEventType\u001b[38;5;241m.\u001b[39mLLM, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mMESSAGES: \u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m}\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m         f_return_val \u001b[38;5;241m=\u001b[39m f(_self, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "retriever.retrieve(\"What is the summary of the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749c34e-97c0-4bd5-8358-377a94b8b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.retrieve(\"What did Paul Graham do after RISD?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae962a0-55c3-42e4-8f90-8332499952b5",
   "metadata": {},
   "source": [
    "#### PydanticMultiSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93cd132-fa4d-431f-9b02-0fc7482f097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RouterRetriever(\n",
    "    selector=PydanticMultiSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        list_tool,\n",
    "        vector_tool,\n",
    "        keyword_tool\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b877dc-50d9-4841-9747-d902a60b767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.retrieve(\n",
    "    \"What were noteable events and people from the authors time at Interleaf and YC?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
