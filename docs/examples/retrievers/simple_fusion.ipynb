{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Fusion Retriever\n",
    "\n",
    "In this example, we walk through how you can combine retrieval results from multiple queries and multiple indexes. \n",
    "\n",
    "The retrieved nodes will be returned as the top-k across all queries and indexes, as well as handling de-duplication of any nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this notebook, we will use two very similar pages of our documentation, each stored in a separaete index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File ../../core_modules/data_modules/storage/vector_stores.md does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDirectoryReader\n\u001b[1;32m      3\u001b[0m documents_1 \u001b[38;5;241m=\u001b[39m SimpleDirectoryReader(\n\u001b[1;32m      4\u001b[0m     input_files\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../community/integrations/vector_stores.md\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m )\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m----> 6\u001b[0m documents_2 \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleDirectoryReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../core_modules/data_modules/storage/vector_stores.md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mload_data()\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-core/llama_index/core/readers/file/base.py:155\u001b[0m, in \u001b[0;36mSimpleDirectoryReader.__init__\u001b[0;34m(self, input_dir, input_files, exclude, exclude_hidden, errors, recursive, encoding, filename_as_id, required_exts, file_extractor, num_files_limit, file_metadata)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m input_files:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path):\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m     input_file \u001b[38;5;241m=\u001b[39m Path(path)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_files\u001b[38;5;241m.\u001b[39mappend(input_file)\n",
      "\u001b[0;31mValueError\u001b[0m: File ../../core_modules/data_modules/storage/vector_stores.md does not exist."
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents_1 = SimpleDirectoryReader(\n",
    "    input_files=[\"../../community/integrations/vector_stores.md\"]\n",
    ").load_data()\n",
    "documents_2 = SimpleDirectoryReader(\n",
    "    input_files=[\"../../module_guides/storing/vector_stores.md\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index_1 = VectorStoreIndex.from_documents(documents_1)\n",
    "index_2 = VectorStoreIndex.from_documents(documents_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuse the Indexes!\n",
    "\n",
    "In this step, we fuse our indexes into a single retriever. This retriever will also generate augment our query by generating extra queries related to the original question, and aggregate the results.\n",
    "\n",
    "This setup will query 4 times, once with your original query, and generate 3 more queries.\n",
    "\n",
    "By default, it uses the following prompt to generate extra queries:\n",
    "\n",
    "```python\n",
    "QUERY_GEN_PROMPT = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [index_1.as_retriever(), index_2.as_retriever()],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=4,  # set this to 1 to disable query generation\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply nested async to run in a notebook\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What are the steps to set up a chroma vector store?\n",
      "2. Best practices for setting up a chroma vector store\n",
      "3. Troubleshooting common issues when setting up a chroma vector store\n"
     ]
    }
   ],
   "source": [
    "nodes_with_scores = retriever.retrieve(\"How do I setup a chroma vector store?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.81 - construct vector store\n",
      "neo4j_vector = Neo4jVectorStore(\n",
      "    username=\"neo4j\",\n",
      "    password=\"pleasele...\n",
      "Score: 0.80 - construct vector store\n",
      "vector_store = ChromaVectorStore(\n",
      "    chroma_collection=chroma_collection,\n",
      ")\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for node in nodes_with_scores:\n",
    "    print(f\"Score: {node.score:.2f} - {node.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use in a Query Engine!\n",
    "\n",
    "Now, we can plug our retriever into a query engine to synthesize natural language responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. How to set up a chroma vector store?\n",
      "2. Step-by-step guide for creating a chroma vector store.\n",
      "3. Examples of chroma vector store setups and configurations.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How do I setup a chroma vector store? Can you give an example?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** To set up a Chroma Vector Store, you can use the `ChromaVectorStore` class from the `llama_index.vector_stores` module. Here is an example of how to set it up:\n",
       "\n",
       "```python\n",
       "from llama_index.vector_stores import ChromaVectorStore\n",
       "\n",
       "# Assuming you have a chroma_collection variable\n",
       "vector_store = ChromaVectorStore(\n",
       "    chroma_collection=chroma_collection,\n",
       ")\n",
       "```\n",
       "\n",
       "This code creates an instance of the `ChromaVectorStore` class, passing in the `chroma_collection` as a parameter."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "display_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v3",
   "language": "python",
   "name": "llama_index_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
