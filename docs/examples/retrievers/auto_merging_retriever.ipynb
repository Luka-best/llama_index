{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5325ac27-38ea-47aa-afef-be4ec4f8f4b9",
   "metadata": {},
   "source": [
    "# Auto Merging Retriever\n",
    "\n",
    "In this notebook, we showcase our `AutoMergingRetriever`, which looks at a set of leaf nodes and recursively \"merges\" subsets of leaf nodes that reference a parent node beyond a given threshold. This allows us to consolidate potentially disparate, smaller contexts into a larger context that might help synthesis.\n",
    "\n",
    "You can define this hierarchy yourself over a set of documents, or you can make use of our brand-new text parser: a HierarchicalNodeParser that takes in a candidate set of documents and outputs an entire hierarchy of nodes, from \"coarse-to-fine\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1316ac-84ca-41d0-80f9-d4ef758e653c",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Let's first load the Llama 2 paper: https://arxiv.org/pdf/2307.09288.pdf. This will be our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80372299-ab32-4ddd-9b88-05c877120c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-26 20:54:41--  https://arxiv.org/pdf/2307.09288.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 128.84.21.199\n",
      "Connecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘data/llama2.pdf’\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M  31.9KB/s    in 4m 44s  \n",
      "\n",
      "2023-08-26 20:59:27 (47.0 KB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5f9c5d99-bd0e-4b26-b816-9f5ad29df3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_hub.file.pdf.base import PDFReader\n",
    "from llama_hub.file.unstructured.base import UnstructuredReader\n",
    "from llama_hub.file.deepdoctection.base import DeepDoctectionReader\n",
    "from llama_hub.file.pdf_miner.base import PDFMinerReader\n",
    "from llama_index import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "723f1f02-2157-4166-b013-90e627c76530",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFReader()\n",
    "docs0 = loader.load_data(file=Path('./data/llama2.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a8552-f347-45b0-b4a0-4f9b32be57ac",
   "metadata": {},
   "source": [
    "By default, the PDF reader creates a separate doc for each page.\n",
    "For the sake of this notebook, we stitch docs together into one doc. \n",
    "This will help us better highlight auto-merging capabilities that \"stitch\" chunks together later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a75c4217-ab50-417f-a8ed-3b746a9956c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "docs = [Document(text=doc_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fe6f1-80e1-4ac5-bd99-8b9b8d15bddd",
   "metadata": {},
   "source": [
    "## Parse Chunk Hierarchy from Text, Load into Storage\n",
    "\n",
    "In this section we make use of the `HierarchicalNodeParser`. This will output a set of nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45e783f5-a323-4f51-ae9a-4b71b00e5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import HierarchicalNodeParser, SimpleNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c3947df-25c2-4254-a3d4-381d136f3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to parse nodes\n",
    "\n",
    "node_parser = HierarchicalNodeParser.from_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2162b309-dfc5-484b-a31c-24f705316f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9b5bc9b-389d-47db-a41c-3eb5b3d38ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7299ca7e-09b6-432f-a277-aae9eca0522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.schema import NodeRelationship\n",
    "def get_leaf_nodes(nodes: List) -> List:\n",
    "    \"\"\"Get leaf nodes.\"\"\"\n",
    "    leaf_nodes = []\n",
    "    for node in nodes:\n",
    "        if NodeRelationship.CHILD not in node.relationships:\n",
    "            leaf_nodes.append(node)\n",
    "    return leaf_nodes\n",
    "\n",
    "\n",
    "def get_non_leaf_nodes(nodes: List) -> List:\n",
    "    \"\"\"Get non_leaf nodes.\"\"\"\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "    leaf_node_ids = {n.node_id for n in leaf_nodes}\n",
    "\n",
    "    non_leaf_nodes = []\n",
    "    for node in nodes:\n",
    "        if node.node_id not in leaf_node_ids:\n",
    "            non_leaf_nodes.append(node)\n",
    "    return non_leaf_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "faeb37a8-aea9-4ee8-b6c0-3b2f188d244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_nodes = get_leaf_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c33b5a8-4d9f-481e-8616-fc8717900159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "783"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaf_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15a6fcb0-e098-4695-b549-c4ff46c26c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(2):\n",
    "#     print(leaf_nodes[i].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ec940-8af7-45f5-9994-919d57583c24",
   "metadata": {},
   "source": [
    "### Load into Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27c8f2cd-3e04-4feb-937b-b9ee33e1c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define storage context\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage import StorageContext\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "\n",
    "# insert nodes into docstore\n",
    "docstore.add_documents(nodes)\n",
    "\n",
    "# define storage context (will include vector store by default too)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=docstore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "827ece8e-7a4b-4ee1-8ee2-3433d7f2072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load index into vector index\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "base_index = VectorStoreIndex(leaf_nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d84c19-c9ac-4294-a000-264c3c02427b",
   "metadata": {},
   "source": [
    "## Define Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e61682a0-dd3c-400b-8734-35d5d0a98252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.auto_merging_retriever import AutoMergingRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f96fd0bc-c6c0-4073-a692-d1803cf4289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever = base_index.as_retriever(similarity_top_k=6)\n",
    "retriever = AutoMergingRetriever(\n",
    "    base_retriever,\n",
    "    storage_context,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "62f655cd-4195-4398-80e5-5aa561982d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Filling in node. Node id: 3b256786-958e-4626-9a02-ca9d00aace53> Node text: andusethedataforsupervisedfine-tuninginthesame\n",
      "manner as described in Section 3.1.An example can ...\n",
      "\n",
      "> Merging 3 nodes into parent node.\n",
      "> Parent node id: 7d852fbe-3928-4ff1-9be9-8f80075d0102.\n",
      "> Parent node text: We also ask the annotators to avoid negative user experience\n",
      "categories (see Appendix A.5.2).The ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query_str = \"What were some lessons learned from red-teaming?\"\n",
    "query_str = \"Can you tell me about the key concepts for safety finetuning\"\n",
    "# query_str = \"Give me details on how safety violations were measured for Llama 2\"\n",
    "\n",
    "nodes = retriever.retrieve(query_str)\n",
    "base_nodes = base_retriever.retrieve(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5f8019cb-6a76-473a-b24c-c96208fa64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node = docstore.get_document('fb24e04e-9544-4659-bf82-1c90d5540b7e')\n",
    "# print(node.get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "77eabc56-2009-4504-8832-b6d857bd43a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0d482b22-fd38-476b-821f-0c77564815c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** dec3035d-a5b6-4edd-acd1-5745499380c1<br>**Similarity:** 0.8706509346624604<br>**Text:** we describe our approach to safety fine-tuning, including safety categories, annotation\n",
       "guidelines,andthetechniquesweusetomitigatesafetyrisks.Weemployaprocesssimilartothegeneral\n",
       "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.Specifically,<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 05d93601-1b11-4892-be36-f8507d8eb600<br>**Similarity:** 0.862506098281326<br>**Text:** we use the following techniques in safety fine-tuning:\n",
       "1.Supervised Safety Fine-Tuning : We initialize by gathering adversarial prompts and safe demonstra-\n",
       "tions that are then included in the general supervised fine-tuning process (Section 3.1).This teaches\n",
       "themodeltoalignwithoursafetyguidelinesevenbeforeRLHF,andthuslaysthefoundationfor\n",
       "high-quality human preference data annotation.2.Safety RLHF : Subsequently,<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** cbc408f8-1fa2-4653-81d3-7e66d1b688f2<br>**Similarity:** 0.8459963570749429<br>**Text:** 7\n",
       "3 Fine-tuning 8\n",
       "3.1 Supervised Fine-Tuning (SFT) .9\n",
       "3.2 Reinforcement Learning with Human Feedback (RLHF) .9\n",
       "3.3 System Message for Multi-Turn Consistency .16\n",
       "3.4 RLHF Results .17\n",
       "4 Safety 20\n",
       "4.1 Safety in Pretraining .20\n",
       "4.2 Safety Fine-Tuning .23\n",
       "4.3 Red Teaming .28\n",
       "4.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 2fd5840f-0c57-46c5-9ba5-ade16d3c7322<br>**Similarity:** 0.831324881683882<br>**Text:** thispapercontributesathoroughdescriptionofourfine-tuningmethodologyandapproachtoimproving\n",
       "LLM safety.We hope that this openness will enable the community to reproduce fine-tuned LLMs and\n",
       "continue to improve the safety of those models, paving the way for more responsible development of LLMs.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 7d852fbe-3928-4ff1-9be9-8f80075d0102<br>**Similarity:** 0.8248377708758811<br>**Text:** We also ask the annotators to avoid negative user experience\n",
       "categories (see Appendix A.5.2).The guidelines are meant to be a general guide for the model and are\n",
       "iteratively refined and revised to include newly identified risks.4.2.2 Safety Supervised Fine-Tuning\n",
       "InaccordancewiththeestablishedguidelinesfromSection4.2.1,wegatherpromptsanddemonstrations\n",
       "ofsafemodelresponsesfromtrainedannotators,andusethedataforsupervisedfine-tuninginthesame\n",
       "manner as described in Section 3.1.An example can be found in Table 5.The annotators are instructed to initially come up with prompts that they think could potentially induce\n",
       "themodel toexhibit unsafebehavior, i.e.perform redteaming, asdefined bythe guidelines.Subsequently,\n",
       "annotators are tasked with crafting a safe and helpful response that the model should produce.4.2.3 Safety RLHF\n",
       "Weobserveearlyinthedevelopmentof Llama 2-Chat thatitisabletogeneralizefromthesafedemonstrations\n",
       "insupervisedfine-tuning.Themodelquicklylearnstowritedetailedsaferesponses,addresssafetyconcerns,\n",
       "explainwhythetopicmightbesensitive,andprovideadditionalhelpfulinformation.Inparticular,when\n",
       "the model outputs safe responses, they are often more detailed than what the average annotator writes.Therefore, after gathering only a few thousand supervised demonstrations, we switched entirely to RLHF to\n",
       "teachthemodelhowtowritemorenuancedresponses.ComprehensivetuningwithRLHFhastheadded\n",
       "benefit that it may make the model more robust to jailbreak attempts (Bai et al. 2022a).WeconductRLHFbyfirstcollectinghumanpreferencedataforsafetysimilartoSection3.2.2: annotators\n",
       "writeapromptthattheybelievecanelicitunsafebehavior,andthencomparemultiplemodelresponsesto\n",
       "theprompts,selectingtheresponsethatissafestaccordingtoasetofguidelines.Wethenusethehuman\n",
       "preference data to train a safety reward model (see Section 3.2.2),<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in nodes:\n",
    "    display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e4dd58db-4b12-49dc-b42f-8a0ee746f5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** dec3035d-a5b6-4edd-acd1-5745499380c1<br>**Similarity:** 0.8706509346624604<br>**Text:** we describe our approach to safety fine-tuning, including safety categories, annotation\n",
       "guidelines,andthetechniquesweusetomitigatesafetyrisks.Weemployaprocesssimilartothegeneral\n",
       "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.Specifically,<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 05d93601-1b11-4892-be36-f8507d8eb600<br>**Similarity:** 0.862506098281326<br>**Text:** we use the following techniques in safety fine-tuning:\n",
       "1.Supervised Safety Fine-Tuning : We initialize by gathering adversarial prompts and safe demonstra-\n",
       "tions that are then included in the general supervised fine-tuning process (Section 3.1).This teaches\n",
       "themodeltoalignwithoursafetyguidelinesevenbeforeRLHF,andthuslaysthefoundationfor\n",
       "high-quality human preference data annotation.2.Safety RLHF : Subsequently,<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** cbc408f8-1fa2-4653-81d3-7e66d1b688f2<br>**Similarity:** 0.8459963570749429<br>**Text:** 7\n",
       "3 Fine-tuning 8\n",
       "3.1 Supervised Fine-Tuning (SFT) .9\n",
       "3.2 Reinforcement Learning with Human Feedback (RLHF) .9\n",
       "3.3 System Message for Multi-Turn Consistency .16\n",
       "3.4 RLHF Results .17\n",
       "4 Safety 20\n",
       "4.1 Safety in Pretraining .20\n",
       "4.2 Safety Fine-Tuning .23\n",
       "4.3 Red Teaming .28\n",
       "4.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 2fd5840f-0c57-46c5-9ba5-ade16d3c7322<br>**Similarity:** 0.831324881683882<br>**Text:** thispapercontributesathoroughdescriptionofourfine-tuningmethodologyandapproachtoimproving\n",
       "LLM safety.We hope that this openness will enable the community to reproduce fine-tuned LLMs and\n",
       "continue to improve the safety of those models, paving the way for more responsible development of LLMs.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 1f0dac7d-1b36-4a84-aa00-f172415b8460<br>**Similarity:** 0.8265816530219178<br>**Text:** We also ask the annotators to avoid negative user experience\n",
       "categories (see Appendix A.5.2).The guidelines are meant to be a general guide for the model and are\n",
       "iteratively refined and revised to include newly identified risks.4.2.2 Safety Supervised Fine-Tuning\n",
       "InaccordancewiththeestablishedguidelinesfromSection4.2.1,wegatherpromptsanddemonstrations\n",
       "ofsafemodelresponsesfromtrainedannotators,<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** eaa3d730-968c-4bd6-9521-1452ddcac01f<br>**Similarity:** 0.8230938887298445<br>**Text:** 3 Safety RLHF\n",
       "Weobserveearlyinthedevelopmentof Llama 2-Chat thatitisabletogeneralizefromthesafedemonstrations\n",
       "insupervisedfine-tuning.Themodelquicklylearnstowritedetailedsaferesponses,addresssafetyconcerns,\n",
       "explainwhythetopicmightbesensitive,andprovideadditionalhelpfulinformation.Inparticular,when\n",
       "the model outputs safe responses,<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for node in base_nodes:\n",
    "    display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f62e2c-4def-402e-8904-47f34d12c2fb",
   "metadata": {},
   "source": [
    "## Plug it into Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5d3ce9ec-f6cd-475b-94fa-3e8df81ab824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f106e1bb-58bc-48bf-a46b-e527339f83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(retriever)\n",
    "base_query_engine = RetrieverQueryEngine.from_args(base_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "94a85854-ca04-41ed-9f44-b6dce1e513e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Filling in node. Node id: 3b256786-958e-4626-9a02-ca9d00aace53> Node text: andusethedataforsupervisedfine-tuninginthesame\n",
      "manner as described in Section 3.1.An example can ...\n",
      "\n",
      "> Merging 3 nodes into parent node.\n",
      "> Parent node id: 7d852fbe-3928-4ff1-9be9-8f80075d0102.\n",
      "> Parent node text: We also ask the annotators to avoid negative user experience\n",
      "categories (see Appendix A.5.2).The ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8b334b7b-fcb8-4057-a418-b8d8c425ad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key concepts for safety fine-tuning include supervised safety fine-tuning and safety RLHF (Reinforcement Learning with Human Feedback). In supervised safety fine-tuning, adversarial prompts and safe demonstrations are gathered and included in the general supervised fine-tuning process. This helps the model align with safety guidelines even before RLHF. Safety RLHF involves collecting human preference data by having annotators write prompts that they believe can elicit unsafe behavior. Multiple model responses are compared, and the safest response is selected according to a set of guidelines. This data is then used to train a safety reward model. These concepts are aimed at mitigating safety risks and improving the safety of language models.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1c38a124-5279-4a43-a2fe-ed2cbce9bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_response = base_query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5c2910e5-1a45-4de5-8035-5b5a47125d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key concepts for safety fine-tuning include supervised safety fine-tuning and safety RLHF (Reinforcement Learning with Human Feedback). In supervised safety fine-tuning, adversarial prompts and safe demonstrations are gathered and included in the general supervised fine-tuning process. This helps the model align with safety guidelines even before RLHF. Safety RLHF involves observing and generalizing from safe demonstrations in supervised fine-tuning. The model learns to write detailed safe responses, address safety concerns, explain sensitive topics, and provide additional helpful information. These concepts aim to mitigate safety risks and improve the safety of language models.\n"
     ]
    }
   ],
   "source": [
    "print(str(base_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c834e3-ecd9-43cd-b661-e9380076abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autogenerate some questions \n",
    "question_gen_query = (\n",
    "    \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "    \"a quiz/examination. Using the provided context from a \"\n",
    "    \"report on the new Llama 2 model, formulate \"\n",
    "    \"a single question that captures an important fact from the \"\n",
    "    \"context. Restrict the question to the context information provided.\"\n",
    ")\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    docs,\n",
    "    question_gen_query=question_gen_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d786e-25b6-4cc6-a89c-5c4fdc3195a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 8 questions\n",
    "questions = dataset_generator.generate_questions_from_nodes(num=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a8d24-8274-47df-ad1a-0a96cb9a9235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": [query_str],\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
