{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8cbe152-de29-4240-8e13-f74dc146a658",
   "metadata": {},
   "source": [
    "# Parallelizing Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd7dec-c846-4ae1-98ab-5436fac08668",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to execute ingestion pipelines using parallel processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0963707-6ebe-4441-a363-1bfb48ce9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4bcbf-491d-4d55-bade-a40d5e8b32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile, pstats\n",
    "from pstats import SortKey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba575e-2635-4598-a74a-d4036c1816db",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92686bb0-85ed-4bb3-99eb-f5fc6c100787",
   "metadata": {},
   "source": [
    "For this notebook, we'll load the `PatronusAIFinanceBenchDataset` llama-dataset from [llamahub](https://llamahub.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b94d62-efa4-479a-9215-e094b5a73061",
   "metadata": {},
   "outputs": [],
   "source": [
    "!llamaindex-cli download-llamadataset PatronusAIFinanceBenchDataset --download-dir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f7e5b-6430-426b-b239-e9280ea7b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data/source_files\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00be91-22ea-403c-b9c4-cd030b7e6c09",
   "metadata": {},
   "source": [
    "### Define our IngestionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1089adee-bc8a-457f-8d96-113435923d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.extractors import TitleExtractor\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n",
    "        TitleExtractor(),\n",
    "        OpenAIEmbedding(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# since we'll be testing performance, using timeit and cProfile\n",
    "# we're going to disable cache\n",
    "pipeline.disable_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1937fbaa-0cef-494d-b3e1-a5ff268fd8d2",
   "metadata": {},
   "source": [
    "### Parallel Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf20d688-5994-4cd7-8f52-079b686328fb",
   "metadata": {},
   "source": [
    "A single run. Setting `num_workers` to a value greater than 1 will invoke parallel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d68e6a-a658-46e8-9b71-d857b1c90d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.48it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.37it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.76it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "nodes = pipeline.run(documents=documents, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627d411-8fad-43ca-a6f0-533635e0c613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bffaefb-f710-4187-a19f-11d10ebae82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.34it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.92it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.59it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.93it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.56it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.42it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.89it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.83it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.99it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.28it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.80it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.39it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.59it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.73it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.75it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.39it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.68it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.91it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.60it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.54it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.38it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.34it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.68it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.66it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.47it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.66it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.63it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.76it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.81it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4 s ± 2.89 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pipeline.run(documents=documents, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d514fe-313b-4b88-8122-c5a44db210df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.51it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.38it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.26it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.96it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan  9 01:48:45 2024    newstats\n",
      "\n",
      "         2054 function calls in 10.402 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 211 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   10.402   10.402 {built-in method builtins.exec}\n",
      "        1    0.010    0.010   10.402   10.402 <string>:1(<module>)\n",
      "        1    0.000    0.000   10.392   10.392 pipeline.py:353(run)\n",
      "       12    0.000    0.000   10.358    0.863 threading.py:589(wait)\n",
      "       12    0.000    0.000   10.358    0.863 threading.py:288(wait)\n",
      "       75   10.358    0.138   10.358    0.138 {method 'acquire' of '_thread.lock' objects}\n",
      "        1    0.000    0.000   10.356   10.356 pool.py:369(starmap)\n",
      "        1    0.000    0.000   10.356   10.356 pool.py:767(get)\n",
      "        1    0.000    0.000   10.356   10.356 pool.py:764(wait)\n",
      "        1    0.000    0.000    0.028    0.028 context.py:115(Pool)\n",
      "        1    0.000    0.000    0.028    0.028 pool.py:183(__init__)\n",
      "        1    0.000    0.000    0.026    0.026 pool.py:305(_repopulate_pool)\n",
      "        1    0.000    0.000    0.026    0.026 pool.py:314(_repopulate_pool_static)\n",
      "        4    0.000    0.000    0.026    0.007 process.py:110(start)\n",
      "        4    0.000    0.000    0.026    0.006 context.py:285(_Popen)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x16bf2df60>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cProfile.run(\n",
    "    \"pipeline.run(documents=documents, parallel=True, num_workers=4)\",\n",
    "    \"newstats\",\n",
    ")\n",
    "p = pstats.Stats(\"newstats\")\n",
    "p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e345d8-0524-4e1b-8d11-88a2a916196e",
   "metadata": {},
   "source": [
    "### Sequential Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80091185-d7ac-4ff2-aba4-e1ba5546a865",
   "metadata": {},
   "source": [
    "By default `num_workers` is set to `None` and this will inovke sequential execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31aabf-da4d-4a4a-b92c-2b83a75b296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.74it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a116fd0-829e-4138-8461-ee4da5708f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8b9c1-9129-43e6-9d7d-cd50b3abc953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.48it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.3 s ± 2.2 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf08074-3bb1-46bb-86f0-aca8e103e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan  9 01:52:26 2024    oldstats\n",
      "\n",
      "         1024732 function calls (989764 primitive calls) in 26.372 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1236 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   26.373   26.373 {built-in method builtins.exec}\n",
      "        1    0.021    0.021   26.373   26.373 <string>:1(<module>)\n",
      "        1    0.000    0.000   26.353   26.353 pipeline.py:353(run)\n",
      "        1    0.000    0.000   26.353   26.353 pipeline.py:51(run_transformations)\n",
      "        1    0.004    0.004   21.593   21.593 base.py:334(__call__)\n",
      "        1    0.001    0.001   21.571   21.571 base.py:234(get_text_embedding_batch)\n",
      "       12    0.000    0.000   21.567    1.797 openai.py:377(_get_text_embeddings)\n",
      "       12    0.000    0.000   21.567    1.797 __init__.py:287(wrapped_f)\n",
      "       12    0.001    0.000   21.567    1.797 __init__.py:369(__call__)\n",
      "       12    0.000    0.000   21.565    1.797 openai.py:145(get_embeddings)\n",
      "       12    0.000    0.000   21.561    1.797 embeddings.py:33(create)\n",
      "       12    0.000    0.000   21.510    1.793 _base_client.py:1074(post)\n",
      "       12    0.000    0.000   21.509    1.792 _base_client.py:844(request)\n",
      "       12    0.000    0.000   21.509    1.792 _base_client.py:861(_request)\n",
      "       12    0.000    0.000   21.217    1.768 _client.py:882(send)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x16ba77970>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cProfile.run(\"pipeline.run(documents=documents)\", \"oldstats\")\n",
    "p = pstats.Stats(\"oldstats\")\n",
    "p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9bfb79-2def-462c-b3bb-d446e3bb9463",
   "metadata": {},
   "source": [
    "### In Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e90d6-013a-43d6-8c49-dbd78709587a",
   "metadata": {},
   "source": [
    "The results above show that with just 4 workers, you can get a speed up of nearly 50% when using parallel execution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
