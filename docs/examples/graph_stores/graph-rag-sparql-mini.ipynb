{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb28685",
   "metadata": {},
   "source": [
    "# SPARQL Graph RAG Demo\n",
    "\n",
    "\n",
    "An OpenAI API key will be required to run this, as well as read/write access to a [SPARQL](https://en.wikipedia.org/wiki/SPARQL) store. Such a store is available with the details as below.\n",
    "\n",
    "This replicates the parts of [Wey Gu](https://siwei.io/en/)'s [Jupyter Notebook](https://www.siwei.io/en/demos/graph-rag/), where [LlamaIndex](https://www.llamaindex.ai/) uses a graph store. In that Notebook a NebulaGraph store is use, here a SPARQL store is used. This is only a proof-of-concept. \n",
    "\n",
    "The current version of `sparql.py` involves a simple mapping of the *subject-relation-object* text snippets into an RDF model and from there SPARQL queries. There are clear issues in terms of performance etc. with the way it currently works which need fixing. But the steps from there to using Linked Data from the Web at large are straightforward. (Later, exploiting structured data/content from HTML pages relevance-targeted by the system shouldn't be difficult using existing libs).    \n",
    "\n",
    "For how the Graph RAG system as a whole works see the original [Notebook](https://www.siwei.io/en/demos/graph-rag/) and the [LlamaIndex Documentation](https://gpt-index.readthedocs.io/en/latest/).\n",
    "\n",
    "\n",
    "### Preparation \n",
    "\n",
    "* install llama_index, openai and sparqlwrapper\n",
    "* make a SPARQL endpoint available, add URL below  UPDATE, /llama_index_sparql-test/\n",
    "* For clean start DROP GRAPH <http://purl.org/stuff/guardians>\n",
    "* Add OpenAI API key below\n",
    "\n",
    "#### 1. Imports, LLM Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b273f74",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'download_loader' from 'llama_index' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_loader\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'download_loader' from 'llama_index' (unknown location)"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader\n",
    "import os\n",
    "import logging\n",
    "from llama_index import (\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import SparqlGraphStore\n",
    "from llama_index.llms import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index import load_index_from_storage\n",
    "import os\n",
    "import openai\n",
    "\n",
    "############\n",
    "# LLM Config\n",
    "############\n",
    "\n",
    "# two ways, at least one will work\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "openai.api_key = \"\"\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"text-davinci-002\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4f8e5",
   "metadata": {},
   "source": [
    "#### 1.1 SPARQL Store Configuration\n",
    "\n",
    "SPARQL Stores may vary in implementation. The [Fuseki](https://jena.apache.org/documentation/fuseki2/) server follows [specifications](https://www.w3.org/TR/sparql11-query/) closely and uses the following scheme :\n",
    "\n",
    "* multiple datasets (= DBs) are supported\n",
    "* each dataset can contain a default graph as well as multiple named graphs\n",
    "* each dataset can be configured with various endpoints, each providing facilities as required (query, update etc)\n",
    "\n",
    "*Fuseki does include basic access control facilities, but the dataset used here is wide open.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# SPARQL Config\n",
    "###############\n",
    "ENDPOINT = 'https://fuseki.hyperdata.it/llama_index_sparql-test/'\n",
    "GRAPH = 'http://purl.org/stuff/guardians'\n",
    "BASE_URI = 'http://purl.org/stuff/data'\n",
    "\n",
    "graph_store = SparqlGraphStore(\n",
    "    sparql_endpoint=ENDPOINT,\n",
    "    sparql_graph=GRAPH,\n",
    "    sparql_base_uri=BASE_URI,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa475ed3",
   "metadata": {},
   "source": [
    "#### 2.1 Load Augmentation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aaa92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "loader = WikipediaReader()\n",
    "documents = loader.load_data(\n",
    "    pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a722d",
   "metadata": {},
   "source": [
    "#### 2.2 Create Index from Augmentation Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    sparql_endpoint=ENDPOINT,\n",
    "    sparql_graph=GRAPH,\n",
    "    sparql_base_uri=BASE_URI,\n",
    "    include_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f366692",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\n",
    "    \"Who is Quill?\")\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4868f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
