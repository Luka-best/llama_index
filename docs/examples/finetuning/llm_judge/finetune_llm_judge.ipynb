{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722c13cc-a78a-4037-859e-48c538d00d9b",
   "metadata": {},
   "source": [
    "# Knowledge Distillation For Fine-Tuning A GPT-3.5 Judge\n",
    "\n",
    "There has been recent research that demonstrated GPT-4's ability to closely align to human judges when evaluating LLM generated texts (e.g., see [[1]](https://arxiv.org/abs/2306.05685), [[2]](https://arxiv.org/abs/2303.16634)). In this notebook, we demonstrate how to use the `llama_index` library to distill knowledge from GPT-4 to GPT-3.5 so that the smaller GPT-3.5 becomes closer to GPT-4 performance; and by proxy, closer to human judges.\n",
    "\n",
    "To do so, we take the following steps:\n",
    "\n",
    "1. Generate datasets: `train` and `test`\n",
    "2. Perform knowledge distillation (using `train`)\n",
    "3. Evaluate the distilled model  on `test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89edf1-b359-4370-8b1e-fad279508c68",
   "metadata": {},
   "source": [
    "## 1 Generate datasets: `train` and `test`\n",
    "\n",
    "We should not lose sight on the ultimate goal here, which is to build an LLM judge that closely matches to human judges when evaluating LLM-generated texts. The work we need to do in this step, therefore, is to build a set of generated texts that our LLM judges will judge. More specifically, we will follow the \"single-grading\" evaluation design pattern, where one text generation is passed to an LLM judge that is subsequently prompted to assign a score between 0 and 1 (higher is better).\n",
    "\n",
    "To generate a varied set of texts we'll use the following LLM text-generators:\n",
    "1. HuggingFace: Vicuna-13B\n",
    "2. HuggingFace: Mistral-7B\n",
    "3. HuggingFace: Falcon-7B\n",
    "\n",
    "The generation task we ask of each of these models will be to generate an abstractive answer to question when provided relevant context (i.e., RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66486ab-38cf-4ed6-bef4-6fe9deee0590",
   "metadata": {},
   "source": [
    "### Using `DatasetGenerator` to build `train` and `test`\n",
    "\n",
    "The specific procedure we will use here involves generating questions against a set of chunks of a given `Document`. With the `<question, chunk>` pairs in hand, (for which we can merely treat as a \"simulated\" retrieval), we pass this information to the three LLM generators and prompt them each to generate an answer.\n",
    "\n",
    "Hang tight, we're almost there (sort of). It's important to note now that our learning objective for performing knowledge distillation takes on the following form:\n",
    "$$\n",
    "\\mathcal{L}_{student} = \\alpha\\mathcal{L}_{CE} + (1-\\alpha)\\mathcal{L}_{KD},\n",
    "$$\n",
    "where $\\mathcal{L}_{CE}$ is the usual cross-entropy loss and $\\mathcal{L}_{KD}$ is the knowledge-distillation loss (which if you're keen on knowing will be based on the Kullback-Leibler divergence).\n",
    "\n",
    "Computing $\\mathcal{L}_{KD}$ in our case requires us to actually get knowledge from the teacher model (i.e., GPT-4). To do that, we will need to prompt GPT-4 to judge a set of generated answers. Specifically, we will present the GPT-4 judge with a single LLM-generated answers and prompt it to assign a score, ranging between 0 and 1, to it. To turn this into a classification problem, we will now assert than any score greater than 0.8 to be an \"acceptable\" answer, and an \"unacceptable\" one otherwise. \n",
    "\n",
    "With all of that we can now build a `dataset` that looks like the one below.\n",
    "| question | generated-answer | gpt-4-score | gpt-4-classification |\n",
    "|----------|------------------|-------------|----------------------|\n",
    "| ...      | ...              | ...         | ...                  |\n",
    "\n",
    "And finally, to get `train` and `test` we will simply randomly shuffle `dataset` and split it using a 70/30 ratio. (Phew!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc37cea-c9d2-4807-ab45-69f8b38db639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# load a document\n",
    "\n",
    "# split document into chunks\n",
    "\n",
    "# generate questions against chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84331853-ac29-49ca-85c1-e874d26e5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import HuggingFaceLLM, OpenAI\n",
    "\n",
    "# define our llm-generators\n",
    "\n",
    "# define our llm judges (also student/teacher models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42937500-a709-4556-aac2-0e5929f66e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our dataset, and split into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb16d0-15b1-45f3-96b0-3b51574d1626",
   "metadata": {},
   "source": [
    "## 2 Perform knowledge distillation\n",
    "\n",
    "Okay, it's now time to distill some knowledge from GPT-4 to GPT-3.5 To do this, we will make use of `OpenAIFinetuneEngine` class of `llama_index`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
