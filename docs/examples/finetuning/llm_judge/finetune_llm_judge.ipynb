{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722c13cc-a78a-4037-859e-48c538d00d9b",
   "metadata": {},
   "source": [
    "# Knowledge Distillation For Fine-Tuning A GPT-3.5 Judge\n",
    "\n",
    "There has been recent research that demonstrated GPT-4's ability to closely align to human judges when evaluating LLM generated texts (e.g., see [[1]](https://arxiv.org/abs/2306.05685), [[2]](https://arxiv.org/abs/2303.16634)). In this notebook, we demonstrate how to use the `llama_index` library to distill knowledge from GPT-4 to GPT-3.5 so that the smaller GPT-3.5 becomes closer to GPT-4 performance; and by proxy, closer to human judges.\n",
    "\n",
    "To do so, we take the following steps:\n",
    "\n",
    "1. Generate datasets: `train` and `test`\n",
    "2. Perform knowledge distillation (using `train`)\n",
    "3. Evaluate the distilled model  on `test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8deb07-ed32-4284-b943-e63867e26288",
   "metadata": {},
   "source": [
    "## 0 Prompt Templates & Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f7df9-a048-43aa-b862-290e832ea631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afd212-38b0-492c-91ea-a810e126ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"QUESTION_GEN\": (\n",
    "        \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "        \"a quiz/examination. Using the provided context, formulate \"\n",
    "        \"a single question that captures an important fact from the \"\n",
    "        \"context. Restrict the question to the context information provided.\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7031c-9da8-4116-969a-b1180a0fc118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(question, source, answer_a, answer_b, result) -> None:\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Question\": question,\n",
    "            \"Source\": source,\n",
    "            \"Model A\": answer_a[\"model\"],\n",
    "            \"Answer A\": answer_a[\"text\"],\n",
    "            \"Model B\": answer_b[\"model\"],\n",
    "            \"Answer B\": answer_b[\"text\"],\n",
    "            \"Judgement\": result,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Answer A\", \"Answer B\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7b54d-6c46-48c2-ac52-40a39e70d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine.retriever_query_engine import (\n",
    "    RetrieverQueryEngine,\n",
    ")\n",
    "from llama_index.llms import HuggingFaceInferenceAPI\n",
    "from llama_index.llm_predictor import LLMPredictor\n",
    "\n",
    "\n",
    "def create_query_engine(hf_name: str) -> RetrieverQueryEngine:\n",
    "    \"\"\"Create a RetrieverQueryEngine using the HuggingFaceInferenceAPI LLM\"\"\"\n",
    "    if hf_name not in hf_llm_generators:\n",
    "        raise KeyError(\"model not listed in hf_llm_generators\")\n",
    "    llm = HuggingFaceInferenceAPI(\n",
    "        model_name=hf_llm_generators[hf_name],\n",
    "        context_window=2048,  # to use refine\n",
    "        token=HUGGING_FACE_TOKEN,\n",
    "    )\n",
    "    context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n",
    "    return RetrieverQueryEngine.from_args(\n",
    "        retriever=retriever, service_context=context\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89edf1-b359-4370-8b1e-fad279508c68",
   "metadata": {},
   "source": [
    "## 1 Generate datasets: `train` and `test`\n",
    "\n",
    "We should not lose sight on the ultimate goal here, which is to build an LLM judge that closely matches to human judges when evaluating LLM-generated texts. The work we need to do in this step, therefore, is to build a set of generated texts that our LLM judges will judge. More specifically, we will follow the \"pairwise comparison\" evaluation design pattern, where one text generation is passed to an LLM judge that is subsequently prompted to assign a score between 0 and 1 (higher is better).\n",
    "\n",
    "To generate a varied set of texts we'll use the following LLM text-generators:\n",
    "1. HuggingFace: Llama2-7B (chat)\n",
    "2. HuggingFace: Mistral-7B (instruct)\n",
    "3. HuggingFace: Falcon-7B (instruct)\n",
    "\n",
    "The generation task we ask of each of these models will be to generate an abstractive answer to question when provided relevant context (i.e., RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66486ab-38cf-4ed6-bef4-6fe9deee0590",
   "metadata": {},
   "source": [
    "### Using `DatasetGenerator` to build `train` and `test`\n",
    "\n",
    "The specific procedure we will use here involves generating questions against a set of chunks of a given `Document`. With the `<question, chunk>` pairs in hand, (for which we can merely treat as a \"simulated\" retrieval), we pass this information to the three LLM generators and prompt them each to generate an answer.\n",
    "\n",
    "Hang tight, we're almost there (sort of). Since we want to distill GPT-4 abilities for this task to GPT-3.5, we now need to generate GPT-4 judgements on the generated answers. To do this, we will pass the `<question, answer A, answer B>` (where `A` and `B` represent answers from any two of the LLM text-generators) as context to the GPT-4 judge and prompt it to decide the better answer of the two.\n",
    "\n",
    "With all of that we can now build a `dataset` that looks like the one below.\n",
    "| question | context-answer-A-answer-B | gpt-4-evaluation |\n",
    "|----------|---------------------------|------------------|\n",
    "| ...      | ...                       | ...              |\n",
    "\n",
    "And finally, to get `train` and `test` we will simply randomly shuffle `dataset` and split it using a 70/30 ratio. (Phew!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef531ed-8e97-4d8f-8cc3-6f7e0c6ca141",
   "metadata": {},
   "source": [
    "#### Generate Questions and LLM-Generated Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc60af-2ef8-43b6-8b24-f46adc223d03",
   "metadata": {},
   "source": [
    "With all that out of the way, let's spring into action. First, we will download the reference pdf document and create the set of questions against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536754e6-666f-4c43-82b0-d81c9281ed07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 20.7M  100 20.7M    0     0   619k      0  0:00:34  0:00:34 --:--:--  648k  441k      0  0:00:48  0:00:02  0:00:46  441k     0   611k      0  0:00:34  0:00:24  0:00:10  635k616k      0  0:00:34  0:00:32  0:00:02  632k\n"
     ]
    }
   ],
   "source": [
    "# Download the pdf document — Uncomment the line of code below\n",
    "# !curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc37cea-c9d2-4807-ab45-69f8b38db639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_index import SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# load a document\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"paul_graham_essay.txt\"]\n",
    ").load_data()\n",
    "\n",
    "# Shuffle the documents\n",
    "random.seed(42)\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e10603-a0e0-4c87-a4d5-fdf8f1ca0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate questions against chunks\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# set context for llm provider\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    ")\n",
    "\n",
    "# instantiate a DatasetGenerator\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents,\n",
    "    question_gen_query=PROMPTS[\"QUESTION_GEN\"],\n",
    "    service_context=gpt_35_context,\n",
    "    show_progress=True,\n",
    "    num_questions_per_chunk=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e2276-92dc-48c3-8cd0-583655ab7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  5.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# use DatasetGenerator to create questions from nodes\n",
    "questions = dataset_generator.generate_questions_from_nodes(num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f3dd3-62ab-4aac-a995-cf16d3306f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What were the two main things the author worked on before college?\n",
      "What language was regarded as the language of AI in the 1980s?\n",
      "What was the author's reason for considering a career in art?\n",
      "What was the main reason for the author's decision to leave the Accademia?\n",
      "What important lesson did the author learn from their experience at Interleaf?\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at a few of these\n",
    "for q in questions[:5]:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201d9cb-4746-4c71-8728-55e56cb8b76f",
   "metadata": {},
   "source": [
    "Now that we have the questions, the next step is to generate answers using the three LLM text-generators: Llama-2, Mistral, and Falcon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc71805-896d-4bbe-9053-495426a26a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(  # what embeddings are being used?\n",
    "    index=index,\n",
    "    node_ids=list(index.index_struct.nodes_dict.values()),\n",
    "    similarity_top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84331853-ac29-49ca-85c1-e874d26e5f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# define our llm-generators\n",
    "hf_llm_generators = {\n",
    "    \"mistral-7b-instruct\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"llama2-7b-chat\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"falcon-7b-instruct\": \"tiiuae/falcon-7b-instruct\",\n",
    "}\n",
    "\n",
    "query_engines = {\n",
    "    mdl: create_query_engine(mdl) for mdl in hf_llm_generators.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eae846-8fad-4e85-a2a4-4a20a97f6c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mistral-7b-instruct': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x169f42950>,\n",
       " 'llama2-7b-chat': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x169f42ef0>,\n",
       " 'falcon-7b-instruct': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x169f40cd0>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760a173-675a-4db0-8f66-0b396c2a34d7",
   "metadata": {},
   "source": [
    "Now, we can create the master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d126f-8fcc-42ea-b143-4533638763a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:19<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "dataset = []\n",
    "for q in tqdm.tqdm(questions):\n",
    "    data_entry = {\"question\": q}\n",
    "\n",
    "    responses = []\n",
    "    source = None\n",
    "    for name, engine in query_engines.items():\n",
    "        response = engine.query(q)\n",
    "        response_struct = {}\n",
    "        response_struct[\"model\"] = name\n",
    "        response_struct[\"text\"] = str(response)\n",
    "        if source is not None:\n",
    "            assert source == response.source_nodes[0].node.text[:1000] + \"...\"\n",
    "        else:\n",
    "            source = response.source_nodes[0].node.text[:1000] + \"...\"\n",
    "        responses.append(response_struct)\n",
    "\n",
    "    data_entry[\"answers\"] = responses\n",
    "    data_entry[\"source\"] = source\n",
    "    dataset.append(data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd45b8-20b2-4ee5-80aa-bece2e328436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9601c-78a9-448c-bac0-c78ef1c74cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What were the two main things the author worked on before college?',\n",
       "  'answers': [{'model': 'mistral-7b-instruct',\n",
       "    'text': '\\n\\nBefore college, the two main things I worked on were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something.\\n\\nAfter'},\n",
       "   {'model': 'llama2-7b-chat',\n",
       "    'text': '\\nBefore college, the author worked on writing and programming, specifically writing essays and spam filters, and also painted.\\n\\nOriginal Answer: \\nThe two main things the author worked on before college were writing and programming.'},\n",
       "   {'model': 'falcon-7b-instruct',\n",
       "    'text': '\\n1. Writing and programming.\\n2. Writing and programming.\\nThe original answer is correct, but it could be improved to better answer the query. The context provided is that the author worked on writing and programming before college, and that they have been working on these skills since then. Adding more information about their work experience and projects could be useful.'}],\n",
       "  'source': 'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the...'},\n",
       " {'question': 'What language was regarded as the language of AI in the 1980s?',\n",
       "  'answers': [{'model': 'mistral-7b-instruct',\n",
       "    'text': \"\\n\\nLisp was regarded as the language of AI in the 1980s, particularly in the context of the TRS-80 computer, which was a popular platform for programming at the time. This was when the author of the response first started programming and writing simple games, a program to predict how high model rockets would fly, and a word processor. However, the author didn't initially plan to study AI in college, as they found philosophy to be boring. It wasn't until they saw a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU that they became interested in AI.\"},\n",
       "   {'model': 'llama2-7b-chat',\n",
       "    'text': ' In the 1980s, Lisp was the language of AI, particularly in the context of the work of Terry Winograd and the SHRDLU project. The novel \"The Moon is a Harsh Mistress\" by Robert Heinlein, which featured an intelligent computer called Mike, may have also contributed to the perception of Lisp as the language of AI during this time.'},\n",
       "   {'model': 'falcon-7b-instruct',\n",
       "    'text': '<strong>Lisp</strong> was regarded as the language of AI in the 1980s.'}],\n",
       "  'source': \"All you had to do was teach SHRDLU more words.\\n\\nThere weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of AI. The commonly used programming languages then were pretty primitive, and programmers' ideas correspondingly so. The default language at Cornell was a Pascal-like language called PL/I, and the situation was similar elsewhere. Learning Lisp expanded my concept of a program so fast that it was years before I started to have a sense of where the new limits were. This was more like it; this was what I had expected college to do. It wasn't happening in a class, like it was supposed to, but that was ok. For the next couple years I was on a roll. I knew what I was going to do.\\n\\nFor my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love working on that program. It was a pleasing bit of code, but what made it even more exciting was my ...\"},\n",
       " {'question': \"What was the author's reason for considering a career in art?\",\n",
       "  'answers': [{'model': 'mistral-7b-instruct',\n",
       "    'text': '\\n\\nThe author considered a career in art because he wanted to build things that would last. He was dissatisfied with systems work and wanted to make a living doing something he could be truly independent in. He was also attracted to the idea of being able to make art, to put the verb \"make\" before the noun \"art,\" which seemed almost miraculous to him. He took art classes at Harvard and decided to write a dissertation on applications of continuations as a way to get out of graduate school. He applied to art schools and was accepted at the Rhode Island School of Design (RISD) and the Accademia di Belli Arti in Florence, Italy. However, he only received an acceptance letter from RISD and decided to take the entrance exam for the Accademia in Florence. Despite not knowing Italian, he passed the exam and was accepted into the school.\\n\\nThe author was curious about what he was seeing and wanted to learn how to see more deeply, to understand what he was seeing, and to be able to make things that would last. He liked painting still lives because he was curious about what he was seeing. In everyday life, we aren\\'t consciously aware of much we\\'re seeing'},\n",
       "   {'model': 'llama2-7b-chat',\n",
       "    'text': \"\\nThe author considered a career in art because they were dissatisfied with their work in computer science and wanted to build something that would last. They were also inspired by the idea of making art that would be independent and not subject to the whims of technology. Additionally, they were drawn to the idea of learning a new skill and exploring a different creative outlet. The author found still life painting particularly appealing because they were curious about the details of everyday objects and wanted to emphasize the visual cues that make an object recognizable, such as the edge of an object. The author's experience with painting still lives also highlights the importance of looking closely at the world around us, as there is always more to notice and discover.\"},\n",
       "   {'model': 'falcon-7b-instruct',\n",
       "    'text': \"\\nThe author's reason for considering a career in art was to be able to create something that could be seen and appreciated by others. This was a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there's a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\"}],\n",
       "  'source': \"I was briefly tempted, but they were so slow by present standards; what was the point? No one else wanted one either, so off they went. That was what happened to systems work.\\n\\nI wanted not just to build things, but to build things that would last.\\n\\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where he was in grad school. One day I went to visit the Carnegie Institute, where I'd spent a lot of time as a kid. While looking at a painting there I realized something that might seem obvious, but was a big surprise to me. There, right on the wall, was something you could make that would last. Paintings didn't become obsolete. Some of the best ones were hundreds of years old.\\n\\nAnd moreover this was something you could make a living doing. Not as easily as you could by writing software, of course, but I thought if you were really industrious and lived really cheaply, it had to be possible to make enough to survive. And as an artist you could be truly independent. You ...\"}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8ee56-5738-41b3-9a4c-ff6296910058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these generations for future use\n",
    "import json\n",
    "\n",
    "with open(\"qa_dataset.jsonl\", \"w\") as outfile:\n",
    "    for entry in dataset:\n",
    "        print(json.dumps(entry), file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab67fef-964a-4806-ad01-ec9be4b7a8e1",
   "metadata": {},
   "source": [
    "#### Generate GPT-4 Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e219b1-58be-4a91-a7ca-0a056806d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_TEMPLATE = (\n",
    "    \"Please act as an impartial judge and evaluate the quality of the responses provided by two \"\n",
    "    \"AI question-answering assistants to the user question along with the retrieved context which \"\n",
    "    \"was provided to both assistants are displayed below. You should choose the assistant that \"\n",
    "    \"follows the user’s instructions and answers the user’s question better using the provided \"\n",
    "    \"context. Your evaluation \"\n",
    "    \"should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, \"\n",
    "    \"and level of detail of their responses. Begin your evaluation by comparing the two \"\n",
    "    \"responses and provide a short explanation. Avoid any position biases and ensure that the \"\n",
    "    \"order in which the responses were presented does not influence your decision. Do not allow \"\n",
    "    \"the length of the responses to influence your evaluation. Do not favor certain names of \"\n",
    "    \"the assistants. Be as objective as possible. After providing your explanation, output your \"\n",
    "    \"final verdict by strictly following this format: '[[A]]' if assistant A is better, '[[B]]' \"\n",
    "    \"if assistant B is better, and '[[C]]' for a tie.\\n\"\n",
    ")\n",
    "\n",
    "DEFAULT_USER_TEMPLATE = (\n",
    "    \"[User Question]\\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n\\n\"\n",
    "    \"[The Start Retrieved Source]\\n\"\n",
    "    \"{source}\\n\"\n",
    "    \"[The End of Retrieved Source]\"\n",
    "    \"\\n\\n\"\n",
    "    \"[The Start of Assistant A’s Answer]\\n\"\n",
    "    \"{answer_a}\\n\"\n",
    "    \"[The End of Assistant A’s Answer]\"\n",
    "    \"\\n\\n\"\n",
    "    \"[The Start of Assistant B’s Answer]\\n\"\n",
    "    \"{answer_b}\\n\"\n",
    "    \"[The End of Assistant B’s Answer]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe6f3c-0fc5-4af9-b644-1c5219bf9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading the jsonl file\n",
    "import json\n",
    "\n",
    "with open(\"qa_dataset.jsonl\") as f:\n",
    "    dataset = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e72360-fae5-49e2-adfc-394a504212ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import PairwiseComparisonEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e653515-75c4-4987-87e4-c0a0b17a0bdf",
   "metadata": {},
   "source": [
    "#### Build Custom Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4ada0-c22e-4c25-9e12-6e53f383b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.evaluation.base import BaseEvaluator, EvaluationResult\n",
    "from llama_index.prompts import (\n",
    "    BasePromptTemplate,\n",
    "    ChatMessage,\n",
    "    ChatPromptTemplate,\n",
    "    MessageRole,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from llama_index.prompts.mixin import (\n",
    "    PromptDictType,\n",
    "    PromptMixin,\n",
    "    PromptMixinType,\n",
    ")\n",
    "\n",
    "DEFAULT_EVAL_TEMPLATE = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=DEFAULT_SYSTEM_TEMPLATE),\n",
    "        ChatMessage(role=MessageRole.USER, content=DEFAULT_USER_TEMPLATE),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8958f-62fd-41a8-8d21-3055cd80de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the gpt-4 judge\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4\")\n",
    ")\n",
    "\n",
    "# gpt4_judge = PairwiseComparisonEvaluator(service_context=gpt_4_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0a4d4-e55f-4189-8e64-132c128bd5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 3\n",
    "result = await gpt_4_context.llm_predictor.apredict(\n",
    "    prompt=DEFAULT_EVAL_TEMPLATE,\n",
    "    question=dataset[ix][\"question\"],\n",
    "    source=dataset[ix][\"source\"],\n",
    "    answer_a=dataset[ix][\"answers\"][0][\"text\"],\n",
    "    answer_b=dataset[ix][\"answers\"][1][\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd4601-289e-4fba-bc74-b9a0cfcce09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0bbe6_row0_col3, #T_0bbe6_row0_col5 {\n",
       "  inline-size: 300px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0bbe6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0bbe6_level0_col0\" class=\"col_heading level0 col0\" >Question</th>\n",
       "      <th id=\"T_0bbe6_level0_col1\" class=\"col_heading level0 col1\" >Source</th>\n",
       "      <th id=\"T_0bbe6_level0_col2\" class=\"col_heading level0 col2\" >Model A</th>\n",
       "      <th id=\"T_0bbe6_level0_col3\" class=\"col_heading level0 col3\" >Answer A</th>\n",
       "      <th id=\"T_0bbe6_level0_col4\" class=\"col_heading level0 col4\" >Model B</th>\n",
       "      <th id=\"T_0bbe6_level0_col5\" class=\"col_heading level0 col5\" >Answer B</th>\n",
       "      <th id=\"T_0bbe6_level0_col6\" class=\"col_heading level0 col6\" >Judgement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0bbe6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0bbe6_row0_col0\" class=\"data row0 col0\" >What was the main reason for the author's decision to leave the Accademia?</td>\n",
       "      <td id=\"T_0bbe6_row0_col1\" class=\"data row0 col1\" >Now all I had to do was learn Italian.\n",
       "\n",
       "Only stranieri (foreigners) had to take this entrance exam. In retrospect it may well have been a way of excluding them, because there were so many stranieri attracted by the idea of studying art in Florence that the Italian students would otherwise have been outnumbered. I was in decent shape at painting and drawing from the RISD foundation that summer, but I still don't know how I managed to pass the written exam. I remember that I answered the essay question by writing about Cezanne, and that I cranked up the intellectual level as high as I could to make the most of my limited vocabulary. [2]\n",
       "\n",
       "I'm only up to age 25 and already there are such conspicuous patterns. Here I was, yet again about to attend some august institution in the hopes of learning about some prestigious subject, and yet again about to be disappointed. The students and faculty in the painting department at the Accademia were the nicest people you could imagine, but they had lo...</td>\n",
       "      <td id=\"T_0bbe6_row0_col2\" class=\"data row0 col2\" >mistral-7b-instruct</td>\n",
       "      <td id=\"T_0bbe6_row0_col3\" class=\"data row0 col3\" >\n",
       "\n",
       "The main reason for the author's decision to leave the Accademia was that he was disappointed with the institution and its approach to teaching. He felt that the students and faculty were not taking their studies seriously and were not pushing each other to improve. Additionally, the author felt that the curriculum was outdated and did not provide him with the skills and knowledge he needed to succeed as an artist. The author was also motivated to leave the Accademia because he wanted to pursue a career in art and had applied to art schools, including the Accademia di Belli Arti in Florence. When he received an invitation to take the entrance exam at the Accademia, he decided to leave the Accademia and focus on learning Italian and preparing for the exam.</td>\n",
       "      <td id=\"T_0bbe6_row0_col4\" class=\"data row0 col4\" >llama2-7b-chat</td>\n",
       "      <td id=\"T_0bbe6_row0_col5\" class=\"data row0 col5\" > The author left the Accademia because he realized that the institution was not providing him with the quality of education he desired, and he had the opportunity to pursue his passion for art at RISD, which offered him a better chance at developing his skills and advancing his career.\n",
       "\n",
       "Explanation:\n",
       "The author's decision to leave the Accademia was not solely based on the fact that he realized he was not learning anything except Italian. The context provided reveals that the author was not satisfied with the quality of education he was receiving at the Accademia, and he had the opportunity to pursue his passion for art at RISD, which offered him a better chance at developing his skills and advancing his career. The author's desire to leave the Accademia and pursue his art career at RISD was likely the main reason for his decision to leave.</td>\n",
       "      <td id=\"T_0bbe6_row0_col6\" class=\"data row0 col6\" >Both Assistant A and Assistant B provide plausible reasons for the author's decision to leave the Accademia, based on the author's disappointment with the institution. However, Assistant A's response includes some details that are not present in the provided context, such as the author's perception that the students and faculty were not taking their studies seriously, the outdated curriculum, and the author's application to other art schools. On the other hand, Assistant B's response is more aligned with the provided context, stating that the author was not satisfied with the quality of education at the Accademia and had the opportunity to pursue his passion for art at RISD. Therefore, Assistant B's response is more accurate and relevant to the provided context. \n",
       "\n",
       "Final Verdict: [[B]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1484e3a60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_eval_df(\n",
    "    question=dataset[ix][\"question\"],\n",
    "    source=dataset[ix][\"source\"],\n",
    "    answer_a=dataset[ix][\"answers\"][0],\n",
    "    answer_b=dataset[ix][\"answers\"][1],\n",
    "    result=result,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42937500-a709-4556-aac2-0e5929f66e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our dataset, and split into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb16d0-15b1-45f3-96b0-3b51574d1626",
   "metadata": {},
   "source": [
    "## 2 Perform knowledge distillation\n",
    "\n",
    "Okay, it's now time to distill some knowledge from GPT-4 to GPT-3.5 To do this, we will make use of `OpenAIFinetuneEngine` class of `llama_index`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
