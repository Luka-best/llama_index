{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722c13cc-a78a-4037-859e-48c538d00d9b",
   "metadata": {},
   "source": [
    "# Knowledge Distillation For Fine-Tuning A GPT-3.5 Judge\n",
    "\n",
    "There has been recent research that demonstrated GPT-4's ability to closely align to human judges when evaluating LLM generated texts (e.g., see [[1]](https://arxiv.org/abs/2306.05685), [[2]](https://arxiv.org/abs/2303.16634)). In this notebook, we demonstrate how to use the `llama_index` library to distill knowledge from GPT-4 to GPT-3.5 so that the smaller GPT-3.5 becomes closer to GPT-4 performance; and by proxy, closer to human judges.\n",
    "\n",
    "To do so, we take the following steps:\n",
    "\n",
    "1. Generate datasets: `train` and `test`\n",
    "2. Perform knowledge distillation (using `train`)\n",
    "3. Evaluate the distilled model  on `test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8deb07-ed32-4284-b943-e63867e26288",
   "metadata": {},
   "source": [
    "## 0 Prompt Templates & Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f7df9-a048-43aa-b862-290e832ea631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afd212-38b0-492c-91ea-a810e126ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"QUESTION_GEN\": (\n",
    "        \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "        \"a quiz/examination. Using the provided context, formulate \"\n",
    "        \"a single question that captures an important fact from the \"\n",
    "        \"context. Restrict the question to the context information provided.\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7031c-9da8-4116-969a-b1180a0fc118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(question, source, answer_a, answer_b, result) -> None:\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Question\": question,\n",
    "            \"Source\": source,\n",
    "            \"Model A\": answer_a[\"model\"],\n",
    "            \"Answer A\": answer_a[\"text\"],\n",
    "            \"Model B\": answer_b[\"model\"],\n",
    "            \"Answer B\": answer_b[\"text\"],\n",
    "            \"Score\": result.score,\n",
    "            \"Judgement\": result.feedback,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Answer A\", \"Answer B\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89edf1-b359-4370-8b1e-fad279508c68",
   "metadata": {},
   "source": [
    "## 1 Generate datasets: `train` and `test`\n",
    "\n",
    "We should not lose sight on the ultimate goal here, which is to build an LLM judge that closely matches to human judges when evaluating LLM-generated texts. The work we need to do in this step, therefore, is to build a set of generated texts that our LLM judges will judge. More specifically, we will follow the \"pairwise comparison\" evaluation design pattern, where one text generation is passed to an LLM judge that is subsequently prompted to assign a score between 0 and 1 (higher is better).\n",
    "\n",
    "To generate a varied set of texts we'll use the following LLM text-generators:\n",
    "1. HuggingFace: Llama2-7B (chat)\n",
    "2. HuggingFace: Mistral-7B (instruct)\n",
    "3. HuggingFace: Falcon-7B (instruct)\n",
    "\n",
    "The generation task we ask of each of these models will be to generate an abstractive answer to question when provided relevant context (i.e., RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66486ab-38cf-4ed6-bef4-6fe9deee0590",
   "metadata": {},
   "source": [
    "### Using `DatasetGenerator` to build `train` and `test`\n",
    "\n",
    "The specific procedure we will use here involves generating questions against a set of chunks of a given `Document`. With the `<question, chunk>` pairs in hand, (for which we can merely treat as a \"simulated\" retrieval), we pass this information to the three LLM generators and prompt them each to generate an answer.\n",
    "\n",
    "Hang tight, we're almost there (sort of). Since we want to distill GPT-4 abilities for this task to GPT-3.5, we now need to generate GPT-4 judgements on the generated answers. To do this, we will pass the `<question, answer A, answer B>` (where `A` and `B` represent answers from any two of the LLM text-generators) as context to the GPT-4 judge and prompt it to decide the better answer of the two.\n",
    "\n",
    "With all of that we can now build a `dataset` that looks like the one below.\n",
    "| question | context-answer-A-answer-B | gpt-4-evaluation |\n",
    "|----------|---------------------------|------------------|\n",
    "| ...      | ...                       | ...              |\n",
    "\n",
    "And finally, to get `train` and `test` we will simply randomly shuffle `dataset` and split it using a 70/30 ratio. (Phew!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef531ed-8e97-4d8f-8cc3-6f7e0c6ca141",
   "metadata": {},
   "source": [
    "#### Generate Questions and LLM-Generated Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc60af-2ef8-43b6-8b24-f46adc223d03",
   "metadata": {},
   "source": [
    "With all that out of the way, let's spring into action. First, we will download the reference pdf document and create the set of questions against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536754e6-666f-4c43-82b0-d81c9281ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pdf document — Uncomment the line of code below\n",
    "# !curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc37cea-c9d2-4807-ab45-69f8b38db639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_index import SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# load a document\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"paul_graham_essay.txt\"]\n",
    ").load_data()\n",
    "\n",
    "# Shuffle the documents\n",
    "random.seed(42)\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e10603-a0e0-4c87-a4d5-fdf8f1ca0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate questions against chunks\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# set context for llm provider\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    ")\n",
    "\n",
    "# instantiate a DatasetGenerator\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents,\n",
    "    question_gen_query=PROMPTS[\"QUESTION_GEN\"],\n",
    "    service_context=gpt_35_context,\n",
    "    show_progress=True,\n",
    "    num_questions_per_chunk=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e2276-92dc-48c3-8cd0-583655ab7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:01<00:00, 16.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# use DatasetGenerator to create questions from nodes\n",
    "questions = dataset_generator.generate_questions_from_nodes(num=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f3dd3-62ab-4aac-a995-cf16d3306f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the author's first experience with programming on a microcomputer?\n",
      "What language was regarded as the language of AI during the time period mentioned in the context?\n",
      "Question: What was the author's motivation for considering a career in art?\n",
      "What was the author's experience like studying at the Accademia?\n",
      "What did the author learn about technology companies while working at Interleaf?\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at a few of these\n",
    "for q in questions[:5]:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201d9cb-4746-4c71-8728-55e56cb8b76f",
   "metadata": {},
   "source": [
    "Now that we have the questions, the next step is to generate answers using the three LLM text-generators: Llama-2, Mistral, and Falcon. But, first we need to create a vector store for our documents and an associated retriever, which all of the LLM answer-generators will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc71805-896d-4bbe-9053-495426a26a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "\n",
    "# Create vector index\n",
    "the_index = VectorStoreIndex.from_documents(documents=documents)\n",
    "\n",
    "# Create the retriver on this index\n",
    "the_retriever = VectorIndexRetriever(\n",
    "    index=the_index,\n",
    "    node_ids=list(the_index.index_struct.nodes_dict.values()),\n",
    "    similarity_top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b687ddf-c8fa-4073-9aa9-0b9cfec23f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine.retriever_query_engine import (\n",
    "    RetrieverQueryEngine,\n",
    ")\n",
    "from llama_index.llms import HuggingFaceInferenceAPI\n",
    "from llama_index.llm_predictor import LLMPredictor\n",
    "\n",
    "\n",
    "def create_query_engine(hf_name: str) -> RetrieverQueryEngine:\n",
    "    \"\"\"Create a RetrieverQueryEngine using the HuggingFaceInferenceAPI LLM\"\"\"\n",
    "    if hf_name not in hf_llm_generators:\n",
    "        raise KeyError(\"model not listed in hf_llm_generators\")\n",
    "    llm = HuggingFaceInferenceAPI(\n",
    "        model_name=hf_llm_generators[hf_name],\n",
    "        context_window=2048,  # to use refine\n",
    "        token=HUGGING_FACE_TOKEN,\n",
    "    )\n",
    "    context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n",
    "    return RetrieverQueryEngine.from_args(\n",
    "        retriever=the_retriever, service_context=context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84331853-ac29-49ca-85c1-e874d26e5f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# define our llm-generators\n",
    "hf_llm_generators = {\n",
    "    \"mistral-7b-instruct\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"llama2-7b-chat\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"falcon-7b-instruct\": \"tiiuae/falcon-7b-instruct\",\n",
    "}\n",
    "\n",
    "query_engines = {\n",
    "    mdl: create_query_engine(mdl) for mdl in hf_llm_generators.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760a173-675a-4db0-8f66-0b396c2a34d7",
   "metadata": {},
   "source": [
    "We're ready to now to produce the anaswers from the various LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d126f-8fcc-42ea-b143-4533638763a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [09:51<00:00, 32.84s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "dataset = []\n",
    "for q in tqdm.tqdm(questions):\n",
    "    # randomly select two LLMs to generate answers to this q\n",
    "    model_versus = random.sample(list(query_engines.items()), 2)\n",
    "\n",
    "    # data for this q\n",
    "    data_entry = {\"question\": q}\n",
    "    responses = []\n",
    "    source = None\n",
    "\n",
    "    # generate answers\n",
    "    for name, engine in model_versus:\n",
    "        response = engine.query(q)\n",
    "        response_struct = {}\n",
    "        response_struct[\"model\"] = name\n",
    "        response_struct[\"text\"] = str(response)\n",
    "        if source is not None:\n",
    "            assert source == response.source_nodes[0].node.text[:1000] + \"...\"\n",
    "        else:\n",
    "            source = response.source_nodes[0].node.text[:1000] + \"...\"\n",
    "        responses.append(response_struct)\n",
    "\n",
    "    data_entry[\"answers\"] = responses\n",
    "    data_entry[\"source\"] = source\n",
    "    data_entry[\"evaluation\"] = None\n",
    "    data_entry[\"fine_tuning_events\"] = None\n",
    "    dataset.append(data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd45b8-20b2-4ee5-80aa-bece2e328436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8ee56-5738-41b3-9a4c-ff6296910058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save these generations for future use\n",
    "with open(\"qa_dataset.jsonl\", \"w\") as outfile:\n",
    "    for entry in dataset:\n",
    "        print(json.dumps(entry), file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab67fef-964a-4806-ad01-ec9be4b7a8e1",
   "metadata": {},
   "source": [
    "#### Generate GPT-4 Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe6f3c-0fc5-4af9-b644-1c5219bf9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading the jsonl file\n",
    "# import json\n",
    "\n",
    "# with open(\"qa_dataset.jsonl\") as f:\n",
    "#     dataset = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e653515-75c4-4987-87e4-c0a0b17a0bdf",
   "metadata": {},
   "source": [
    "#### Build Custom Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59397086-ccd9-4e80-92be-3d9e2be515ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_TEMPLATE = (\n",
    "    \"Please act as an impartial judge and evaluate the quality of the responses provided by two \"\n",
    "    \"AI question-answering assistants to the user question along with the retrieved context which \"\n",
    "    \"was provided to both assistants are displayed below. You should choose the assistant that \"\n",
    "    \"follows the user’s instructions and answers the user’s question better using the provided \"\n",
    "    \"context. Your evaluation \"\n",
    "    \"should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, \"\n",
    "    \"and level of detail of their responses. Begin your evaluation by comparing the two \"\n",
    "    \"responses and provide a short explanation. Avoid any position biases and ensure that the \"\n",
    "    \"order in which the responses were presented does not influence your decision. Do not allow \"\n",
    "    \"the length of the responses to influence your evaluation. Do not favor certain names of \"\n",
    "    \"the assistants. Be as objective as possible. After providing your explanation, output your \"\n",
    "    \"final verdict by strictly following this format: '[[A]]' if assistant A is better, '[[B]]' \"\n",
    "    \"if assistant B is better, and '[[C]]' for a tie.\\n\"\n",
    ")\n",
    "\n",
    "DEFAULT_USER_TEMPLATE = (\n",
    "    \"[User Question]\\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n\\n\"\n",
    "    \"[The Start Retrieved Source]\\n\"\n",
    "    \"{source}\\n\"\n",
    "    \"[The End of Retrieved Source]\"\n",
    "    \"\\n\\n\"\n",
    "    \"[The Start of Assistant A’s Answer]\\n\"\n",
    "    \"{answer_a}\\n\"\n",
    "    \"[The End of Assistant A’s Answer]\"\n",
    "    \"\\n\\n\"\n",
    "    \"[The Start of Assistant B’s Answer]\\n\"\n",
    "    \"{answer_b}\\n\"\n",
    "    \"[The End of Assistant B’s Answer]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4ada0-c22e-4c25-9e12-6e53f383b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation.base import EvaluationResult\n",
    "from llama_index.prompts import (\n",
    "    BasePromptTemplate,\n",
    "    ChatMessage,\n",
    "    ChatPromptTemplate,\n",
    "    MessageRole,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from llama_index.prompts.mixin import (\n",
    "    PromptDictType,\n",
    "    PromptMixin,\n",
    "    PromptMixinType,\n",
    ")\n",
    "from typing import Tuple\n",
    "\n",
    "DEFAULT_EVAL_TEMPLATE = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=DEFAULT_SYSTEM_TEMPLATE),\n",
    "        ChatMessage(role=MessageRole.USER, content=DEFAULT_USER_TEMPLATE),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def parse_eval_result(\n",
    "    query: str,\n",
    "    response: str,\n",
    ") -> EvaluationResult:\n",
    "    \"\"\"Take an Evaluation Result and parse response to assign a score.\n",
    "    - 1.0 if Answer A is better than Answer B\n",
    "    - 0.0 if Answer B is better than Answer A\n",
    "    - 0.5 if tie\n",
    "    \"\"\"\n",
    "\n",
    "    score = None\n",
    "    if \"[[A]]\" in response:\n",
    "        score = 1.0\n",
    "    elif \"[[B]]\" in response:\n",
    "        score = 0.0\n",
    "    elif \"[[C]]\" in response:\n",
    "        score = 0.5\n",
    "    else:\n",
    "        raise ValueError(\"Unable to parse response\")\n",
    "\n",
    "    return EvaluationResult(\n",
    "        query=query,\n",
    "        response=response,\n",
    "        score=score,\n",
    "        feedback=response,\n",
    "    )\n",
    "\n",
    "\n",
    "def resolve_results(\n",
    "    eval_result: EvaluationResult,\n",
    "    flipped_eval_result: EvaluationResult,\n",
    ") -> Tuple[EvaluationResult, str]:\n",
    "    \"\"\"Resolve eval results from evaluation + flipped evaluation.\"\"\"\n",
    "    votes_a = eval_result.score + (1 - flipped_eval_result.score)\n",
    "    votes_b = (1 - eval_result.score) + flipped_eval_result.score\n",
    "    assert votes_a + votes_b == 2\n",
    "\n",
    "    a_voters = [(eval_result, \"original\")] * (eval_result.score == 1.0) + [\n",
    "        (flipped_eval_result, \"flipped\")\n",
    "    ] * (flipped_eval_result.score == 0.0)\n",
    "\n",
    "    b_voters = [(eval_result, \"original\")] * (eval_result.score == 0.0) + [\n",
    "        (flipped_eval_result, \"flipped\")\n",
    "    ] * (flipped_eval_result.score == 1.0)\n",
    "\n",
    "    if votes_a > votes_b:\n",
    "        return a_voters[0]\n",
    "    elif votes_b > votes_a:\n",
    "        return b_voters[0]\n",
    "    else:\n",
    "        if eval_result.score == 0.5:  # voted tie twice\n",
    "            return (eval_result, \"original\")\n",
    "        else:\n",
    "            return (\n",
    "                EvaluationResult(\n",
    "                    query=eval_result.query,\n",
    "                    response=\"\",\n",
    "                    passing=None,\n",
    "                    score=0.5,\n",
    "                    feedback=\"Inconclusive.\",\n",
    "                ),\n",
    "                \"\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8958f-62fd-41a8-8d21-3055cd80de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the gpt-4 judge\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "\n",
    "flipped_finetuning_handler = OpenAIFineTuningHandler()\n",
    "flipped_callback_manager = CallbackManager([flipped_finetuning_handler])\n",
    "flipped_gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    callback_manager=flipped_callback_manager,\n",
    ")\n",
    "\n",
    "# gpt4_judge = PairwiseComparisonEvaluator(service_context=gpt_4_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0a4d4-e55f-4189-8e64-132c128bd5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [07:49<00:00, 26.07s/it]\n"
     ]
    }
   ],
   "source": [
    "for data_entry in tqdm.tqdm(dataset):\n",
    "    result = await gpt_4_context.llm_predictor.apredict(\n",
    "        prompt=DEFAULT_EVAL_TEMPLATE,\n",
    "        question=data_entry[\"question\"],\n",
    "        source=data_entry[\"source\"],\n",
    "        answer_a=data_entry[\"answers\"][0][\"text\"],\n",
    "        answer_b=data_entry[\"answers\"][1][\"text\"],\n",
    "    )\n",
    "    eval_result = parse_eval_result(data_entry[\"question\"], result)\n",
    "\n",
    "    # flip A and B for addressing position bias\n",
    "    result = await flipped_gpt_4_context.llm_predictor.apredict(\n",
    "        prompt=DEFAULT_EVAL_TEMPLATE,\n",
    "        question=data_entry[\"question\"],\n",
    "        source=data_entry[\"source\"],\n",
    "        answer_a=data_entry[\"answers\"][1][\"text\"],\n",
    "        answer_b=data_entry[\"answers\"][0][\"text\"],\n",
    "    )\n",
    "    flipped_eval_result = parse_eval_result(data_entry[\"question\"], result)\n",
    "\n",
    "    # merge result\n",
    "    final_eval_result, judgement_source = resolve_results(\n",
    "        eval_result,\n",
    "        flipped_eval_result,\n",
    "    )\n",
    "\n",
    "    # save final result\n",
    "    judgement = {}\n",
    "    judgement[\"llm\"] = \"gpt_4\"\n",
    "    judgement[\"score\"] = final_eval_result.score\n",
    "    judgement[\"text\"] = final_eval_result.response\n",
    "    judgement[\"source\"] = judgement_source\n",
    "    data_entry[\"evaluations\"] = [judgement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e488bbc-2dcc-459c-a4f0-ca1b6f79981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these generations for future use\n",
    "import json\n",
    "\n",
    "with open(\"qa_dataset.jsonl\", \"w\") as outfile:\n",
    "    for entry in dataset:\n",
    "        print(json.dumps(entry), file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d6e45-b6c6-4694-8aee-fbba853d57a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 19 examples to finetuning_events.jsonl\n",
      "Wrote 19 examples to finetuning_events.jsonl\n"
     ]
    }
   ],
   "source": [
    "finetuning_handler.save_finetuning_events(\"finetuning_events.jsonl\")\n",
    "flipped_finetuning_handler.save_finetuning_events(\"finetuning_events.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05e278-0f94-4aa1-a745-6ad02f7ee99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fine_tuning_examples master dataset\n",
    "with open(\"finetuning_events.jsonl\") as f:\n",
    "    finetuning_events = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"finetuning_events.jsonl\") as f:\n",
    "    flipped_finetuning_events = [json.loads(line) for line in f]\n",
    "\n",
    "assert len(finetuning_events) == len(flipped_finetuning_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6e0a5-a8a3-4270-a514-a80f1e2aaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_finetuning_events = []\n",
    "for ix, data_entry in enumerate(dataset):\n",
    "    if data_entry[\"evaluations\"][0][\"source\"] == \"original\":\n",
    "        final_finetuning_events += [finetuning_events[ix]]\n",
    "    elif data_entry[\"evaluations\"][0][\"source\"] == \"flipped\":\n",
    "        final_finetuning_events += [flipped_finetuning_events[ix]]\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e56e30-9676-418d-bb57-a0ed90909b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_finetuning_events.jsonl\", \"w\") as outfile:\n",
    "    for entry in final_finetuning_events:\n",
    "        print(json.dumps(entry), file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058f316-3b46-4ded-97a9-8a01c798690a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_finetuning_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cef380-6e81-4919-a080-ce4ddaa62cf5",
   "metadata": {},
   "source": [
    "Let's just see how one of these looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd4601-289e-4fba-bc74-b9a0cfcce09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_55975_row0_col3, #T_55975_row0_col5 {\n",
       "  inline-size: 300px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_55975\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_55975_level0_col0\" class=\"col_heading level0 col0\" >Question</th>\n",
       "      <th id=\"T_55975_level0_col1\" class=\"col_heading level0 col1\" >Source</th>\n",
       "      <th id=\"T_55975_level0_col2\" class=\"col_heading level0 col2\" >Model A</th>\n",
       "      <th id=\"T_55975_level0_col3\" class=\"col_heading level0 col3\" >Answer A</th>\n",
       "      <th id=\"T_55975_level0_col4\" class=\"col_heading level0 col4\" >Model B</th>\n",
       "      <th id=\"T_55975_level0_col5\" class=\"col_heading level0 col5\" >Answer B</th>\n",
       "      <th id=\"T_55975_level0_col6\" class=\"col_heading level0 col6\" >Score</th>\n",
       "      <th id=\"T_55975_level0_col7\" class=\"col_heading level0 col7\" >Judgement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_55975_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_55975_row0_col0\" class=\"data row0 col0\" >What is an example of a concept that would likely be known to a sufficiently advanced alien civilization?</td>\n",
       "      <td id=\"T_55975_row0_col1\" class=\"data row0 col1\" >[19] One way to get more precise about the concept of invented vs discovered is to talk about space aliens. Any sufficiently advanced alien civilization would certainly know about the Pythagorean theorem, for example. I believe, though with less certainty, that they would also know about the Lisp in McCarthy's 1960 paper.\n",
       "\n",
       "But if so there's no reason to suppose that this is the limit of the language that might be known to them. Presumably aliens need numbers and errors and I/O too. So it seems likely there exists at least one path out of McCarthy's Lisp along which discoveredness is preserved.\n",
       "\n",
       "\n",
       "\n",
       "Thanks to Trevor Blackwell, John Collison, Patrick Collison, Daniel Gackle, Ralph Hazell, Jessica Livingston, Robert Morris, and Harj Taggar for reading drafts of this....</td>\n",
       "      <td id=\"T_55975_row0_col2\" class=\"data row0 col2\" >llama2-7b-chat</td>\n",
       "      <td id=\"T_55975_row0_col3\" class=\"data row0 col3\" > The Pythagorean theorem is an example of a concept that would likely be known to a sufficiently advanced alien civilization.\n",
       "Explanation:  The Pythagorean theorem is a fundamental concept in mathematics that has been widely studied and applied across various fields, including physics, engineering, and computer science. It is a simple yet powerful idea that can be easily understood and applied, making it a likely candidate for a concept that would be known to a sufficiently advanced alien civilization.\n",
       "\n",
       "Additionally, the author of the passage suggests that any sufficiently advanced alien civilization would likely have a similar understanding of numbers, errors, and I/O, which are fundamental components of computer science and programming. Therefore, it is likely that a sufficiently advanced alien civilization would be familiar with concepts like Lisp, which is a programming language that is widely used in computer science and artificial intelligence.</td>\n",
       "      <td id=\"T_55975_row0_col4\" class=\"data row0 col4\" >mistral-7b-instruct</td>\n",
       "      <td id=\"T_55975_row0_col5\" class=\"data row0 col5\" >\n",
       "\n",
       "An example of a concept that would likely be known to a sufficiently advanced alien civilization is the Pythagorean theorem.</td>\n",
       "      <td id=\"T_55975_row0_col6\" class=\"data row0 col6\" >1.000000</td>\n",
       "      <td id=\"T_55975_row0_col7\" class=\"data row0 col7\" >Assistant A provides a more detailed and comprehensive response. While both assistants correctly identify the Pythagorean theorem as an example of a concept that would likely be known to a sufficiently advanced alien civilization, Assistant A goes further to explain why this is the case. Assistant A also mentions other concepts such as numbers, errors, and I/O, and the Lisp programming language, which are suggested in the provided context as likely known to an advanced alien civilization. Therefore, Assistant A's response is more helpful, accurate, and detailed. \n",
       "\n",
       "Final Verdict: [[A]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x29492c220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see the last one\n",
    "display_eval_df(\n",
    "    question=data_entry[\"question\"],\n",
    "    source=data_entry[\"source\"],\n",
    "    answer_a=data_entry[\"answers\"][0],\n",
    "    answer_b=data_entry[\"answers\"][1],\n",
    "    result=final_eval_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb16d0-15b1-45f3-96b0-3b51574d1626",
   "metadata": {},
   "source": [
    "## 2 Perform knowledge distillation\n",
    "\n",
    "Okay, it's now time to distill some knowledge from GPT-4 to GPT-3.5 To do this, we will make use of `OpenAIFinetuneEngine` class of `llama_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab328da-c235-4ae6-a7ff-4315fa0b07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import OpenAIFinetuneEngine\n",
    "\n",
    "finetune_engine = OpenAIFinetuneEngine(\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"final_finetuning_events.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd14ca3-97c5-4a1c-a257-6ed183b09968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 17\n",
      "First example:\n",
      "{'role': 'system', 'content': \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI question-answering assistants to the user question along with the retrieved context which was provided to both assistants are displayed below. You should choose the assistant that follows the user’s instructions and answers the user’s question better using the provided context. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: '[[A]]' if assistant A is better, '[[B]]' if assistant B is better, and '[[C]]' for a tie.\\n\"}\n",
      "{'role': 'user', 'content': '[User Question]\\nWhat was the author\\'s first experience with programming on a microcomputer?\\n\\n[The Start Retrieved Source]\\nWhat I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the...\\n[The End of Retrieved Source]\\n\\n[The Start of Assistant A’s Answer]\\n\\n\\nThe author\\'s first experience with programming on a microcomputer was when he built a Heathkit computer in about 1978. The author was envious of his friend and impressed by his ability to program the computer directly. However, the author soon realized that there was an unbridgeable gap between what they could do and actually understanding natural language. It was not, in fact, simply a matter of teaching SHRDLU more words. That whole way of doing AI, with explicit data structures representing concepts, was not going to work. The author then looked around to see what he could salvage from the wreckage of his plans and found Lisp. He decided to focus on Lisp and wrote a book about Lisp hacking. The author was drawn to computer science because it was an uneasy alliance between two halves, theory and systems. He wanted to build things, but he realized that any program he wrote today would be obsolete in a couple decades at best.\\n[The End of Assistant A’s Answer]\\n\\n[The Start of Assistant B’s Answer]\\n\\nThe author\\'s first experience with programming on a microcomputer was with the TRS-80.\\n[The End of Assistant B’s Answer]'}\n",
      "{'role': 'assistant', 'content': \"Both Assistant A and Assistant B do not provide accurate responses based on the provided context. The context mentions that the author's first experience with programming was on an IBM 1401 using an early version of Fortran. Assistant A talks about the author building a Heathkit computer and learning Lisp, while Assistant B mentions a TRS-80. Neither of these details are present in the provided context. Therefore, both responses are incorrect and irrelevant to the user's question. \\n\\nFinal Verdict: [[C]] for a tie.\"}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 677, 958\n",
      "mean / median: 843.5882352941177, 874.0\n",
      "p5 / p95: 734.2, 951.8\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 67, 156\n",
      "mean / median: 107.58823529411765, 105.0\n",
      "p5 / p95: 79.8, 134.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~14341 tokens that will be charged for during training\n",
      "By default, you'll train for 5 epochs on this dataset\n",
      "By default, you'll be charged for ~71705 tokens\n",
      "As of August 22, 2023, fine-tuning gpt-3.5-turbo is $0.008 / 1K Tokens.\n",
      "This means your total cost for training will be $0.11472800000000001 per epoch.\n"
     ]
    }
   ],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03fbfd9-40c8-4e95-baa2-7148fa1d4f57",
   "metadata": {},
   "source": [
    "We can check the status of our current job as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882690c-b072-406c-a3d2-91ffca762a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-Sov8D6e31JITVoN8oDi9mB21 at 0x1744e1440> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-Sov8D6e31JITVoN8oDi9mB21\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1698706048,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-1ZDAvajC6v2ZtAP9hLEIsXRz\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"queued\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-GEiTBygEV5QFEq8Vgf4eniUM\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 5\n",
       "  },\n",
       "  \"trained_tokens\": null,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_engine.get_current_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e631ccf-b4f5-478d-b112-52dc88ffed1e",
   "metadata": {},
   "source": [
    "## 3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0324b-5b3f-4b9b-b943-8e0c411ce9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_llm = finetune_engine.get_finetuned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106759f5-010e-4d00-90da-44c94db28991",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_context = ServiceContext.from_defaults(\n",
    "    llm=ft_llm,\n",
    ")\n",
    "\n",
    "# a non-fine-tuned judge\n",
    "gpt_3p5_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a888e-865c-48de-a5c0-b98daa992904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [01:00<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# predicting on training set for now just to get rest of pipeline established\n",
    "for data_entry in tqdm.tqdm(dataset):\n",
    "    question = data_entry[\"question\"]\n",
    "\n",
    "    result = await ft_context.llm_predictor.apredict(\n",
    "        prompt=DEFAULT_EVAL_TEMPLATE,\n",
    "        question=data_entry[\"question\"],\n",
    "        source=data_entry[\"source\"],\n",
    "        answer_a=data_entry[\"answers\"][0][\"text\"],\n",
    "        answer_b=data_entry[\"answers\"][1][\"text\"],\n",
    "    )\n",
    "    eval_result = parse_eval_result(data_entry[\"question\"], result)\n",
    "\n",
    "    # flip A and B for addressing position bias\n",
    "    flipped_result = await ft_context.llm_predictor.apredict(\n",
    "        prompt=DEFAULT_EVAL_TEMPLATE,\n",
    "        question=data_entry[\"question\"],\n",
    "        source=data_entry[\"source\"],\n",
    "        answer_a=data_entry[\"answers\"][1][\"text\"],\n",
    "        answer_b=data_entry[\"answers\"][0][\"text\"],\n",
    "    )\n",
    "    flipped_eval_result = parse_eval_result(\n",
    "        data_entry[\"question\"], flipped_result\n",
    "    )\n",
    "\n",
    "    # merge result\n",
    "    final_eval_result, judgement_source = resolve_results(\n",
    "        eval_result,\n",
    "        flipped_eval_result,\n",
    "    )\n",
    "\n",
    "    # save final result\n",
    "    judgement = {}\n",
    "    judgement[\"llm\"] = \"ft_gpt_3p5\"\n",
    "    judgement[\"score\"] = final_eval_result.score\n",
    "    judgement[\"text\"] = final_eval_result.response\n",
    "    judgement[\"source\"] = judgement_source\n",
    "    data_entry[\"evaluations\"] += [judgement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2caba7-6e8e-4daa-9145-807f9670a6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [01:45<00:00,  5.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# predicting on training set for now just to get rest of pipeline established\n",
    "for data_entry in tqdm.tqdm(dataset):\n",
    "    question = data_entry[\"question\"]\n",
    "\n",
    "    result = await gpt_3p5_context.llm_predictor.apredict(\n",
    "        prompt=DEFAULT_EVAL_TEMPLATE,\n",
    "        question=data_entry[\"question\"],\n",
    "        source=data_entry[\"source\"],\n",
    "        answer_a=data_entry[\"answers\"][0][\"text\"],\n",
    "        answer_b=data_entry[\"answers\"][1][\"text\"],\n",
    "    )\n",
    "    try:\n",
    "        eval_result = parse_eval_result(data_entry[\"question\"], result)\n",
    "    except:\n",
    "        eval_result = EvaluationResult(\n",
    "            query=eval_result.query,\n",
    "            response=\"\",\n",
    "            passing=None,\n",
    "            score=0.5,\n",
    "            feedback=\"Didn't follow output criteria.\",\n",
    "        )\n",
    "\n",
    "    # flip A and B for addressing position bias\n",
    "    flipped_result = await gpt_3p5_context.llm_predictor.apredict(\n",
    "        prompt=DEFAULT_EVAL_TEMPLATE,\n",
    "        question=data_entry[\"question\"],\n",
    "        source=data_entry[\"source\"],\n",
    "        answer_a=data_entry[\"answers\"][1][\"text\"],\n",
    "        answer_b=data_entry[\"answers\"][0][\"text\"],\n",
    "    )\n",
    "    flipped_eval_result = parse_eval_result(\n",
    "        data_entry[\"question\"], flipped_result\n",
    "    )\n",
    "\n",
    "    # merge result\n",
    "    final_eval_result, judgement_source = resolve_results(\n",
    "        eval_result,\n",
    "        flipped_eval_result,\n",
    "    )\n",
    "\n",
    "    # save final result\n",
    "    judgement = {}\n",
    "    judgement[\"llm\"] = \"gpt_3p5\"\n",
    "    judgement[\"score\"] = final_eval_result.score\n",
    "    judgement[\"text\"] = final_eval_result.response\n",
    "    judgement[\"source\"] = judgement_source\n",
    "    data_entry[\"evaluations\"] += [judgement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87454f07-6c25-498f-b689-0f38fc0da232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store for future analyses\n",
    "with open(\"qa_dataset_complete.jsonl\", \"w\") as outfile:\n",
    "    for entry in dataset:\n",
    "        print(json.dumps(entry), file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35120c06-3218-4317-ad40-b94df1d6ca14",
   "metadata": {},
   "source": [
    "Let's now compute the agreement rates between the llm-judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63636af6-326c-41de-abdb-d54e7b59fb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: inconclusive\n",
      "11: inconclusive\n",
      "12: inconclusive\n",
      "14: inconclusive\n",
      "16: inconclusive\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "scores = {\"gpt_4\": [], \"gpt_3p5\": [], \"ft_gpt_3p5\": []}\n",
    "for ix, d in enumerate(dataset):\n",
    "    responses = [\n",
    "        el[\"text\"] for el in d[\"evaluations\"]\n",
    "    ]  # remove any inconclusive results from any of the judges\n",
    "    if any(t == \"\" for t in responses):\n",
    "        print(f\"{ix}: inconclusive\")\n",
    "        continue\n",
    "    for e in d[\"evaluations\"]:\n",
    "        scores[e[\"llm\"]].append(e[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c6ba7-4346-41ed-a5fb-885ac913c0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3.5 w/ fine-tuning\n",
      "----------------\n",
      "Number of agreements with GPT-4: 8\n",
      "agreement rate: 0.6153846153846154\n",
      "\n",
      "\n",
      "GPT-3.5 w/out fine-tuning\n",
      "----------------\n",
      "Number of agreements with GPT-4: 7\n",
      "agreement rate: 0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "# agreement rates\n",
    "agreement_ft = sum(\n",
    "    x == y for x, y in zip(scores[\"gpt_4\"], scores[\"ft_gpt_3p5\"])\n",
    ")\n",
    "agreement_no_ft = sum(\n",
    "    x == y for x, y in zip(scores[\"gpt_4\"], scores[\"gpt_3p5\"])\n",
    ")\n",
    "\n",
    "# final report\n",
    "print(\n",
    "    f\"GPT-3.5 w/ fine-tuning\\n----------------\\nNumber of agreements with GPT-4: {agreement_ft}\\nagreement rate: {agreement_ft/len(scores['gpt_4'])}\"\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(\n",
    "    f\"GPT-3.5 w/out fine-tuning\\n----------------\\nNumber of agreements with GPT-4: {agreement_no_ft}\\nagreement rate: {agreement_no_ft/len(scores['gpt_4'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49eda7a-aa65-477f-b73a-d1cb79569711",
   "metadata": {},
   "source": [
    "So, we can see that with fine-tuning our GPT-3.5 model gets close to GPT-4 and thus, by proxy, closer to human judgement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
