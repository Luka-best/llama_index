{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How to Finetune a cross-encoder using LLamaIndex, push the trained model to HuggingFace Hub and the recorded improvements due to CrossEncoder Finetuning"
   ],
   "metadata": {
    "id": "NQqX8tcQr4da"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Download Requirements\n",
    "!pip install datasets --quiet\n",
    "!pip install sentence-transformers --quiet\n",
    "!pip install openai --quiet"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:08:44.807363Z",
     "iopub.execute_input": "2023-10-02T10:08:44.807991Z",
     "iopub.status.idle": "2023-10-02T10:09:20.329820Z",
     "shell.execute_reply.started": "2023-10-02T10:08:44.807958Z",
     "shell.execute_reply": "2023-10-02T10:09:20.328381Z"
    },
    "trusted": true,
    "id": "2RhgoOwVron4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process\n",
    "\n",
    "- Download the QASPER Dataset from HuggingFace Hub using Datasets Library (https://huggingface.co/datasets/allenai/qasper)\n",
    "\n",
    "- From the train and test splits of the dataset extract 800 and 80 samples respectively\n",
    "\n",
    "- Use the 800 samples collected from train data which have the respective questions framed on a research paper to generate a dataset in the respective format required for CrossEncoder finetuning. Currently the format we use is that a single sample of fine tune data consists of two sentences(question and context) and a score either 0 or 1 where 1 shows that the question and context are relevant to each other and 0 shows they are not relevant to each other.\n",
    "\n",
    "- Use the 100 samples of test set to extract two kinds of evaluation datasets\n",
    "  * Rag Eval Dataset:-One dataset consists of samples where a single sample consists of a research paper content, list of questions on the research paper, answers of the list of questions on the research paper. While forming this dataset we keep only questions which have long answers/ free-form answers for better comparision with RAG generated answers.\n",
    "\n",
    "  * Reranking Eval Dataset:- The other datasets consists of samples where a single sample consists of the research paper content, list of questions on the research paper, list of contexts from the research paper contents relevant to each question\n",
    "\n",
    "- We finetuned the cross-encoder using helper utilities written in llamaindex and push it to HuggingFace Hub using the huggingface cli tokens login which can be found here:- https://huggingface.co/settings/tokens\n",
    "\n",
    "- We evaluate on both datasets using two metrics and three cases\n",
    "   * Cases:-\n",
    "     - Just OpenAI embeddings without any reranker\n",
    "     - OpenAI embeddings combined with cross-encoder/ms-marco-MiniLM-L-12-v2 as reranker\n",
    "     - OpenAI embeddings combined with our fine-tuned cross encoder model as reranker\n",
    "\n",
    "  * Evaluation Criteria for each Eval Dataset\n",
    "    - Hits metric:- For evaluating the Reranking Eval Dataset we just simply use the retriever+ post-processor functionalities of LLamaIndex to see in the different cases how many times does the relevant context gets retrieved and call it the hits metric.\n",
    "\n",
    "    - Pairwise Comparision Evaluator:- We use the Pairwise Comparision Evaluator provided by LLamaIndex (https://github.com/run-llama/llama_index/blob/main/llama_index/evaluation/pairwise.py) to compare the responses of the respective query engines created in each case with the reference free-form answers provided.\n"
   ],
   "metadata": {
    "id": "LIxJS1PivP8S"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the Dataset"
   ],
   "metadata": {
    "id": "IC7n2OA6zrwo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Download QASPER dataset from HuggingFace https://huggingface.co/datasets/allenai/qasper\n",
    "dataset = load_dataset(\"allenai/qasper\")\n",
    "\n",
    "# Split the dataset into train, validation, and test splits\n",
    "train_dataset = dataset[\"train\"]\n",
    "validation_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "random.seed(42)  # Set a random seed for reproducibility\n",
    "\n",
    "# Randomly sample 800 rows from the training split\n",
    "train_sampled_indices = random.sample(range(len(train_dataset)), 800)\n",
    "train_samples = [train_dataset[i] for i in train_sampled_indices]\n",
    "\n",
    "\n",
    "# Randomly sample 100 rows from the test split\n",
    "test_sampled_indices = random.sample(range(len(test_dataset)), 80)\n",
    "test_samples = [test_dataset[i] for i in test_sampled_indices]\n",
    "\n",
    "# Now we have 800 research papers for training and 80 research papers to evaluate on"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T12:31:44.352255Z",
     "iopub.execute_input": "2023-10-01T12:31:44.352623Z",
     "iopub.status.idle": "2023-10-01T12:31:45.668861Z",
     "shell.execute_reply.started": "2023-10-01T12:31:44.352596Z",
     "shell.execute_reply": "2023-10-01T12:31:45.667528Z"
    },
    "trusted": true,
    "colab": {
     "referenced_widgets": [
      "0442e0b9d4c848efae67e2d3bb8f76ee"
     ]
    },
    "id": "yG_6WZx8ron7",
    "outputId": "62a60b40-42b1-42a3-86f5-8f9af641a6fa"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0442e0b9d4c848efae67e2d3bb8f76ee"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Structure of each sample row of the QASPER Dataset\n",
    "* Each row has the below 6 columns\n",
    "    - id: Unique identifier of the research paper\n",
    "\n",
    "    - title: Title of the Research paper\n",
    "\n",
    "    - abstract: Abstract of the research paper\n",
    "\n",
    "    - full_text: full text of the research paper\n",
    "\n",
    "    - qas: Questions and answers pertaining to each research paper\n",
    "\n",
    "    - figures_and_tables: figures and tables of each research paper\n"
   ],
   "metadata": {
    "id": "Iep7pXXrron8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Get full text paper data , questions on the paper from training samples of QASPER to generate training dataset for cross-encoder finetuning\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Utility function to get full-text of the research papers from the dataset\n",
    "def get_full_text(sample: dict) -> str:\n",
    "    \"\"\"\n",
    "    :param dict sample: the row sample from QASPER\n",
    "    \"\"\"\n",
    "    title = sample[\"title\"]\n",
    "    abstract = sample[\"abstract\"]\n",
    "    sections_list = sample[\"full_text\"][\"section_name\"]\n",
    "    paragraph_list = sample[\"full_text\"][\"paragraphs\"]\n",
    "    combined_sections_with_paras = \"\"\n",
    "    if len(sections_list) == len(paragraph_list):\n",
    "        combined_sections_with_paras += title + \"\\t\"\n",
    "        combined_sections_with_paras += abstract + \"\\t\"\n",
    "        for index in range(0, len(sections_list)):\n",
    "            combined_sections_with_paras += str(sections_list[index]) + \"\\t\"\n",
    "            combined_sections_with_paras += \"\".join(paragraph_list[index])\n",
    "        return combined_sections_with_paras\n",
    "\n",
    "    else:\n",
    "        print(\"Not the same number of sections as paragraphs list\")\n",
    "\n",
    "\n",
    "# utility function to extract list of questions from the dataset\n",
    "def get_questions(sample: dict) -> List[str]:\n",
    "    \"\"\"\n",
    "    :param dict sample: the row sample from QASPER\n",
    "    \"\"\"\n",
    "    questions_list = sample[\"qas\"][\"question\"]\n",
    "    return questions_list\n",
    "\n",
    "\n",
    "doc_qa_dict_list = []\n",
    "\n",
    "for train_sample in train_samples:\n",
    "    full_text = get_full_text(train_sample)\n",
    "    questions_list = get_questions(train_sample)\n",
    "    local_dict = {\"paper\": full_text, \"questions\": questions_list}\n",
    "    doc_qa_dict_list.append(local_dict)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T12:31:45.672683Z",
     "iopub.execute_input": "2023-10-01T12:31:45.673236Z",
     "iopub.status.idle": "2023-10-01T12:31:45.742292Z",
     "shell.execute_reply.started": "2023-10-01T12:31:45.673202Z",
     "shell.execute_reply": "2023-10-01T12:31:45.741229Z"
    },
    "trusted": true,
    "id": "dz5QlupYron-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(doc_qa_dict_list)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T12:31:46.104595Z",
     "iopub.execute_input": "2023-10-01T12:31:46.105230Z",
     "iopub.status.idle": "2023-10-01T12:31:46.110943Z",
     "shell.execute_reply.started": "2023-10-01T12:31:46.105199Z",
     "shell.execute_reply": "2023-10-01T12:31:46.110026Z"
    },
    "trusted": true,
    "id": "gRjXE7e7ron-",
    "outputId": "649cbc4b-d748-4527-a1b3-3ef439afee6e"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "800"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save training data as a csv\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.DataFrame(doc_qa_dict_list)\n",
    "df_train.to_csv(\"train.csv\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T12:31:46.552373Z",
     "iopub.execute_input": "2023-10-01T12:31:46.552741Z",
     "iopub.status.idle": "2023-10-01T12:31:47.087640Z",
     "shell.execute_reply.started": "2023-10-01T12:31:46.552715Z",
     "shell.execute_reply": "2023-10-01T12:31:47.086447Z"
    },
    "trusted": true,
    "id": "fV4L7o0Kron_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate RAG Eval dataset"
   ],
   "metadata": {
    "id": "UaMjSH_H2RiN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Get evaluation data papers , questions and answers\n",
    "\"\"\"\n",
    "The Answers field in the dataset follow the below format:-\n",
    "Unanswerable answers have \"unanswerable\" set to true.\n",
    "\n",
    "The remaining answers have exactly one of the following fields being non-empty.\n",
    "\n",
    "\"extractive_spans\" are spans in the paper which serve as the answer.\n",
    "\"free_form_answer\" is a written out answer.\n",
    "\"yes_no\" is true iff the answer is Yes, and false iff the answer is No.\n",
    "\n",
    "We accept only free-form answers and for all the other kind of answers we set their value to 'Unacceptable',\n",
    "to better evaluate the performance of the query engine using pairwise comparision evaluator as it uses GPT-4 which is biased towards preferring long answers more.\n",
    "https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1\n",
    "\n",
    "So in the case of 'yes_no' answers it can favour Query Engine answers more than reference answers.\n",
    "Also in the case of extracted spans it can favour reference answers more than Query engine generated answers.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "eval_doc_qa_answer_list = []\n",
    "\n",
    "\n",
    "# Utility function to extract answers from the dataset\n",
    "def get_answers(sample: dict) -> List[str]:\n",
    "    \"\"\"\n",
    "    :param dict sample: the row sample from the train split of QASPER\n",
    "    \"\"\"\n",
    "    final_answers_list = []\n",
    "    answers = sample[\"qas\"][\"answers\"]\n",
    "    for answer in answers:\n",
    "        local_answer = \"\"\n",
    "        types_of_answers = answer[\"answer\"][0]\n",
    "        if types_of_answers[\"unanswerable\"] == False:\n",
    "            if types_of_answers[\"free_form_answer\"] != \"\":\n",
    "                local_answer = types_of_answers[\"free_form_answer\"]\n",
    "            else:\n",
    "                local_answer = \"Unacceptable\"\n",
    "        else:\n",
    "            local_answer = \"Unacceptable\"\n",
    "\n",
    "        final_answers_list.append(local_answer)\n",
    "\n",
    "    return final_answers_list\n",
    "\n",
    "\n",
    "for test_sample in test_samples:\n",
    "    full_text = get_full_text(test_sample)\n",
    "    questions_list = get_questions(test_sample)\n",
    "    answers_list = get_answers(test_sample)\n",
    "    local_dict = {\n",
    "        \"paper\": full_text,\n",
    "        \"questions\": questions_list,\n",
    "        \"answers\": answers_list,\n",
    "    }\n",
    "    eval_doc_qa_answer_list.append(local_dict)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T12:31:47.145032Z",
     "iopub.execute_input": "2023-10-01T12:31:47.145310Z",
     "iopub.status.idle": "2023-10-01T12:31:47.156963Z",
     "shell.execute_reply.started": "2023-10-01T12:31:47.145286Z",
     "shell.execute_reply": "2023-10-01T12:31:47.156072Z"
    },
    "trusted": true,
    "id": "ksT5Jducron_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(eval_doc_qa_answer_list)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T12:31:47.731793Z",
     "iopub.execute_input": "2023-10-01T12:31:47.732171Z",
     "iopub.status.idle": "2023-10-01T12:31:47.738454Z",
     "shell.execute_reply.started": "2023-10-01T12:31:47.732145Z",
     "shell.execute_reply": "2023-10-01T12:31:47.737539Z"
    },
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEt2dz_IrooA",
    "outputId": "59b3894f-a887-4fb1-d3ad-0801ddcd5aba"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "80\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save eval data as a csv\n",
    "import pandas as pd\n",
    "\n",
    "df_test = pd.DataFrame(eval_doc_qa_answer_list)\n",
    "df_test.to_csv(\"test.csv\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T12:31:48.526891Z",
     "iopub.execute_input": "2023-10-01T12:31:48.527533Z",
     "iopub.status.idle": "2023-10-01T12:31:48.594546Z",
     "shell.execute_reply.started": "2023-10-01T12:31:48.527502Z",
     "shell.execute_reply": "2023-10-01T12:31:48.593385Z"
    },
    "trusted": true,
    "id": "eARlxgEUrooA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pulling my github branch with changes in LLamaIndex to support Cross-encoder fine-tuning and pushing the trained model to HuggingFace Hub"
   ],
   "metadata": {
    "id": "q-DV7n5q1o3j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Download the branch with changes from https://github.com/bhavishpahwa/llama_index.git\n",
    "!git clone -b feat/add_cross_encoder_finetuning https://github.com/bhavishpahwa/llama_index.git"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-10-02T07:31:31.726613Z",
     "iopub.execute_input": "2023-10-02T07:31:31.726946Z",
     "iopub.status.idle": "2023-10-02T07:31:39.650527Z",
     "shell.execute_reply.started": "2023-10-02T07:31:31.726911Z",
     "shell.execute_reply": "2023-10-02T07:31:39.649536Z"
    },
    "trusted": true,
    "id": "Qe-WNrIJrooA",
    "outputId": "39ecd190-44b6-4d95-9c9e-0a3bb96a3b26"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Cloning into 'llama_index'...\nremote: Enumerating objects: 21164, done.\u001b[K\nremote: Counting objects: 100% (7/7), done.\u001b[K\nremote: Compressing objects: 100% (7/7), done.\u001b[K\nremote: Total 21164 (delta 0), reused 6 (delta 0), pack-reused 21157\u001b[K\nReceiving objects: 100% (21164/21164), 61.72 MiB | 12.88 MiB/s, done.\nResolving deltas: 100% (14320/14320), done.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/llama_index"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T07:31:39.653139Z",
     "iopub.execute_input": "2023-10-02T07:31:39.653476Z",
     "iopub.status.idle": "2023-10-02T07:31:39.660962Z",
     "shell.execute_reply.started": "2023-10-02T07:31:39.653447Z",
     "shell.execute_reply": "2023-10-02T07:31:39.659690Z"
    },
    "trusted": true,
    "id": "PBSffYXmrooB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Download all requirements of llama_index\n",
    "!pip install -r requirements.txt --quiet"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T07:31:39.664037Z",
     "iopub.execute_input": "2023-10-02T07:31:39.664442Z",
     "iopub.status.idle": "2023-10-02T07:32:26.784819Z",
     "shell.execute_reply.started": "2023-10-02T07:31:39.664404Z",
     "shell.execute_reply": "2023-10-02T07:32:26.783834Z"
    },
    "trusted": true,
    "id": "JIIuJEEyrooB",
    "outputId": "e61bc014-22af-4e39-8f78-ffce261e54b1"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Add the repository path to the Python path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/content/llama_index\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T07:32:26.786212Z",
     "iopub.execute_input": "2023-10-02T07:32:26.786563Z",
     "iopub.status.idle": "2023-10-02T07:32:26.794502Z",
     "shell.execute_reply.started": "2023-10-02T07:32:26.786533Z",
     "shell.execute_reply": "2023-10-02T07:32:26.793436Z"
    },
    "trusted": true,
    "id": "Z8kQn6QQrooB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T07:32:26.796161Z",
     "iopub.execute_input": "2023-10-02T07:32:26.797003Z",
     "iopub.status.idle": "2023-10-02T07:32:26.810802Z",
     "shell.execute_reply.started": "2023-10-02T07:32:26.796962Z",
     "shell.execute_reply": "2023-10-02T07:32:26.809722Z"
    },
    "trusted": true,
    "id": "hrrRDKTLrooB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Finetuning Dataset"
   ],
   "metadata": {
    "id": "drseaRHR2Dw6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate the respective training dataset from the intial train data collected from QASPER in the format required by\n",
    "import os\n",
    "from llama_index import SimpleDirectoryReader\n",
    "import openai\n",
    "from llama_index.finetuning.cross_encoders.dataset_gen import (\n",
    "    generate_ce_fine_tuning_dataset,\n",
    "    generate_synthetic_queries_over_documents,\n",
    ")\n",
    "\n",
    "from llama_index.finetuning.cross_encoders.cross_encoder import (\n",
    "    CrossEncoderFinetuneEngine,\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T07:32:26.812508Z",
     "iopub.execute_input": "2023-10-02T07:32:26.812895Z",
     "iopub.status.idle": "2023-10-02T07:32:35.018455Z",
     "shell.execute_reply.started": "2023-10-02T07:32:26.812849Z",
     "shell.execute_reply": "2023-10-02T07:32:35.017552Z"
    },
    "trusted": true,
    "id": "yG8lchqxrooC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index import Document\n",
    "\n",
    "final_finetuning_data_list = []\n",
    "for paper in doc_qa_dict_list:\n",
    "    questions_list = paper[\"questions\"]\n",
    "    documents = [Document(text=paper[\"paper\"])]\n",
    "    local_finetuning_dataset = generate_ce_fine_tuning_dataset(\n",
    "        documents=documents,\n",
    "        questions_list=questions_list,\n",
    "        max_chunk_length=256,\n",
    "        top_k=5,\n",
    "    )\n",
    "    final_finetuning_data_list.extend(local_finetuning_dataset)"
   ],
   "metadata": {
    "trusted": true,
    "id": "k4MioksvrooC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Total samples in the final fine-tuning dataset\n",
    "len(final_finetuning_data_list)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T14:10:17.573031Z",
     "iopub.execute_input": "2023-10-01T14:10:17.573646Z",
     "iopub.status.idle": "2023-10-01T14:10:17.580307Z",
     "shell.execute_reply.started": "2023-10-01T14:10:17.573612Z",
     "shell.execute_reply": "2023-10-01T14:10:17.579371Z"
    },
    "trusted": true,
    "id": "5ZV-vJ1zrooC",
    "outputId": "e3ba3e54-2165-417e-8be9-20e4f03409eb"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 24,
     "output_type": "execute_result",
     "data": {
      "text/plain": "11674"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save final fine-tuning dataset\n",
    "import pandas as pd\n",
    "\n",
    "df_finetuning_dataset = pd.DataFrame(final_finetuning_data_list)\n",
    "df_finetuning_dataset.to_csv(\"fine_tuning.csv\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T14:10:17.581485Z",
     "iopub.execute_input": "2023-10-01T14:10:17.582290Z",
     "iopub.status.idle": "2023-10-01T14:10:18.032076Z",
     "shell.execute_reply.started": "2023-10-01T14:10:17.582260Z",
     "shell.execute_reply": "2023-10-01T14:10:18.031156Z"
    },
    "trusted": true,
    "id": "3eRsWh1zrooC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load fine-tuning dataset\n",
    "\n",
    "finetuning_dataset = final_finetuning_data_list"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T14:10:18.033583Z",
     "iopub.execute_input": "2023-10-01T14:10:18.034222Z",
     "iopub.status.idle": "2023-10-01T14:10:18.038474Z",
     "shell.execute_reply.started": "2023-10-01T14:10:18.034190Z",
     "shell.execute_reply": "2023-10-01T14:10:18.037425Z"
    },
    "trusted": true,
    "id": "7Kgv2M2GrooC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "finetuning_dataset[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T14:10:18.039617Z",
     "iopub.execute_input": "2023-10-01T14:10:18.040059Z",
     "iopub.status.idle": "2023-10-01T14:10:18.050415Z",
     "shell.execute_reply.started": "2023-10-01T14:10:18.039963Z",
     "shell.execute_reply": "2023-10-01T14:10:18.049462Z"
    },
    "trusted": true,
    "id": "UO09H4DTrooC",
    "outputId": "c699c5ea-d018-4842-cbfd-2b5d99040115"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 27,
     "output_type": "execute_result",
     "data": {
      "text/plain": "CrossEncoderFinetuningDatasetSample(query='Do they repot results only on English data?', context='addition to precision, recall, and F1 scores for both tasks, we show the average of the F1 scores across both tasks. On the ADE dataset, we achieve SOTA results for both the NER and RE tasks. On the CoNLL04 dataset, we achieve SOTA results on the NER task, while our performance on the RE task is competitive with other recent models. On both datasets, we achieve SOTA results when considering the average F1 score across both tasks. The largest gain relative to the previous SOTA performance is on the RE task of the ADE dataset, where we see an absolute improvement of 4.5 on the macro-average F1 score.While the model of Eberts and Ulges eberts2019span outperforms our proposed architecture on the CoNLL04 RE task, their results come at the cost of greater model complexity. As mentioned above, Eberts and Ulges fine-tune the BERTBASE model, which has 110 million trainable parameters. In contrast, given the hyperparameters used for final training on the CoNLL04 dataset, our proposed architecture has approximately 6 million trainable parameters.The fact that the optimal number of task-specific layers differed between the two datasets demonstrates the', score=0)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Reranking Eval dataset"
   ],
   "metadata": {
    "id": "4TxzQGWl2JKk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate Reranking Eval Dataset from the Eval data\n",
    "import pandas as pd\n",
    "import ast  # Used to safely evaluate the string as a list\n",
    "\n",
    "# Load Eval Data\n",
    "df_test = pd.read_csv(\"/content/test.csv\", index_col=0)\n",
    "\n",
    "df_test[\"questions\"] = df_test[\"questions\"].apply(ast.literal_eval)\n",
    "df_test[\"answers\"] = df_test[\"answers\"].apply(ast.literal_eval)\n",
    "print(f\"Number of papers in the test sample:- {len(df_test)}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T07:32:35.019455Z",
     "iopub.execute_input": "2023-10-02T07:32:35.019917Z",
     "iopub.status.idle": "2023-10-02T07:32:35.115207Z",
     "shell.execute_reply.started": "2023-10-02T07:32:35.019894Z",
     "shell.execute_reply": "2023-10-02T07:32:35.114187Z"
    },
    "trusted": true,
    "id": "sghWC8rZrooD",
    "outputId": "c3df1174-e8aa-4276-9326-78484315e03f"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Number of papers in the test sample:- 80\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index import Document\n",
    "\n",
    "final_eval_data_list = []\n",
    "for index, row in df_test.iterrows():\n",
    "    documents = [Document(text=row[\"paper\"])]\n",
    "    query_list = row[\"questions\"]\n",
    "    local_eval_dataset = generate_ce_fine_tuning_dataset(\n",
    "        documents=documents, questions_list=query_list, max_chunk_length=256, top_k=5\n",
    "    )\n",
    "    relevant_query_list = []\n",
    "    relevant_context_list = []\n",
    "\n",
    "    for item in local_eval_dataset:\n",
    "        if item.score == 1:\n",
    "            relevant_query_list.append(item.query)\n",
    "            relevant_context_list.append(item.context)\n",
    "\n",
    "    if len(relevant_query_list) > 0:\n",
    "        final_eval_data_list.append(\n",
    "            {\n",
    "                \"paper\": row[\"paper\"],\n",
    "                \"questions\": relevant_query_list,\n",
    "                \"context\": relevant_context_list,\n",
    "            }\n",
    "        )"
   ],
   "metadata": {
    "trusted": true,
    "id": "mYdbfMjMrooD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Length of Reranking Eval Dataset\n",
    "len(final_eval_data_list)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T07:57:58.808651Z",
     "iopub.execute_input": "2023-10-02T07:57:58.809287Z",
     "iopub.status.idle": "2023-10-02T07:57:58.816779Z",
     "shell.execute_reply.started": "2023-10-02T07:57:58.809252Z",
     "shell.execute_reply": "2023-10-02T07:57:58.816009Z"
    },
    "trusted": true,
    "id": "cM1r4CyQrooD",
    "outputId": "863b42a5-b416-4b27-ac6d-510297f90fae"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "38"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save Reranking eval dataset\n",
    "import pandas as pd\n",
    "\n",
    "df_finetuning_dataset = pd.DataFrame(final_eval_data_list)\n",
    "df_finetuning_dataset.to_csv(\"reranking_test.csv\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T07:57:58.817986Z",
     "iopub.execute_input": "2023-10-02T07:57:58.819086Z",
     "iopub.status.idle": "2023-10-02T07:57:58.863135Z",
     "shell.execute_reply.started": "2023-10-02T07:57:58.819060Z",
     "shell.execute_reply": "2023-10-02T07:57:58.862407Z"
    },
    "trusted": true,
    "id": "17zOQ69CrooD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Finetune Cross-Encoder and push the model to HuggingFace Hub"
   ],
   "metadata": {
    "id": "2-LyBwcOJHdE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install huggingface_hub --quiet"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T14:15:14.307355Z",
     "iopub.execute_input": "2023-10-01T14:15:14.308388Z",
     "iopub.status.idle": "2023-10-01T14:15:22.743912Z",
     "shell.execute_reply.started": "2023-10-01T14:15:14.308345Z",
     "shell.execute_reply": "2023-10-01T14:15:22.742629Z"
    },
    "trusted": true,
    "id": "q2eLfKMcrooD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T14:15:22.746551Z",
     "iopub.execute_input": "2023-10-01T14:15:22.746925Z",
     "iopub.status.idle": "2023-10-01T14:15:22.775082Z",
     "shell.execute_reply.started": "2023-10-01T14:15:22.746885Z",
     "shell.execute_reply": "2023-10-01T14:15:22.774173Z"
    },
    "trusted": true,
    "colab": {
     "referenced_widgets": [
      "1ed078bb7d4e49678ecfa42dc06a2398"
     ]
    },
    "id": "gSEqQ2eRrooD",
    "outputId": "aa9fe9e9-1dae-4b4a-be12-f640d3bd95f1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ed078bb7d4e49678ecfa42dc06a2398"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialise the cross-encoder fine-tuning engine\n",
    "finetuning_engine = CrossEncoderFinetuneEngine(\n",
    "    dataset=finetuning_dataset, epochs=2, batch_size=8\n",
    ")\n",
    "\n",
    "# Finetune the cross-encoder model\n",
    "finetuning_engine.finetune()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T14:18:50.297316Z",
     "iopub.execute_input": "2023-10-01T14:18:50.297955Z",
     "iopub.status.idle": "2023-10-01T14:24:05.360711Z",
     "shell.execute_reply.started": "2023-10-01T14:18:50.297922Z",
     "shell.execute_reply": "2023-10-01T14:24:05.359735Z"
    },
    "trusted": true,
    "colab": {
     "referenced_widgets": [
      "91f37d51eceb442885a371db97cf3381",
      "c331e837ed604a9ba04acdd723e8ea89",
      "7f6f2a6f61ad48c1a97bd2a5eb0bc26f"
     ]
    },
    "id": "CggshREirooE",
    "outputId": "b55d3152-434f-429b-81e2-4dfebe2184b3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91f37d51eceb442885a371db97cf3381"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Iteration:   0%|          | 0/1460 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c331e837ed604a9ba04acdd723e8ea89"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Iteration:   0%|          | 0/1460 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f6f2a6f61ad48c1a97bd2a5eb0bc26f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Push model to HuggingFace Hub\n",
    "finetuning_engine.push_to_hub(repo_id=\"bpHigh/Cross-Encoder-LLamaIndex-Demo-v2\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-01T14:24:05.363079Z",
     "iopub.execute_input": "2023-10-01T14:24:05.363411Z",
     "iopub.status.idle": "2023-10-01T14:24:14.002446Z",
     "shell.execute_reply.started": "2023-10-01T14:24:05.363379Z",
     "shell.execute_reply": "2023-10-01T14:24:14.001406Z"
    },
    "trusted": true,
    "colab": {
     "referenced_widgets": [
      "4d836ead969d49d2b35a56483bf09889"
     ]
    },
    "id": "f1htE3eorooE",
    "outputId": "d0656f1e-a4dc-4f92-a35f-862a8f19b48b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d836ead969d49d2b35a56483bf09889"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reranking Evaluation"
   ],
   "metadata": {
    "id": "QL9cd6mSrooF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Download the latest version of llama-index\n",
    "!pip install llama-index --quiet\n",
    "!pip install nest-asyncio --quiet"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:09:20.332102Z",
     "iopub.execute_input": "2023-10-02T10:09:20.332463Z",
     "iopub.status.idle": "2023-10-02T10:10:00.579295Z",
     "shell.execute_reply.started": "2023-10-02T10:09:20.332436Z",
     "shell.execute_reply": "2023-10-02T10:10:00.577924Z"
    },
    "trusted": true,
    "id": "SkcIiz6ErooF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# attach to the same event-loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:10:00.581056Z",
     "iopub.execute_input": "2023-10-02T10:10:00.582233Z",
     "iopub.status.idle": "2023-10-02T10:10:00.592413Z",
     "shell.execute_reply.started": "2023-10-02T10:10:00.582199Z",
     "shell.execute_reply": "2023-10-02T10:10:00.591452Z"
    },
    "trusted": true,
    "id": "PmtPKGVurooF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load Reranking Dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df_reranking = pd.read_csv(\"/content/reranking_test.csv\", index_col=0)\n",
    "df_reranking[\"questions\"] = df_reranking[\"questions\"].apply(ast.literal_eval)\n",
    "df_reranking[\"context\"] = df_reranking[\"context\"].apply(ast.literal_eval)\n",
    "print(f\"Number of papers in the reranking eval dataset:- {len(df_reranking)}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:45:56.855995Z",
     "iopub.execute_input": "2023-10-02T10:45:56.856902Z",
     "iopub.status.idle": "2023-10-02T10:45:56.912131Z",
     "shell.execute_reply.started": "2023-10-02T10:45:56.856868Z",
     "shell.execute_reply": "2023-10-02T10:45:56.910885Z"
    },
    "trusted": true,
    "id": "qawp33iCrooF",
    "outputId": "f7a55d88-ed0e-479c-8b6b-8251c276fd98"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Number of papers in the reranking eval dataset:- 38\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df_reranking.head(1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:45:57.885232Z",
     "iopub.execute_input": "2023-10-02T10:45:57.885600Z",
     "iopub.status.idle": "2023-10-02T10:45:57.896121Z",
     "shell.execute_reply.started": "2023-10-02T10:45:57.885576Z",
     "shell.execute_reply": "2023-10-02T10:45:57.895078Z"
    },
    "trusted": true,
    "id": "Fe6cBjhxrooF",
    "outputId": "74bb719a-b84f-4330-8180-73eb92ee3dce"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                               paper  \\\n0  Identifying Condition-Action Statements in Med...   \n\n                                           questions  \\\n0  [What supervised machine learning models do th...   \n\n                                             context  \n0  [Identifying Condition-Action Statements in Me...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paper</th>\n      <th>questions</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Identifying Condition-Action Statements in Med...</td>\n      <td>[What supervised machine learning models do th...</td>\n      <td>[Identifying Condition-Action Statements in Me...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# We evaluate by calculating hits for each (question, context) pair,\n",
    "# we retrieve top-k documents with the question, and\n",
    "# it’s a hit if the results contain the context\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    Response,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import Document\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "service_context_reranker_eval = ServiceContext.from_defaults(chunk_size=256)\n",
    "rerank_base = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\", top_n=3\n",
    ")\n",
    "\n",
    "rerank_finetuned = SentenceTransformerRerank(\n",
    "    model=\"bpHigh/Cross-Encoder-LLamaIndex-Demo-v2\", top_n=3\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:46:01.861334Z",
     "iopub.execute_input": "2023-10-02T10:46:01.862838Z",
     "iopub.status.idle": "2023-10-02T10:46:05.806002Z",
     "shell.execute_reply.started": "2023-10-02T10:46:01.862793Z",
     "shell.execute_reply": "2023-10-02T10:46:05.804693Z"
    },
    "trusted": true,
    "colab": {
     "referenced_widgets": [
      "34cf70bc3dbb48e2b1f1cf74836ec442",
      "31993884af454bfa835dbaec8d0a0be1",
      "bf39b437040f44af8bf41bf3d4a38a26",
      "42fbc3d09200448f81651b5ddcd5e773",
      "184ef6ea433747e9b2db933613db71ad",
      "447733e06f7347a181624c40e859e46d"
     ]
    },
    "id": "vLXcIJaLrooL",
    "outputId": "d94c2ef6-b6aa-4cd1-ff2a-7d079d7f410f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/854 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34cf70bc3dbb48e2b1f1cf74836ec442"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31993884af454bfa835dbaec8d0a0be1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf39b437040f44af8bf41bf3d4a38a26"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42fbc3d09200448f81651b5ddcd5e773"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "184ef6ea433747e9b2db933613db71ad"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "447733e06f7347a181624c40e859e46d"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "without_reranker_hits = 0\n",
    "base_reranker_hits = 0\n",
    "finetuned_reranker_hits = 0\n",
    "total_number_of_context = 0\n",
    "for index, row in df_reranking.iterrows():\n",
    "    documents = [Document(text=row[\"paper\"])]\n",
    "    query_list = row[\"questions\"]\n",
    "    context_list = row[\"context\"]\n",
    "\n",
    "    assert len(query_list) == len(context_list)\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents, service_context=service_context_reranker_eval\n",
    "    )\n",
    "\n",
    "    retriever_without_reranker = vector_index.as_query_engine(\n",
    "        similarity_top_k=3, response_mode=\"no_text\"\n",
    "    )\n",
    "    retriever_with_base_reranker = vector_index.as_query_engine(\n",
    "        similarity_top_k=8, response_mode=\"no_text\", node_postprocessors=[rerank_base]\n",
    "    )\n",
    "    retriever_with_finetuned_reranker = vector_index.as_query_engine(\n",
    "        similarity_top_k=8,\n",
    "        response_mode=\"no_text\",\n",
    "        node_postprocessors=[rerank_finetuned],\n",
    "    )\n",
    "\n",
    "    for index in range(0, len(query_list)):\n",
    "        query = query_list[index]\n",
    "        context = context_list[index]\n",
    "        total_number_of_context += 1\n",
    "\n",
    "        response_without_reranker = retriever_without_reranker.query(query)\n",
    "        without_reranker_nodes = response_without_reranker.source_nodes\n",
    "\n",
    "        for node in without_reranker_nodes:\n",
    "            if context in node.node.text or node.node.text in context:\n",
    "                without_reranker_hits += 1\n",
    "\n",
    "        response_with_base_reranker = retriever_with_base_reranker.query(query)\n",
    "        with_base_reranker_nodes = response_with_base_reranker.source_nodes\n",
    "\n",
    "        for node in with_base_reranker_nodes:\n",
    "            if context in node.node.text or node.node.text in context:\n",
    "                base_reranker_hits += 1\n",
    "\n",
    "        response_with_finetuned_reranker = retriever_with_finetuned_reranker.query(\n",
    "            query\n",
    "        )\n",
    "        with_finetuned_reranker_nodes = response_with_finetuned_reranker.source_nodes\n",
    "\n",
    "        for node in with_finetuned_reranker_nodes:\n",
    "            if context in node.node.text or node.node.text in context:\n",
    "                finetuned_reranker_hits += 1\n",
    "\n",
    "        assert (\n",
    "            len(with_finetuned_reranker_nodes)\n",
    "            == len(with_base_reranker_nodes)\n",
    "            == len(without_reranker_nodes)\n",
    "            == 3\n",
    "        )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:48:46.248334Z",
     "iopub.execute_input": "2023-10-02T10:48:46.248754Z",
     "iopub.status.idle": "2023-10-02T10:55:24.154368Z",
     "shell.execute_reply.started": "2023-10-02T10:48:46.248726Z",
     "shell.execute_reply": "2023-10-02T10:55:24.153613Z"
    },
    "trusted": true,
    "id": "8voIjR25rooM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results of Reranking Eval Dataset\n",
    "\n",
    "As we can see below we get more hits with finetuned_cross_encoder compared to other options."
   ],
   "metadata": {
    "id": "pLZCwFDCrooM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "without_reranker_scores = [without_reranker_hits]\n",
    "base_reranker_scores = [base_reranker_hits]\n",
    "finetuned_reranker_scores = [finetuned_reranker_hits]\n",
    "reranker_eval_dict = {\n",
    "    \"Metric\": \"Hits\",\n",
    "    \"OpenAI_Embeddings\": without_reranker_scores,\n",
    "    \"Base_cross_encoder\": base_reranker_scores,\n",
    "    \"Finetuned_cross_encoder\": finetuned_reranker_hits,\n",
    "    \"Total Relevant Context\": total_number_of_context,\n",
    "}\n",
    "df_reranker_eval_results = pd.DataFrame(reranker_eval_dict)\n",
    "display(df_reranker_eval_results)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:55:24.156127Z",
     "iopub.execute_input": "2023-10-02T10:55:24.156577Z",
     "iopub.status.idle": "2023-10-02T10:55:24.171307Z",
     "shell.execute_reply.started": "2023-10-02T10:55:24.156543Z",
     "shell.execute_reply": "2023-10-02T10:55:24.170268Z"
    },
    "trusted": true,
    "id": "rj1OHi16rooM",
    "outputId": "e1a96ad5-03b4-4872-e9f3-c703cc139fe2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  Metric  OpenAI_Embeddings  Base_cross_encoder  Finetuned_cross_encoder  \\\n0   Hits                 30                  34                       37   \n\n   Total Relevant Context  \n0                      85  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metric</th>\n      <th>OpenAI_Embeddings</th>\n      <th>Base_cross_encoder</th>\n      <th>Finetuned_cross_encoder</th>\n      <th>Total Relevant Context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hits</td>\n      <td>30</td>\n      <td>34</td>\n      <td>37</td>\n      <td>85</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RAG Evaluation"
   ],
   "metadata": {
    "id": "va40VxlqrooM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import ast  # Used to safely evaluate the string as a list\n",
    "\n",
    "# Load Eval Data\n",
    "df_test = pd.read_csv(\"/content/test.csv\", index_col=0)\n",
    "\n",
    "df_test[\"questions\"] = df_test[\"questions\"].apply(ast.literal_eval)\n",
    "df_test[\"answers\"] = df_test[\"answers\"].apply(ast.literal_eval)\n",
    "print(f\"Number of papers in the test sample:- {len(df_test)}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:10:00.594116Z",
     "iopub.execute_input": "2023-10-02T10:10:00.594778Z",
     "iopub.status.idle": "2023-10-02T10:10:01.108041Z",
     "shell.execute_reply.started": "2023-10-02T10:10:00.594740Z",
     "shell.execute_reply": "2023-10-02T10:10:01.106873Z"
    },
    "trusted": true,
    "id": "OshmffdlrooM",
    "outputId": "5749b228-1403-496a-e28d-70f563838257"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Number of papers in the test sample:- 80\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Look at one sample of eval data which has a research paper questions on it and the respective reference answers\n",
    "df_test.head(1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:10:01.111436Z",
     "iopub.execute_input": "2023-10-02T10:10:01.111844Z",
     "iopub.status.idle": "2023-10-02T10:10:01.130104Z",
     "shell.execute_reply.started": "2023-10-02T10:10:01.111814Z",
     "shell.execute_reply": "2023-10-02T10:10:01.128853Z"
    },
    "trusted": true,
    "id": "Gh2O7O6brooM",
    "outputId": "ab463f33-b60c-49de-f8d4-0ac9f48e4b65"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                               paper  \\\n0  Identifying Condition-Action Statements in Med...   \n\n                                           questions  \\\n0  [What supervised machine learning models do th...   \n\n                                             answers  \n0  [Unacceptable, Unacceptable, 1470 sentences, U...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paper</th>\n      <th>questions</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Identifying Condition-Action Statements in Med...</td>\n      <td>[What supervised machine learning models do th...</td>\n      <td>[Unacceptable, Unacceptable, 1470 sentences, U...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the accuracy without using cross-encoder/ms-marco-MiniLM-L-12-v2 as reranker on the QASPER Eval Data using PairwiseComparisonEvaluator"
   ],
   "metadata": {
    "id": "FHyHcOuZrooN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Eval Method:-\n",
    "1. Iterate over each row of the test dataset:-\n",
    "    1. For the current row being iterated, create a vector index using the paper document provided in the paper column of the dataset\n",
    "    2. Query the vector index with a top_k value of top 3 nodes without any reranker\n",
    "    3. Compare the generated answers with the reference answers of the respective sample using Pairwise Comparison Evaluator and add the scores to a list\n",
    "5. Repeat 1 untill all the rows have been iterated\n",
    "6. Calculate avg scores over all samples/ rows\n"
   ],
   "metadata": {
    "id": "OSubH2herooN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    Response,\n",
    ")\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import Document\n",
    "from llama_index.evaluation import PairwiseComparisonEvaluator\n",
    "from llama_index.evaluation.eval_utils import get_responses, get_results_df\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
    "\n",
    "evaluator_gpt4_pairwise = PairwiseComparisonEvaluator(\n",
    "    service_context=service_context_gpt4\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T05:13:45.419850Z",
     "iopub.execute_input": "2023-10-02T05:13:45.420298Z",
     "iopub.status.idle": "2023-10-02T05:13:45.429761Z",
     "shell.execute_reply.started": "2023-10-02T05:13:45.420263Z",
     "shell.execute_reply": "2023-10-02T05:13:45.428206Z"
    },
    "trusted": true,
    "id": "ZAhE4cWTrooN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pairwise_scores_list = []\n",
    "\n",
    "no_reranker_dict_list = []\n",
    "\n",
    "\n",
    "# Iterate over the rows of the dataset\n",
    "for index, row in df_test.iterrows():\n",
    "    documents = [Document(text=row[\"paper\"])]\n",
    "    query_list = row[\"questions\"]\n",
    "    reference_answers_list = row[\"answers\"]\n",
    "    number_of_accepted_queries = 0\n",
    "    # Create vector index for the current row being iterated\n",
    "    vector_index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    # Query the vector index with a top_k value of top 3 documents without any reranker\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "    assert len(query_list) == len(reference_answers_list)\n",
    "    pairwise_local_score = 0\n",
    "\n",
    "    for index in range(0, len(query_list)):\n",
    "        query = query_list[index]\n",
    "        reference = reference_answers_list[index]\n",
    "\n",
    "        if reference != \"Unacceptable\":\n",
    "            number_of_accepted_queries += 1\n",
    "\n",
    "            response = str(query_engine.query(query))\n",
    "\n",
    "            no_reranker_dict = {\n",
    "                \"query\": query,\n",
    "                \"response\": response,\n",
    "                \"reference\": reference,\n",
    "            }\n",
    "            no_reranker_dict_list.append(no_reranker_dict)\n",
    "\n",
    "            # Compare the generated answers with the reference answers of the respective sample using\n",
    "            # Pairwise Comparison Evaluator and add the scores to a list\n",
    "\n",
    "            pairwise_eval_result = await evaluator_gpt4_pairwise.aevaluate(\n",
    "                query, response=response, reference=reference\n",
    "            )\n",
    "\n",
    "            pairwise_score = pairwise_eval_result.score\n",
    "\n",
    "            pairwise_local_score += pairwise_score\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if number_of_accepted_queries > 0:\n",
    "        avg_pairwise_local_score = pairwise_local_score / number_of_accepted_queries\n",
    "        pairwise_scores_list.append(avg_pairwise_local_score)\n",
    "\n",
    "\n",
    "overal_pairwise_average_score = sum(pairwise_scores_list) / len(pairwise_scores_list)\n",
    "\n",
    "df_responses = pd.DataFrame(no_reranker_dict_list)\n",
    "df_responses.to_csv(\"No_Reranker_Responses.csv\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T05:13:45.807784Z",
     "iopub.execute_input": "2023-10-02T05:13:45.808179Z",
     "iopub.status.idle": "2023-10-02T05:26:40.142718Z",
     "shell.execute_reply.started": "2023-10-02T05:13:45.808151Z",
     "shell.execute_reply": "2023-10-02T05:26:40.141807Z"
    },
    "trusted": true,
    "id": "YjicEpHProoN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results_dict = {\n",
    "    \"name\": [\"Without Reranker\"],\n",
    "    \"pairwise score\": [overal_pairwise_average_score],\n",
    "}\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "display(results_df)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T05:26:40.144280Z",
     "iopub.execute_input": "2023-10-02T05:26:40.144602Z",
     "iopub.status.idle": "2023-10-02T05:26:40.155613Z",
     "shell.execute_reply.started": "2023-10-02T05:26:40.144565Z",
     "shell.execute_reply": "2023-10-02T05:26:40.154516Z"
    },
    "trusted": true,
    "id": "4927eKjMrooN",
    "outputId": "c6a70f1d-24ee-498d-cfb4-112df4de9398"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "               name  pairwise score\n0  Without Reranker        0.553788",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>pairwise score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Without Reranker</td>\n      <td>0.553788</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the accuracy using cross-encoder/ms-marco-MiniLM-L-12-v2 as reranker on the QASPER Eval Data using PairwiseComparisonEvaluator"
   ],
   "metadata": {
    "id": "ezBJ6IVBrooN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Eval Method:-\n",
    "1. Iterate over each row of the test dataset:-\n",
    "    1. For the current row being iterated, create a vector index using the paper document provided in the paper column of the dataset\n",
    "    2. Query the vector index with a top_k value of top 5 nodes.\n",
    "    3. Use cross-encoder/ms-marco-MiniLM-L-12-v2 as a reranker as a NodePostprocessor to get top_k value of top 3 nodes out of the 8 nodes\n",
    "    3. Compare the generated answers with the reference answers of the respective sample using Pairwise Comparison Evaluator and add the scores to a list\n",
    "5. Repeat 1 untill all the rows have been iterated\n",
    "6. Calculate avg scores over all samples/ rows\n"
   ],
   "metadata": {
    "id": "fz2aU_UgrooN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    Response,\n",
    ")\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import Document\n",
    "from llama_index.evaluation import PairwiseComparisonEvaluator\n",
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\", top_n=3\n",
    ")\n",
    "\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
    "\n",
    "evaluator_gpt4_pairwise = PairwiseComparisonEvaluator(\n",
    "    service_context=service_context_gpt4\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:10:01.131899Z",
     "iopub.execute_input": "2023-10-02T10:10:01.132272Z",
     "iopub.status.idle": "2023-10-02T10:10:24.740748Z",
     "shell.execute_reply.started": "2023-10-02T10:10:01.132242Z",
     "shell.execute_reply": "2023-10-02T10:10:24.739588Z"
    },
    "trusted": true,
    "colab": {
     "referenced_widgets": [
      "4cd72d8ca9ab45548335b59e673c1ab6",
      "43fcf99b246e45dcab91746fdad3eb43",
      "01f043960f8248b48f7db3dfe765bf7b",
      "1c27a842b2964548898ca3f1152756b4",
      "8ae5a97aea424b4a93d70b7a1e75c7f3"
     ]
    },
    "id": "EEabbhm-rooN",
    "outputId": "1f77a958-4a24-47a5-ccae-b5e84ce7c411"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/791 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cd72d8ca9ab45548335b59e673c1ab6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43fcf99b246e45dcab91746fdad3eb43"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01f043960f8248b48f7db3dfe765bf7b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c27a842b2964548898ca3f1152756b4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ae5a97aea424b4a93d70b7a1e75c7f3"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pairwise_scores_list = []\n",
    "\n",
    "base_reranker_dict_list = []\n",
    "\n",
    "\n",
    "# Iterate over the rows of the dataset\n",
    "for index, row in df_test.iterrows():\n",
    "    documents = [Document(text=row[\"paper\"])]\n",
    "    query_list = row[\"questions\"]\n",
    "    reference_answers_list = row[\"answers\"]\n",
    "\n",
    "    number_of_accepted_queries = 0\n",
    "    # Create vector index for the current row being iterated\n",
    "    vector_index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    # Query the vector index with a top_k value of top 8 nodes with reranker\n",
    "    # as cross-encoder/ms-marco-MiniLM-L-12-v2\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=8, node_postprocessors=[rerank]\n",
    "    )\n",
    "\n",
    "    assert len(query_list) == len(reference_answers_list)\n",
    "    pairwise_local_score = 0\n",
    "\n",
    "    for index in range(0, len(query_list)):\n",
    "        query = query_list[index]\n",
    "        reference = reference_answers_list[index]\n",
    "\n",
    "        if reference != \"Unacceptable\":\n",
    "            number_of_accepted_queries += 1\n",
    "\n",
    "            response = str(query_engine.query(query))\n",
    "\n",
    "            base_reranker_dict = {\n",
    "                \"query\": query,\n",
    "                \"response\": response,\n",
    "                \"reference\": reference,\n",
    "            }\n",
    "            base_reranker_dict_list.append(base_reranker_dict)\n",
    "\n",
    "            # Compare the generated answers with the reference answers of the respective sample using\n",
    "            # Pairwise Comparison Evaluator and add the scores to a list\n",
    "\n",
    "            pairwise_eval_result = await evaluator_gpt4_pairwise.aevaluate(\n",
    "                query=query, response=response, reference=reference\n",
    "            )\n",
    "\n",
    "            pairwise_score = pairwise_eval_result.score\n",
    "\n",
    "            pairwise_local_score += pairwise_score\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if number_of_accepted_queries > 0:\n",
    "        avg_pairwise_local_score = pairwise_local_score / number_of_accepted_queries\n",
    "        pairwise_scores_list.append(avg_pairwise_local_score)\n",
    "\n",
    "overal_pairwise_average_score = sum(pairwise_scores_list) / len(pairwise_scores_list)\n",
    "\n",
    "df_responses = pd.DataFrame(base_reranker_dict_list)\n",
    "df_responses.to_csv(\"Base_Reranker_Responses.csv\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:10:24.742287Z",
     "iopub.execute_input": "2023-10-02T10:10:24.742666Z",
     "iopub.status.idle": "2023-10-02T10:37:59.749042Z",
     "shell.execute_reply.started": "2023-10-02T10:10:24.742637Z",
     "shell.execute_reply": "2023-10-02T10:37:59.747998Z"
    },
    "trusted": true,
    "id": "9yDGglPJrooO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results_dict = {\n",
    "    \"name\": [\"With base cross-encoder/ms-marco-MiniLM-L-12-v2 as Reranker\"],\n",
    "    \"pairwise score\": [overal_pairwise_average_score],\n",
    "}\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "display(results_df)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T10:45:30.141287Z",
     "iopub.execute_input": "2023-10-02T10:45:30.141689Z",
     "iopub.status.idle": "2023-10-02T10:45:30.152872Z",
     "shell.execute_reply.started": "2023-10-02T10:45:30.141664Z",
     "shell.execute_reply": "2023-10-02T10:45:30.151753Z"
    },
    "trusted": true,
    "id": "M_OygMaLrooO",
    "outputId": "909be9ff-acfa-4bc5-dab4-b2b139338557"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                                name  pairwise score\n0  With base cross-encoder/ms-marco-MiniLM-L-12-v...        0.556818",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>pairwise score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>With base cross-encoder/ms-marco-MiniLM-L-12-v...</td>\n      <td>0.556818</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the accuracy using our own finetuned version of cross-encoder/ms-marco-MiniLM-L-12-v2 saved as bpHigh/Cross-Encoder-LLamaIndex-Demo-v2 as reranker on the QASPER Eval Data using PairwiseComparisonEvaluator"
   ],
   "metadata": {
    "id": "pM9s122QrooO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Eval Method:-\n",
    "1. Iterate over each row of the test dataset:-\n",
    "    1. For the current row being iterated, create a vector index using the paper document provided in the paper column of the dataset\n",
    "    2. Query the vector index with a top_k value of top 5 nodes.\n",
    "    3. Use finetuned version of cross-encoder/ms-marco-MiniLM-L-12-v2 saved as bpHigh/Cross-Encoder-LLamaIndex-Demo as a reranker as a NodePostprocessor to get top_k value of top 3 nodes out of the 8 nodes\n",
    "    3. Compare the generated answers with the reference answers of the respective sample using Pairwise Comparison Evaluator and add the scores to a list\n",
    "5. Repeat 1 untill all the rows have been iterated\n",
    "6. Calculate avg scores over all samples/ rows"
   ],
   "metadata": {
    "id": "7eEKMvu8rooO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    Response,\n",
    ")\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import Document\n",
    "from llama_index.evaluation import PairwiseComparisonEvaluator\n",
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"bpHigh/Cross-Encoder-LLamaIndex-Demo-v2\", top_n=3\n",
    ")\n",
    "\n",
    "\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
    "\n",
    "evaluator_gpt4_pairwise = PairwiseComparisonEvaluator(\n",
    "    service_context=service_context_gpt4\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T06:39:27.741609Z",
     "iopub.execute_input": "2023-10-02T06:39:27.741956Z",
     "iopub.status.idle": "2023-10-02T06:39:28.319218Z",
     "shell.execute_reply.started": "2023-10-02T06:39:27.741930Z",
     "shell.execute_reply": "2023-10-02T06:39:28.318389Z"
    },
    "trusted": true,
    "id": "TTlktgterooO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pairwise_scores_list = []\n",
    "\n",
    "\n",
    "finetuned_reranker_dict_list = []\n",
    "\n",
    "# Iterate over the rows of the dataset\n",
    "for index, row in df_test.iterrows():\n",
    "    documents = [Document(text=row[\"paper\"])]\n",
    "    query_list = row[\"questions\"]\n",
    "    reference_answers_list = row[\"answers\"]\n",
    "\n",
    "    number_of_accepted_queries = 0\n",
    "    # Create vector index for the current row being iterated\n",
    "    vector_index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    # Query the vector index with a top_k value of top 8 nodes with reranker\n",
    "    # as cross-encoder/ms-marco-MiniLM-L-12-v2\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=8, node_postprocessors=[rerank]\n",
    "    )\n",
    "\n",
    "    assert len(query_list) == len(reference_answers_list)\n",
    "    pairwise_local_score = 0\n",
    "\n",
    "    for index in range(0, len(query_list)):\n",
    "        query = query_list[index]\n",
    "        reference = reference_answers_list[index]\n",
    "\n",
    "        if reference != \"Unacceptable\":\n",
    "            number_of_accepted_queries += 1\n",
    "\n",
    "            response = str(query_engine.query(query))\n",
    "\n",
    "            finetuned_reranker_dict = {\n",
    "                \"query\": query,\n",
    "                \"response\": response,\n",
    "                \"reference\": reference,\n",
    "            }\n",
    "            finetuned_reranker_dict_list.append(finetuned_reranker_dict)\n",
    "\n",
    "            # Compare the generated answers with the reference answers of the respective sample using\n",
    "            # Pairwise Comparison Evaluator and add the scores to a list\n",
    "\n",
    "            pairwise_eval_result = await evaluator_gpt4_pairwise.aevaluate(\n",
    "                query, response=response, reference=reference\n",
    "            )\n",
    "\n",
    "            pairwise_score = pairwise_eval_result.score\n",
    "\n",
    "            pairwise_local_score += pairwise_score\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if number_of_accepted_queries > 0:\n",
    "        avg_pairwise_local_score = pairwise_local_score / number_of_accepted_queries\n",
    "        pairwise_scores_list.append(avg_pairwise_local_score)\n",
    "\n",
    "overal_pairwise_average_score = sum(pairwise_scores_list) / len(pairwise_scores_list)\n",
    "df_responses = pd.DataFrame(finetuned_reranker_dict_list)\n",
    "df_responses.to_csv(\"Finetuned_Reranker_Responses.csv\")"
   ],
   "metadata": {
    "trusted": true,
    "id": "gBl-zCWkrooO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results_dict = {\n",
    "    \"name\": [\"With fine-tuned cross-encoder/ms-marco-MiniLM-L-12-v2\"],\n",
    "    \"pairwise score\": [overal_pairwise_average_score],\n",
    "}\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "display(results_df)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-02T06:52:52.437902Z",
     "iopub.execute_input": "2023-10-02T06:52:52.438588Z",
     "iopub.status.idle": "2023-10-02T06:52:52.449218Z",
     "shell.execute_reply.started": "2023-10-02T06:52:52.438552Z",
     "shell.execute_reply": "2023-10-02T06:52:52.448168Z"
    },
    "trusted": true,
    "id": "vvqu2GozrooP",
    "outputId": "06f3171e-4dad-4545-ffd9-d1170f561c5c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                                name  pairwise score\n0  With fine-tuned cross-encoder/ms-marco-MiniLM-...             0.6",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>pairwise score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>With fine-tuned cross-encoder/ms-marco-MiniLM-...</td>\n      <td>0.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results of RAG Evaluation\n",
    "\n",
    "As we can see we get the highest pairwise score with finetuned cross-encoder.\n",
    "\n",
    "Although I would like to point that the reranking eval based on hits is a more robust metric compared to pairwise comparision evaluator as I have seen inconsistencies with the scores and there are also many inherent biases present when evaluating using GPT-4"
   ],
   "metadata": {
    "id": "5YwY8RVBgnDi"
   }
  }
 ]
}
