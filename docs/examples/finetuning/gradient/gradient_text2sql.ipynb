{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning for Text-to-SQL With Gradient and LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index gradientai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms import GradientBaseModelLLM\n",
    "from llama_index.finetuning.gradient.base import GradientFinetuneEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GRADIENT_ACCESS_TOKEN\"] = os.getenv(\"GRADIENT_API_KEY\")\n",
    "os.environ[\n",
    "    \"GRADIENT_WORKSPACE_ID\"\n",
    "] = \"3c1cc9e5-1ea3-4807-aee8-c416b59a1250_workspace\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect = \"sqlite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "def load_jsonl(data_dir):\n",
    "    data_path = Path(data_dir).as_posix()\n",
    "    data = load_dataset(\"json\", data_files=data_path)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_jsonl(data_dicts, out_path):\n",
    "    with open(out_path, \"w\") as fp:\n",
    "        for data_dict in data_dicts:\n",
    "            fp.write(json.dumps(data_dict) + \"\\n\")\n",
    "\n",
    "\n",
    "def load_data_sql(data_dir: str = \"data_sql\"):\n",
    "    dataset = load_dataset(\"b-mc2/sql-create-context\")\n",
    "\n",
    "    dataset_splits = {\"train\": dataset[\"train\"]}\n",
    "    out_path = Path(data_dir)\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for key, ds in dataset_splits.items():\n",
    "        with open(out_path, \"w\") as f:\n",
    "            for item in ds:\n",
    "                newitem = {\n",
    "                    \"input\": item[\"question\"],\n",
    "                    \"context\": item[\"context\"],\n",
    "                    \"output\": item[\"answer\"],\n",
    "                }\n",
    "                f.write(json.dumps(newitem) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_sql(data_dir=\"data_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "def get_train_val_splits(\n",
    "    data_dir: str = \"data_sql\",\n",
    "    val_ratio: float = 0.1,\n",
    "    seed: int = 42,\n",
    "    shuffle: bool = True,\n",
    "):\n",
    "    data = load_jsonl(data_dir)\n",
    "    # data_path = Path(data_dir).as_posix()\n",
    "    # data = load_dataset(\"json\", data_files=data_path)\n",
    "    num_samples = len(data[\"train\"])\n",
    "    val_set_size = ceil(val_ratio * num_samples)\n",
    "\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=val_set_size, shuffle=shuffle, seed=seed\n",
    "    )\n",
    "    return train_val[\"train\"].shuffle(), train_val[\"test\"].shuffle()\n",
    "    # train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    # val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009280920028686523,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a85cf2bce948d398504e461d1c41d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0035130977630615234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "Extracting data files",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fcfb98295247f1a7e4e8a9e58f68d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0032150745391845703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e61a839e424a1a8392ab7042661786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_train_data, raw_val_data = get_train_val_splits(data_dir=\"data_sql\")\n",
    "save_jsonl(raw_train_data, \"train_data_raw.jsonl\")\n",
    "save_jsonl(raw_val_data, \"val_data_raw.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'When was the ship launched when the commissioned or completed(*) is 6 june 1864?',\n",
       " 'context': 'CREATE TABLE table_12592074_1 (launched VARCHAR, commissioned_or_completed_ VARCHAR, _ VARCHAR)',\n",
       " 'output': 'SELECT launched FROM table_12592074_1 WHERE commissioned_or_completed_ * _ = \"6 June 1864\"'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to format into prompts amenable for training\n",
    "\n",
    "# text_to_sql_tmpl_str = \"\"\"\\\n",
    "# <s>### Instruction:\\n{user_message}\\n\\n### Response:\\n{response}</s>\"\"\"\n",
    "\n",
    "# text_to_sql_inference_tmpl_str = \"\"\"\\\n",
    "# <s>### Instruction:\\n{user_message}\\n\\n### Response:\\n\"\"\"\n",
    "\n",
    "text_to_sql_tmpl_str = \"\"\"\\\n",
    "<s>### Instruction:\\n{system_message}{user_message}\\n\\n### Response:\\n{response}</s>\"\"\"\n",
    "\n",
    "text_to_sql_inference_tmpl_str = \"\"\"\\\n",
    "<s>### Instruction:\\n{system_message}{user_message}\\n\\n### Response:\\n\"\"\"\n",
    "\n",
    "# text_to_sql_tmpl_str = \"\"\"\\\n",
    "# <s>[INST] SYS\\n{system_message}\\n<</SYS>>\\n\\n{user_message} [/INST] {response} </s>\"\"\"\n",
    "\n",
    "# text_to_sql_inference_tmpl_str = \"\"\"\\\n",
    "# <s>[INST] SYS\\n{system_message}\\n<</SYS>>\\n\\n{user_message} [/INST] \"\"\"\n",
    "\n",
    "\n",
    "def _generate_prompt_sql(input, context, dialect=\"sqlite\", output=\"\"):\n",
    "    system_message = f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "    \n",
    "    \"\"\"\n",
    "    user_message = f\"\"\"### Dialect:\n",
    "{dialect}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    if output:\n",
    "        return text_to_sql_tmpl_str.format(\n",
    "            system_message=system_message,\n",
    "            user_message=user_message,\n",
    "            response=output,\n",
    "        )\n",
    "    else:\n",
    "        return text_to_sql_inference_tmpl_str.format(\n",
    "            system_message=system_message, user_message=user_message\n",
    "        )\n",
    "\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    full_prompt = _generate_prompt_sql(\n",
    "        data_point[\"input\"],\n",
    "        data_point[\"context\"],\n",
    "        dialect=\"sqlite\",\n",
    "        output=data_point[\"output\"],\n",
    "    )\n",
    "    return {\"inputs\": full_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0017120838165283203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 70719,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d3e7112b5a404699fb07f90c746f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0018231868743896484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 7858,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eeca14a1e8248dcb300755ae4ba7f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7858 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = raw_train_data.map(generate_prompt)\n",
    "save_jsonl(train_data, \"train_data.jsonl\")\n",
    "val_data = raw_val_data.map(generate_prompt)\n",
    "save_jsonl(val_data, \"val_data.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Fine-tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model_slug = \"nous-hermes2\"\n",
    "base_model_slug = \"llama2-7b-chat\"\n",
    "base_llm = GradientBaseModelLLM(\n",
    "    base_model_slug=base_model_slug, max_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step max steps to 20 just for testing purposes\n",
    "finetune_engine = GradientFinetuneEngine(\n",
    "    base_model_slug=base_model_slug,\n",
    "    name=\"text_to_sql\",\n",
    "    data_path=\"train_data.jsonl\",\n",
    "    verbose=True,\n",
    "    max_steps=200,\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'805c6fd6-daa8-4fc8-a509-bebb2f2c1024_model_adapter'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_engine.model_adapter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** EPOCH 0 **\n",
      "fine-tuning step 4: loss=1980.5546, trainable tokens=631\n",
      "fine-tuning step 8: loss=1312.1577, trainable tokens=648\n",
      "fine-tuning step 12: loss=1150.765, trainable tokens=665\n",
      "fine-tuning step 16: loss=867.1619, trainable tokens=620\n",
      "fine-tuning step 20: loss=747.4055, trainable tokens=650\n",
      "fine-tuning step 24: loss=577.833, trainable tokens=640\n",
      "fine-tuning step 28: loss=375.29626, trainable tokens=583\n",
      "fine-tuning step 32: loss=506.39142, trainable tokens=604\n",
      "fine-tuning step 36: loss=489.96997, trainable tokens=653\n",
      "fine-tuning step 40: loss=380.26834, trainable tokens=613\n",
      "fine-tuning step 44: loss=432.8493, trainable tokens=585\n",
      "fine-tuning step 48: loss=597.9427, trainable tokens=638\n",
      "fine-tuning step 52: loss=546.3855, trainable tokens=636\n",
      "fine-tuning step 56: loss=466.26605, trainable tokens=614\n",
      "fine-tuning step 60: loss=317.3381, trainable tokens=627\n",
      "fine-tuning step 64: loss=447.2127, trainable tokens=655\n",
      "fine-tuning step 68: loss=323.37115, trainable tokens=585\n",
      "fine-tuning step 72: loss=524.6482, trainable tokens=689\n",
      "fine-tuning step 76: loss=303.81665, trainable tokens=627\n",
      "fine-tuning step 80: loss=260.98657, trainable tokens=621\n",
      "fine-tuning step 84: loss=365.6654, trainable tokens=668\n",
      "fine-tuning step 88: loss=344.54272, trainable tokens=639\n",
      "fine-tuning step 92: loss=327.66827, trainable tokens=656\n",
      "fine-tuning step 96: loss=289.7859, trainable tokens=619\n",
      "fine-tuning step 100: loss=292.8275, trainable tokens=588\n",
      "fine-tuning step 104: loss=382.1244, trainable tokens=672\n",
      "fine-tuning step 108: loss=389.34393, trainable tokens=670\n",
      "fine-tuning step 112: loss=353.45465, trainable tokens=701\n",
      "fine-tuning step 116: loss=331.74487, trainable tokens=643\n",
      "fine-tuning step 120: loss=253.0273, trainable tokens=580\n",
      "fine-tuning step 124: loss=236.73608, trainable tokens=597\n",
      "fine-tuning step 128: loss=317.4, trainable tokens=670\n",
      "fine-tuning step 132: loss=274.91995, trainable tokens=600\n",
      "fine-tuning step 136: loss=257.7248, trainable tokens=623\n",
      "fine-tuning step 140: loss=434.95764, trainable tokens=666\n",
      "fine-tuning step 144: loss=300.26312, trainable tokens=572\n",
      "fine-tuning step 148: loss=383.97775, trainable tokens=673\n",
      "fine-tuning step 152: loss=295.93292, trainable tokens=621\n",
      "fine-tuning step 156: loss=326.1684, trainable tokens=638\n",
      "fine-tuning step 160: loss=220.7796, trainable tokens=587\n",
      "fine-tuning step 164: loss=434.9798, trainable tokens=661\n",
      "fine-tuning step 168: loss=339.8578, trainable tokens=674\n",
      "fine-tuning step 172: loss=333.2976, trainable tokens=664\n",
      "fine-tuning step 176: loss=269.01715, trainable tokens=606\n",
      "fine-tuning step 180: loss=351.7721, trainable tokens=658\n",
      "fine-tuning step 184: loss=322.53317, trainable tokens=625\n",
      "fine-tuning step 188: loss=303.31616, trainable tokens=677\n",
      "fine-tuning step 192: loss=264.65088, trainable tokens=609\n",
      "fine-tuning step 196: loss=309.31155, trainable tokens=612\n",
      "fine-tuning step 200: loss=207.39243, trainable tokens=579\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    print(f\"** EPOCH {i} **\")\n",
    "    finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_llm = finetune_engine.get_finetuned_model(max_tokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out + Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    select,\n",
    "    column,\n",
    ")\n",
    "from llama_index import SQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create city SQL table\n",
    "table_name = \"city_stats\"\n",
    "city_stats_table = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"city_name\", String(16), primary_key=True),\n",
    "    Column(\"population\", Integer),\n",
    "    Column(\"country\", String(16), nullable=False),\n",
    ")\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert sample rows\n",
    "from sqlalchemy import insert\n",
    "\n",
    "rows = [\n",
    "    {\"city_name\": \"Toronto\", \"population\": 2930000, \"country\": \"Canada\"},\n",
    "    {\"city_name\": \"Tokyo\", \"population\": 13960000, \"country\": \"Japan\"},\n",
    "    {\n",
    "        \"city_name\": \"Chicago\",\n",
    "        \"population\": 2679000,\n",
    "        \"country\": \"United States\",\n",
    "    },\n",
    "    {\"city_name\": \"Seoul\", \"population\": 9776000, \"country\": \"South Korea\"},\n",
    "]\n",
    "for row in rows:\n",
    "    stmt = insert(city_stats_table).values(**row)\n",
    "    with engine.connect() as connection:\n",
    "        cursor = connection.execute(stmt)\n",
    "        connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "test_datapoint = raw_val_data[3]\n",
    "\n",
    "\n",
    "def get_text2sql_completion(llm, sql_database, raw_datapoint):\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    text2sql_tmpl_str = _generate_prompt_sql(\n",
    "        raw_datapoint[\"input\"],\n",
    "        raw_datapoint[\"context\"],\n",
    "        dialect=\"sqlite\",\n",
    "        output=None,\n",
    "    )\n",
    "    # text2sql_prompt = PromptTemplate(text2sql_tmpl_str)\n",
    "\n",
    "    response = llm.complete(text2sql_tmpl_str)\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is the lowest number conceded for the team that had less than 8 wins, scored 21, and had less than 23 points?', 'context': 'CREATE TABLE table_name_77 (conceded INTEGER, points VARCHAR, wins VARCHAR, scored VARCHAR)', 'output': 'SELECT MIN(conceded) FROM table_name_77 WHERE wins < 8 AND scored = 21 AND points < 23'}\n"
     ]
    }
   ],
   "source": [
    "print(test_datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT * FROM table_name_77 WHERE conceded = (SELECT MIN(conceded) FROM table_name_77 WHERE points < 23 AND wins < 8);\\n\\nThis SQL query will return all the rows from the `table_name_77` table where the `conceded` column is equal to the minimum value of `conceded` found in rows where `points` is less than 23 and `wins`'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tmp_model_slug = \"nous-hermes2\"\n",
    "tmp_model_slug = \"llama2-7b-chat\"\n",
    "tmp_llm = GradientBaseModelLLM(base_model_slug=tmp_model_slug, max_tokens=100)\n",
    "# tmp_llm = OpenAI(model=\"gpt-4\")\n",
    "get_text2sql_completion(tmp_llm, sql_database, test_datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT MIN(conceded) FROM table_name_77 WHERE wins < 8 AND scored = 21 AND points < 23'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text2sql_completion(ft_llm, sql_database, test_datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import NLSQLTableQueryEngine\n",
    "from llama_index import ServiceContext, PromptTemplate\n",
    "\n",
    "\n",
    "def get_text2sql_query_engine(\n",
    "    llm,\n",
    "    sql_database,\n",
    "):\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    text2sql_tmpl_str = _generate_prompt_sql(\n",
    "        \"{query_str}\", \"{schema}\", dialect=\"{dialect}\", output=\"\"\n",
    "    )\n",
    "    sql_prompt = PromptTemplate(text2sql_tmpl_str)\n",
    "    # print(sql_prompt.template)\n",
    "    # raise Exception\n",
    "    query_engine = NLSQLTableQueryEngine(\n",
    "        sql_database,\n",
    "        text_to_sql_prompt=sql_prompt,\n",
    "        service_context=service_context,\n",
    "        synthesize_response=False,\n",
    "    )\n",
    "    return query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Which city has the highest population?\"\n",
    "query = \"What is the average population and total population of the cities?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7336250.0, 29345000)]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "tmp_llm = OpenAI(model=\"gpt-4\")\n",
    "tmp_query_engine = get_text2sql_query_engine(tmp_llm, sql_database)\n",
    "\n",
    "tmp_response = tmp_query_engine.query(query)\n",
    "print(str(tmp_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_query_engine = get_text2sql_query_engine(base_llm, sql_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Warning",
     "evalue": "You can only execute one statement at a time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWarning\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_response \u001b[38;5;241m=\u001b[39m \u001b[43mbase_query_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/indices/query/base.py:23\u001b[0m, in \u001b[0;36mquery\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, str_or_query_bundle: QueryType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     24\u001b[0m             str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query(str_or_query_bundle)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/indices/struct_store/sql_query.py:275\u001b[0m, in \u001b[0;36mBaseSQLTableQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m     retrieved_nodes, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_with_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_bundle\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     sql_query_str \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_query\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_synthesize_response:\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/indices/struct_store/sql_retriever.py:254\u001b[0m, in \u001b[0;36mNLSQLRetriever.retrieve_with_metadata\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# assume that it's a valid SQL query\u001b[39;00m\n\u001b[1;32m    253\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> Predicted SQL query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql_query_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 254\u001b[0m retrieved_nodes, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sql_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_with_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_query_str\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_nodes, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_query\u001b[39m\u001b[38;5;124m\"\u001b[39m: sql_query_str, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata}\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/indices/struct_store/sql_retriever.py:72\u001b[0m, in \u001b[0;36mSQLRetriever.retrieve_with_metadata\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     query_bundle \u001b[38;5;241m=\u001b[39m str_or_query_bundle\n\u001b[0;32m---> 72\u001b[0m raw_response_str, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sql_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_raw:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [NodeWithScore(node\u001b[38;5;241m=\u001b[39mTextNode(text\u001b[38;5;241m=\u001b[39mraw_response_str))], metadata\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/utilities/sql_wrapper.py:194\u001b[0m, in \u001b[0;36mSQLDatabase.run_sql\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mbegin() \u001b[38;5;28;01mas\u001b[39;00m connection:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 194\u001b[0m         cursor \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (ProgrammingError, OperationalError) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatement \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommand\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is invalid SQL.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1412\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py:483\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[0;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[0;32m--> 483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1635\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[0;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[1;32m   1623\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1624\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[1;32m   1625\u001b[0m )\n\u001b[1;32m   1627\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[1;32m   1628\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[1;32m   1629\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1633\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[1;32m   1634\u001b[0m )\n\u001b[0;32m-> 1635\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[1;32m   1648\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[1;32m   1649\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1650\u001b[0m         elem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         ret,\n\u001b[1;32m   1655\u001b[0m     )\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1844\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(\n\u001b[1;32m   1840\u001b[0m         dialect,\n\u001b[1;32m   1841\u001b[0m         context,\n\u001b[1;32m   1842\u001b[0m     )\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1984\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1981\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[1;32m   1983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1984\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1985\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2342\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2341\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2342\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reentrant_error\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1965\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1965\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1971\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1972\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1976\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[1;32m   1977\u001b[0m     )\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py:921\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 921\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mWarning\u001b[0m: You can only execute one statement at a time."
     ]
    }
   ],
   "source": [
    "base_response = base_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(\u001b[43mbase_response\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_response' is not defined"
     ]
    }
   ],
   "source": [
    "print(str(base_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbase_response\u001b[49m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_query\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_response' is not defined"
     ]
    }
   ],
   "source": [
    "base_response.metadata[\"sql_query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_query_engine = get_text2sql_query_engine(ft_llm, sql_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_response = ft_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2930000.0, 1), (13960000.0, 1), (9776000.0, 1), (2679000.0, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(str(ft_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT AVG(population), COUNT(*) FROM city_stats GROUP BY country'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_response.metadata[\"sql_query\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
