{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/data_connectors/PathwayReaderDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pathway Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [Pathway](https://pathway.com/) is an open data processing framework. It allows you to easily develop data transformation pipelines and Machine Learning applications that work with live data sources and changing data.\n",
    "\n",
    "This notebook shows how to use the Pathway to deploy a live data indexing pipeline which can be queried from reader. You can add documents to Pathway from existing connectors or create your own connector with Python ensuring your LLM stays up to date with latest information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pathway\n",
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no Pathway instance running, we need to start one.\n",
    "For the demo, lets create an instance that listens local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import PathwayVectorServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import pathway as pw\n",
    "\n",
    "# omit if embedder of choice is not OpenAI\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define inputs Pathway will listen to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = []\n",
    "data_sources.append(\n",
    "    pw.io.fs.read(\n",
    "        \"../data/paul_graham\",\n",
    "        format=\"binary\",\n",
    "        mode=\"streaming\",\n",
    "        with_metadata=True,\n",
    "    )  # This creates a `pathway` connector that tracks\n",
    "    # all the files in the `data/paul_graham` directory.\n",
    ")\n",
    "\n",
    "# We can add more connectors from various sources/formats with pw.io.\n",
    "# This creates a connector that tracks files in Google drive.\n",
    "# please follow the instructions at https://pathway.com/developers/tutorials/connectors/gdrive-connector/ to get credentials\n",
    "\n",
    "# data_sources.append(\n",
    "#     pw.io.gdrive.read(object_id=\"17H4YpBOAKQzEJ93xmC2z170l0bP2npMy\", service_user_credentials_file=\"credentials.json\", with_metadata=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create document transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.node_parser import TokenTextSplitter\n",
    "\n",
    "embed_model = OpenAIEmbedding(embed_batch_size=10)\n",
    "\n",
    "transformations_example = [\n",
    "    TokenTextSplitter(\n",
    "        chunk_size=80,\n",
    "        chunk_overlap=40,\n",
    "        separator=\" \",\n",
    "    ),\n",
    "    embed_model,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Thread(Thread-5 (run), started 140155873506880)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-21T17:37:11]:INFO:Preparing Pathway computation\n",
      "[2023-12-21T17:37:11]:ERROR:unhandled exception during asyncio.run() shutdown\n",
      "task: <Task finished name='Task-5' coro=<_run_app() done, defined at /home/berke/miniconda3/envs/langchain310/lib/python3.10/site-packages/aiohttp/web.py:303> exception=OSError(98, \"error while attempting to bind on address ('127.0.0.1', 8754): address already in use\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/berke/miniconda3/envs/langchain310/lib/python3.10/site-packages/aiohttp/web.py\", line 544, in run_app\n",
      "    loop.run_until_complete(main_task)\n",
      "  File \"/home/berke/miniconda3/envs/langchain310/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/home/berke/miniconda3/envs/langchain310/lib/python3.10/site-packages/aiohttp/web.py\", line 448, in _run_app\n",
      "    await site.start()\n",
      "  File \"/home/berke/miniconda3/envs/langchain310/lib/python3.10/site-packages/aiohttp/web_runner.py\", line 119, in start\n",
      "    self._server = await loop.create_server(\n",
      "  File \"/home/berke/miniconda3/envs/langchain310/lib/python3.10/asyncio/base_events.py\", line 1519, in create_server\n",
      "    raise OSError(err.errno, 'error while attempting '\n",
      "OSError: [Errno 98] error while attempting to bind on address ('127.0.0.1', 8754): address already in use\n",
      "[2023-12-21T17:37:11]:ERROR:There had been an error processing the row read result OSError: [Errno 98] error while attempting to bind on address ('127.0.0.1', 8754): address already in use\n",
      "[2023-12-21T17:37:11]:INFO:FilesystemReader-0: 0 entries (1 minibatch(es)) have been sent to the engine\n",
      "[2023-12-21T17:37:11]:INFO:PythonReader-1: 0 entries (1 minibatch(es)) have been sent to the engine\n",
      "[2023-12-21T17:37:11]:INFO:PythonReader-1: 0 entries (1 minibatch(es)) have been sent to the engine\n",
      "[2023-12-21T17:37:11]:WARNING:PythonReader-1: Closing the data source\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-21T17:37:13]:INFO:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2023-12-21T17:37:13]:INFO:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2023-12-21T17:37:13]:INFO:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2023-12-21T17:37:13]:INFO:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2023-12-21T17:37:17]:INFO:FilesystemReader-0: 1 entries (5 minibatch(es)) have been sent to the engine\n",
      "[2023-12-21T17:37:23]:INFO:FilesystemReader-0: 0 entries (4 minibatch(es)) have been sent to the engine\n"
     ]
    }
   ],
   "source": [
    "pr = PathwayVectorServer(\n",
    "    *data_sources,\n",
    "    transformations=transformations_example,\n",
    ")\n",
    "\n",
    "# Define the Host and port that Pathway will be on\n",
    "PATHWAY_HOST = \"127.0.0.1\"\n",
    "PATHWAY_PORT = 8754\n",
    "\n",
    "pr.run_server(\n",
    "    host=PATHWAY_HOST, port=PATHWAY_PORT, with_cache=False, threaded=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `reader` client for Pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.pathway import PathwayReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PathwayReader(host=PATHWAY_HOST, port=PATHWAY_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-20T13:08:16]:INFO:PythonReader-1: 1 entries (1119 minibatch(es)) have been sent to the engine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-20T13:08:17]:INFO:127.0.0.1 [20/Dec/2023:13:08:16 +0100] \"POST / HTTP/1.1\" 200 5230 \"-\" \"python-requests/2.31.0\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='8182945624577964255', embedding=None, metadata={'created_at': None, 'modified_at': 1702989823, 'owner': 'berke', 'path': '/home/berke/IoT-Pathway/experimental/berke/llama_reader/sample_documents/repo_readme.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='05e6d1a99ca31c406061f2257fcdc9f45c0d5a31696fa11a3a75876e73fdb336', text='Pathway is an open framework for high-throughput and low-latency real-time data processing. It is used to create Python code which seamlessly combines batch processing, streaming, and real-time API\\'s for LLM apps. Pathway\\'s distributed runtime (ü¶Ä-üêç) provides fresh results of your data pipelines whenever new inputs and requests are received.\\n\\nIn the first place, Pathway was designed to be a life-saver (or at least a time-saver) for Python developers and ML/AI engineers faced with live data sources, where you need to react quickly to fresh data. Still, Pathway is a powerful tool that can be used for a lot of things. If you want to do streaming in Python, build an AI data pipeline, or if you are looking for your next Python data processing framework, keep reading.\\n\\nPathway provides a high-level programming interface in Python for defining data transformations, aggregations, and other operations on data streams.\\nWith Pathway, you can effortlessly design and deploy sophisticated data workflows that efficiently handle high volumes of data in real time.\\n\\nPathway is interoperable with various data sources and sinks such as Kafka, CSV files, SQL/noSQL databases, and REST API\\'s, allowing you to connect and process data from different storage systems.\\n\\nTypical use-cases of Pathway include realtime data processing, ETL (Extract, Transform, Load) pipelines, data analytics, monitoring, anomaly detection, and recommendation. Pathway can also independently provide the backbone of a light LLMops stack for real-time LLM applications.\\n\\nIn Pathway, data is represented in the form of Tables. Live data streams are also treated as Tables. The library provides a rich set of operations like filtering, joining, grouping, and windowing.\\n\\nFor any questions, you will find the community and team behind the project on Discord.\\n\\nScreencast animation of converting batch code to streaming by changing one keyword argument in the script.\\n\\n## Getting started\\n\\n\\n### Installation\\n\\nPathway requires Python 3.10 or above.\\n\\nYou can install the current release of Pathway using `pip`:\\n\\n```\\n$ pip install -U pathway\\n```\\n\\n‚ö†Ô∏è Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.\\n\\n### Running Pathway locally\\n\\nTo use Pathway, you only need to import it:\\n\\n```python\\nimport pathway as pw\\n```\\n\\nNow, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:\\n\\n```python\\npw.run()\\n```\\n\\nYou can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`. Alternatively, use the pathway\\'ish version:\\n\\n```\\n$ pathway spawn python main.py\\n```\\n\\nPathway natively supports multithreading.\\nTo launch your application with 3 threads, you can do as follows:\\n```\\n$ pathway spawn --threads 3 python main.py\\n```\\n\\nTo jumpstart a Pathway project, you can use our cookiecutter template\\n\\n\\n### Example\\n\\n```python\\nimport pathway as pw\\n\\n# Using the `demo` module to create a data stream\\ntable = pw.demo.range_stream(nb_rows=50)\\n# Storing the stream into a CSV file\\npw.io.csv.write(table, \"output_table.csv\")\\n\\n# Summing all the values in a new table\\nsum_table = table.reduce(sum=pw.reducers.sum(pw.this.value))\\n# Storing the sum (which is a stream) in another CSV file\\npw.io.csv.write(sum_table, \"sum_table.csv\")\\n\\n# Now that the pipeline is built, the computation is started\\npw.run()\\n```\\n\\nRun this example in Google Colab\\n\\n## Deployment\\n\\nDo you feel limited by a local run?\\nIf you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.\\nPathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.\\nIt scales using distributed computing on the cloud and supports Kubernetes deployment.\\n\\nYou can learn more about the features of Pathway for Enterprise on our website.\\n\\nIf you are interested, don\\'t hesitate to contact us to learn more.\\n\\n## Monitoring Pathway\\n\\nPathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. \\n\\nThis dashboard is enabled by default; you can disable it by passing `monitoring_level = pathway.MonitoringLevel.NONE` to `pathway.run()`.\\n\\nIn addition to Pathway\\'s built-in dashboard, you can use Prometheus to monitor your Pathway application.\\n\\n## Resources\\n\\nSee also: Pathway Documentation (https://pathway.com/developers/) webpage (including API Docs).\\n\\n### Videos about Pathway<a id=\"videos-about-pathway\"></a>\\n[‚ñ∂Ô∏è Building an LLM Application without a vector', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-20T13:08:21]:INFO:PythonReader-1: 1 entries (101 minibatch(es)) have been sent to the engine\n",
      "[2023-12-20T13:08:26]:INFO:PythonReader-1: 0 entries (101 minibatch(es)) have been sent to the engine\n"
     ]
    }
   ],
   "source": [
    "# let us create some dummy vector that is compatible with OpenAI embeddings\n",
    "reader.load_data(query_text=\"some search input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a summary index with llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-20T11:42:27]:INFO:127.0.0.1 [20/Dec/2023:11:42:26 +0100] \"POST / HTTP/1.1\" 200 9509 \"-\" \"python-requests/2.31.0\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-20T11:42:27]:INFO:PythonReader-1: 2 entries (101 minibatch(es)) have been sent to the engine\n",
      "[2023-12-20T11:42:32]:INFO:PythonReader-1: 0 entries (100 minibatch(es)) have been sent to the engine\n"
     ]
    }
   ],
   "source": [
    "docs = reader.load_data(query_text=\"some search input\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.list import SummaryIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = SummaryIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-20T11:43:43]:INFO:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is Pathway?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pathway is an open framework for high-throughput and low-latency real-time data processing. It is a powerful tool that provides a high-level programming interface in Python for defining data transformations, aggregations, and other operations on data streams. Pathway is interoperable with various data sources and sinks such as Kafka, CSV files, SQL/noSQL databases, and REST API's, allowing users to connect and process data from different storage systems. It is commonly used for real-time data processing, ETL pipelines, data analytics, monitoring, anomaly detection, and recommendation. Pathway represents data in the form of Tables and provides a rich set of operations like filtering, joining, grouping, and windowing. It can be easily installed using pip and supports multithreading. Pathway also comes with a monitoring dashboard and supports Prometheus for monitoring purposes.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
