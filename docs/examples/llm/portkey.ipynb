{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portkey | Building Resilient Llamaindex Apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portkey integration with Llamaindex adds the following capabilities to your apps out of the box:\n",
    "\n",
    "1. AI Gateway\n",
    "    - Automated Fallbacks & Retries\n",
    "    - Load Balancing\n",
    "    - Semantic Caching\n",
    "2. Observability\n",
    "    - Logging of all requess\n",
    "    - Requests Tracing\n",
    "    - Adding custom tags to each request\n",
    "\n",
    "It does all this with just one command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Portkey\n",
    "from llama_index.llms import ChatMessage #We'll use this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need to keep track of installing any other SDKs or importing them to your Llamaindex app.\n",
    "\n",
    "### Here's how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Get your Portkey API key by logging into [Portkey here](https://app.portkey.ai/) --> Click on the profile icon on top right and \"Copy API Key\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PORTKEY_API_KEY\"]=\"\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"]=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Add all the Portkey features you want (as illustrated above) by calling the Portkey class\n",
    "\n",
    "This is a quick guide to all Portkey features and what they expect:\n",
    "\n",
    "| Feature             | Config Key              | Value(Type)                                      | Required    |\n",
    "|---------------------|-------------------------|--------------------------------------------------|-------------|\n",
    "| API Key             | `api_key`               | `string`                                         | ✅ Required |\n",
    "| Mode                | `mode`                  | `fallback`, `loadbalance`, `single`              | ✅ Required |\n",
    "| Cache Type          | `cache_status`          | `simple`, `semantic`                             | ❔ Optional |\n",
    "| Force Cache Refresh | `cache_force_refresh`   | `True`, `False`                                  | ❔ Optional |\n",
    "| Cache Age           | `cache_age`             | `integer` (in seconds)                           | ❔ Optional |\n",
    "| Trace ID            | `trace_id`              | `string`                                         | ❔ Optional |\n",
    "| Retries         | `retry_count`           | `integer` [0,5]                                  | ❔ Optional |\n",
    "| Metadata            | `metadata`              | `json object` [More info](https://docs.portkey.ai/key-features/custom-metadata)          | ❔ Optional |\n",
    "| Base URL | `base_url` | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_llm = Portkey(mode=\"single\",cache_status=\"semantic\", cache_force_refresh=\"True\", cache_age=\"1000\", trace_id=\"portkey_llamaindex\", retry=\"5\") \n",
    "\n",
    "# Since we have defined the Portkey API Key with os.environ, we do not need to set it again here\n",
    "# Let us also add some metadata!\n",
    "\n",
    "metadata={\n",
    "    \"_environment\":\"production\",\n",
    "    \"_prompt\":\"test\",\n",
    "    \"_user\":\"user\",\n",
    "    \"_organisation\":\"acme\"\n",
    "}\n",
    "\n",
    "pk_llm.metadata=metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Now let's pick which LLMs we want. \n",
    "\n",
    "With the Portkey integration, we have simplified how you construct an LLM by using a single function for all differnt providers: PortkeyBase. It has all the exact same keys you are already using with your OpenAI or Anthropic constructors, with the only addition of one new key - `weight`. This key is used for the load balancing feature. Scroll here if you want to jump to seeing how to implement load balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm = LLMBase(provider=\"openai\", model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Now let's activate our Portkey LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_llm.add_llms(openai_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, in these 4 steps, you have infused your Llamaindex app with the most sophisticated production capabilities. Let's test our our integration now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant\"),\n",
    "    ChatMessage(role=\"user\", content=\"What can you do?\")\n",
    "]\n",
    "print(\"Testing Portkey Llamaindex integration:\")\n",
    "response = pk_llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your request and responses, along with the trace id, cache status, and all metadata are now logged to Portkey, and you can [see them here](https://app.portkey.ai/).\n",
    "\n",
    "To recap,\n",
    "\n",
    "Step 1 - Import Portkey from llama_index.llms.\n",
    "Step 2 - Grab your Portkey API Key from [here](https://app.portkey.ai/).\n",
    "Step 3 - Construct your Portkey LLM with `pk_llm=Portkey(mode=\"fallback\")` and any other Portkey features you want\n",
    "Step 4 - Construct your provider LLM with opneai_llm=PortkeyBase(provider=\"openai\",model=\"gpt-4\")\n",
    "Step 5 - Add the provider LLM to Portkey LLM with `pk_llm.add_llms(openai_llm)`\n",
    "Step 6 - Call the Portkey LLM regularly like you would any other LLM, with `pk_llm.chat(messages)`\n",
    "\n",
    "Here's the guide to all the functions and their params:\n",
    "- [Portkey LLM Constructor]()\n",
    "- [PortkeyBase Constructor]() \n",
    "- [List of Portkey + Llamaindex Features]()\n",
    "- [List of Portkey + Llamaindex limitations]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Fallbacks with Portkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_llm.mode=\"fallback\"\n",
    "\n",
    "llm1 = LLMBase(provider=\"openai\", model=\"gpt-4\")\n",
    "llm2 = LLMBase(provider=\"openai\", model=\"gpt-3.5-turbo\")\n",
    "\n",
    "pk_llm.add_llms(llm_params=[llm1,llm2])\n",
    "\n",
    "print(\"Testing Fallback functionality:\")\n",
    "response = pk_llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Load Balancing with Portkey\n",
    "\n",
    "For Load Balancing, we have to add one more param to PortkeyBase - weight.\n",
    "\n",
    "The way this works is, for each new request that comes, we load balance it according to your defined weights among the LLMs. It's that simple!\n",
    "\n",
    "Weight for all LLMs that are passed to Portkey should sum up to 1. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_llm.mode=\"loadbalance\"\n",
    "\n",
    "llm1 = LLMBase(provider=\"openai\", model=\"gpt-4\",weight=0.2)\n",
    "llm2 = LLMBase(provider=\"openai\", model=\"gpt-3.5-turbo\",weight=0.8)\n",
    "\n",
    "pk_llm.add_llms(llm_params=[llm1,llm2])\n",
    "\n",
    "print(\"Testing Loadbalance functionality:\")\n",
    "response = pk_llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Semantic Caching with Portkey\n",
    "\n",
    "See the cache status on your Portkey dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "pk_llm.cache_status=\"semantic\"\n",
    "\n",
    "current_messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant\"),\n",
    "    ChatMessage(role=\"user\", content=\"What are the ingredients of a pizza?\")\n",
    "]\n",
    "\n",
    "print(\"Testing Portkey Semantic Cache:\")\n",
    "\n",
    "start = time.time()\n",
    "response = pk_llm.chat(current_messages)\n",
    "end = time.time() - start\n",
    "\n",
    "print(response)\n",
    "print(\"\\n--------------------------------------\\n\")\n",
    "print(f'Served in {end} seconds.')\n",
    "\n",
    "new_messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant\"),\n",
    "    ChatMessage(role=\"user\", content=\"Ingredients of pizza\")\n",
    "]\n",
    "\n",
    "print(\"Testing Portkey Semantic Cache:\")\n",
    "\n",
    "start = time.time()\n",
    "response = pk_llm.chat(new_messages)\n",
    "end = time.time() - start\n",
    "\n",
    "print(response)\n",
    "print(\"\\n--------------------------------------\\n\")\n",
    "print(f'Served in {end} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portkey's cache supports two more cache-critical functions - Force Refresh and Age.\n",
    "\n",
    "cache_force_refresh: Force-send a request to your provider instead of serving it from a cache\n",
    "cache_age: Decide the interval at which the cache store for this particular string should get automatically refreshed\n",
    "\n",
    "Here's how you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_llm.cache_age=1000\n",
    "pk_llm.cache_force_refresh=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observability with Portkey\n",
    "\n",
    "All of your requests are automatically logged to Portkey where you can see the whole payload, "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
