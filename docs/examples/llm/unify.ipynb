{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify\n",
    "The Unify platform dynamically routes each query to the best LLM, with support for providers such as OpenAI, MistralAI, Perplexity AI, and Together AI. Access all providers on the [platform](https://unify.ai/hub) seamlessly with single sign-on using just one API key. \n",
    "When using the Unify platform, you get:\n",
    "- Single sign-on with all providers.\n",
    "- Dynamic routing to the best provider for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install LlamaIndex ü¶ô and the Unify integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-unify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "- Get your Unify API Key from the [Unify console](https://console.unify.ai/)\n",
    "- Set the `UNIFY_API_KEY` environment variable to the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"UNIFY_API_KEY\"] = \"<YOUR API KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `complete` with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I'm doing well, thanks for asking! I'm feeling a bit mischievous today, so I hope you're ready for some fun and games. What do you say we play a game of \"Guess the Number\" or \"Hangman\" to get things started? Or if you have a different game in mind, I'm always up for trying something new!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.unify import Unify\n",
    "llm = Unify(model=\"llama-2-70b-chat@deepinfra\")\n",
    "print(llm.complete(\"How are you today, llama?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access endpoints from all providers via a single api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Together AI response: \n",
      "   I'm called Llama because I'm a machine learning model, and my creators wanted to give me a unique and memorable name. They chose the name Llama because it's a bit quirky and unexpected, and it stands out among other AI models.\n",
      "\n",
      "Llamas are actually domesticated animals that are native to South America, and they are known for their distinctive long necks and ears. They are not closely related to goats, although they are both members of the camelid family.\n",
      "\n",
      "I don't have a physical body like a llama, of course - I exist only as a computer program designed to understand and generate text. But I'm glad to have a name that's a bit different and fun!\n",
      "\n",
      "\n",
      "Anyscale response: \n",
      "   Both goat and llama are good names, but they have different connotations and associations.\n",
      "\n",
      "Goat is a more general term that can refer to any member of the Bovidae family, including both male and female goats. It's a simple and straightforward name that is easily recognizable and understandable.\n",
      "\n",
      "Llama, on the other hand, specifically refers to the South American relative of the camel, known for its long neck and ears, as well as its ability to spit when threatened or annoyed. The name llama is derived from the Quechua language and has a more exotic and unique sound to it.\n",
      "\n",
      "Ultimately, whether goat or llama is a better name depends on the context and personal preference. If you're looking for a more general term that can refer to any member of the Bovidae family, goat might be a better choice. If you're looking for a more specific term that refers to the South American animal with a distinctive appearance and temperament, llama might be a better choice.\n"
     ]
    }
   ],
   "source": [
    "llm_together = Unify(model=\"llama-2-70b-chat@together-ai\")\n",
    "print(\"Together AI response: \\n\", llm_together.complete(\"Why are you called llama, isn't it a goat or something?\"))\n",
    "\n",
    "llm_anyscale = Unify(model=\"llama-2-70b-chat@anyscale\")\n",
    "print(\"\\n\\nAnyscale response: \\n\", llm_anyscale.complete(\"Isn't goat a better name than llama?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming and Dynamic routing\n",
    "For rapid live translation using mixtral-8x7b-instruct-v0.1, Unify's dynamic routing selects the fastest provider based on [live benchmarks](https://unify.ai/hub/mixtral-8x7b-instruct-v0). \n",
    "Configure dynamic routing [here](https://unify.ai/docs/hub/concepts/runtime_routing.html#how-to-use-it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Unify(model=\"mixtral-8x7b-instruct-v0.1@tks-per-sec\")\n",
    "\n",
    "response = llm.stream_complete(\n",
    "    \"Translate the following to German: \"\n",
    "    \"Hey, there's an emergency in translation street, \"\n",
    "    \"please send help asap!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and provider are : mixtral-8x7b-instruct-v0.1@fireworks-ai\n",
      "\n",
      "\"Hallo, es gibt einen Notfall in der √úbersetzungsstra√üe, bitte senden Sie sofort Hilfe!\" Please note that this is a loose translation and the sentence might not sound natural to native German speakers, as \"√úbersetzungsstra√üe\" is not a commonly used term in German. A more natural way to say it could be \"Hallo, es herrscht Notfall bei den √úbersetzungen, bitte schicken Sie schnellstm√∂gliche Hilfe!\""
     ]
    }
   ],
   "source": [
    "show_provider = True\n",
    "for r in response:\n",
    "    if show_provider:\n",
    "        print(f\"Model and provider are : {r.raw['model']}\\n\")\n",
    "        show_provider = False        \n",
    "    print(r.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async and cost based routing\n",
    "Alternatively, you can run this asynchronously.  For tasks like long document summarization, optimizing for input costs is crucial. Unify's dynamic router can do this too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and provider are : mixtral-8x7b-instruct-v0.1@deepinfra\n",
      "\n",
      " OpenAI: Pioneering 'safe' artificial general intelligence.\n"
     ]
    }
   ],
   "source": [
    "llm = Unify(model=\"mixtral-8x7b-instruct-v0.1@input-cost\")\n",
    "\n",
    "response = await llm.acomplete(\n",
    "    \"Summarize this in 10 words or less. OpenAI is a U.S. based artificial intelligence \"\n",
    "    \"(AI) research organization founded in December 2015, researching artificial intelligence \"\n",
    "    \"with the goal of developing 'safe and beneficial' artificial general intelligence, \"\n",
    "    \"which it defines as 'highly autonomous systems that outperform humans at most economically \"\n",
    "    \"valuable work'. As one of the leading organizations of the AI spring, it has developed \"\n",
    "    \"several large language models, advanced image generation models, and previously, released \"\n",
    "    \"open-source models. Its release of ChatGPT has been credited with starting the AI spring\"\n",
    ")\n",
    "\n",
    "print(f\"Model and provider are : {response.raw['model']}\\n\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
