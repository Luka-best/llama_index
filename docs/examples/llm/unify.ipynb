{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify\n",
    "Unify is a platform that offers dynamic routing of queries to the most appropriate endpoint served by different providers like OpenAI, MistralAI, Perplexity AI, Together AI and more. It offers single sign on access with one api key to access all the different providers supported by the [platform](https://unify.ai/hub).\n",
    "\n",
    "When using models through the Unify API endpoint, you get:\n",
    "- Single sign-on with all providers.\n",
    "- Dynamic routing to the best provider for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install LlamaIndex ðŸ¦™ and the Unify integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-unify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "- Get your Unify API Key from the [Unify console](https://console.unify.ai/)\n",
    "- Set the `UNIFY_API_KEY` environment variable to the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"UNIFY_API_KEY\"] = \"<YOUR API KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `complete` with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='  I\\'m doing well, thanks for asking! I\\'m feeling a bit mischievous today, so I hope you\\'re ready for some fun and games. What do you say we play a game of \"Guess the Number\" or \"Hangman\" to get things started? Or if you have a different game in mind, I\\'m always up for trying something new. Just let me know what you\\'d like to do and we\\'ll get started!', additional_kwargs={}, raw={'id': 'chatcmpl-1fb0b98b67d34b7188ec4b30d26f4ac0', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='  I\\'m doing well, thanks for asking! I\\'m feeling a bit mischievous today, so I hope you\\'re ready for some fun and games. What do you say we play a game of \"Guess the Number\" or \"Hangman\" to get things started? Or if you have a different game in mind, I\\'m always up for trying something new. Just let me know what you\\'d like to do and we\\'ll get started!', role='assistant', function_call=None, tool_calls=None, name=None))], 'created': 1710442328, 'model': 'llama-2-70b-chat@deepinfra', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=101, prompt_tokens=17, total_tokens=118, cost=0.0001028)}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.unify import Unify\n",
    "llm = Unify(model=\"llama-2-70b-chat@deepinfra\")\n",
    "llm.complete(\"How are you today, llama?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of using Unify is that through a single api key, you can access endpoints from different providers like Perplexity, Together AI, OctoAI and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Together AI response: \n",
      "   I'm called Llama because I'm a machine learning model, and my creators wanted to give me a unique and memorable name. They chose the name Llama because it's a bit quirky and unexpected, and it stands out among other AI models.\n",
      "\n",
      "Llamas are actually domesticated animals that are native to South America, and they are known for their distinctive long necks and ears. They are not closely related to goats, although they are both members of the camelid family.\n",
      "\n",
      "I don't have a physical body like a llama, of course - I exist only as a computer program designed to understand and generate text. But I'm glad to have a name that's a bit different and fun!\n",
      "\n",
      "\n",
      "Anyscale response: \n",
      "   Both goat and llama are good names, but they have different connotations and associations.\n",
      "\n",
      "Goat is a more general term that can refer to any member of the Bovidae family, including both male and female goats. It's a simple and straightforward name that is easily recognizable and understandable.\n",
      "\n",
      "Llama, on the other hand, specifically refers to the South American relative of the camel, known for its long neck and ears, as well as its ability to spit when threatened or annoyed. The name llama is derived from the Quechua language and has a more exotic and unique sound to it.\n",
      "\n",
      "Ultimately, whether goat or llama is a better name depends on the context and personal preference. If you're looking for a more general term that can refer to any member of the Bovidae family, goat might be a better choice. If you're looking for a more specific term that refers to the South American animal with a distinctive appearance and temperament, llama might be a better choice.\n"
     ]
    }
   ],
   "source": [
    "llm_together = Unify(model=\"llama-2-70b-chat@together-ai\")\n",
    "print(\"Together AI response: \\n\", llm_together.complete(\"Why are you called llama, isn't it a goat or something?\"))\n",
    "\n",
    "llm_anyscale = Unify(model=\"llama-2-70b-chat@anyscale\")\n",
    "print(\"\\n\\nAnyscale response: \\n\", llm_anyscale.complete(\"Isn't goat a better name than llama?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming and Dynamic routing\n",
    "You may also want to use `mixtral-8x7b-instruct-v0.1` with the fastest output speed, and are not concerned with which provider you are using. Unify's dynamic routing gets this data from our live benchmarks and routes you to the provider that is performing better than the rest at that instant.\n",
    "\n",
    "You can see how to configure dynamic routing [here](https://unify.ai/docs/hub/concepts/runtime_routing.html#how-to-use-it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Unify(model=\"mixtral-8x7b-instruct-v0.1@tks-per-sec\")\n",
    "response = llm.stream_complete(\"Why is mixtral so competitive with bigger models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and provider are : mixtral-8x7b-instruct-v0.1@fireworks-ai\n",
      "\n",
      "I'm assuming you are referring to MixTraL, a recently proposed language model architecture. MixTraL is a relatively new model, and while it has shown promising results in some benchmarks, it's not accurate to label it as \"so competitive with bigger models\" just yet.\n",
      "\n",
      "MixTraL combines the strengths of Transformer and Mixture of Experts (MoE) models to achieve good performance while keeping the model size relatively small. The key idea is to partition the model's parameters into several expert sub-networks and use a gating network to select the most relevant experts for each input token. This approach allows MixTraL to scale up to a larger model size without a proportional increase in computational requirements.\n",
      "\n",
      "However, MixTraL still faces some challenges compared to bigger models:\n",
      "\n",
      "1. Data size: Larger models typically have access to more data, which can lead to better performance. MixTraL has been trained"
     ]
    }
   ],
   "source": [
    "show_provider = True\n",
    "for r in response:\n",
    "    if show_provider:\n",
    "        print(f\"Model and provider are : {r.raw['model']}\\n\")\n",
    "        show_provider = False        \n",
    "    print(r.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async and cost based routing\n",
    "You can also run this asynchronously, and may want to optimize for cost. If you need to run large volumes of data through some models, and need it by tomorrow, and want to optimize for cost and aren't interested too much in speed, but want to save on input costs. Unify's dynamic router can be used for this too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and provider are : mixtral-8x7b-instruct-v0.1@deepinfra\n",
      "\n",
      " Synchronous (sync) code and asynchronous (async) code are two different ways to write code that allow you to control the order in which operations are executed in your program.\n",
      "\n",
      "Synchronous code is executed in a sequential manner, meaning that each statement is executed one after the other, and the program waits for the current statement to complete before moving on to the next one. This is useful when you have a series of operations that depend on each other and need to be executed in a specific order.\n",
      "\n",
      "Asynchronous code, on the other hand, allows you to execute multiple operations concurrently, without waiting for each one to complete before moving on to the next. This is useful when you have operations that take a long time to complete, such as reading from a file or making a network request, and you don't want to block the execution of the rest of your program while you wait for them to finish.\n",
      "\n",
      "In summary, the main difference between sync and async code is that sync code is executed in a sequential manner, while async code is executed concurrently. Sync code is useful for operations that need to be executed in a specific order, while async code is useful for operations that can be executed concurrently without affecting the overall flow of the program.\n"
     ]
    }
   ],
   "source": [
    "llm = Unify(model=\"mixtral-8x7b-instruct-v0.1@input-cost\")\n",
    "response = await llm.acomplete(\"What is the difference between sync and async code?\")\n",
    "print(f\"Model and provider are : {response.raw['model']}\\n\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
