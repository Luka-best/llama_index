{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bcdd1c-c69f-4da6-9712-29a8ad25b858",
   "metadata": {},
   "source": [
    "## Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0522215-4d36-4bbc-8c77-945c4e1bc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://openai.com/blog/new-models-and-developer-products-announced-at-devday\"\n",
    "\n",
    "from llama_hub.web.simple_web.base import SimpleWebPageReader\n",
    "\n",
    "reader = SimpleWebPageReader(html_to_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca253893-b34c-476f-b897-d47e2c56a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = reader.load_data(urls=[url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ce02e1-1edc-44fd-b160-0e15cc1961f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(documents[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85593c00-24f7-4a67-9492-70d485266867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image document\n",
    "from llama_index.schema import ImageDocument\n",
    "\n",
    "image_document = ImageDocument(image_path=\"other_images/openai/dev_day.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dfd978-7f56-4a15-9e56-019c7a35f35e",
   "metadata": {},
   "source": [
    "## Setup Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b510d5-419e-4963-bba9-cb76efcf7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "367e4884-bf62-45f1-9e0c-211379287bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60b78d55-5a36-4943-8a25-468e26566c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc8bb089-f84f-465f-a9df-d883df82d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tool = QueryEngineTool(\n",
    "    query_engine=vector_index.as_query_engine(),\n",
    "    metadata=ToolMetadata(\n",
    "        name=f\"vector_tool\",\n",
    "        description=(\n",
    "            \"Useful to lookup new features announced by OpenAI\"\n",
    "            # \"Useful to lookup any information regarding the image\"\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa1543-5c43-43bd-8d22-fe2729a51b25",
   "metadata": {},
   "source": [
    "## Setup Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8c6c99c-6de1-44af-ae4b-b4c8144ca0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.react_multimodal.step import MultimodalReActAgentWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f698d34d-6cb6-419b-b4f8-983f85ee2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import AgentRunner\n",
    "from llama_index.multi_modal_llms import MultiModalLLM, OpenAIMultiModal\n",
    "from llama_index.agent import Task\n",
    "\n",
    "mm_llm = OpenAIMultiModal(model=\"gpt-4-vision-preview\", max_new_tokens=1000)\n",
    "\n",
    "# Option 2: Initialize AgentRunner with OpenAIAgentWorker\n",
    "react_step_engine = MultimodalReActAgentWorker.from_tools(\n",
    "    [query_tool], \n",
    "    # [],\n",
    "    multi_modal_llm=mm_llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(react_step_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afbe933d-fa7c-4d56-8c0f-faf1746bc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "    # \"The photo shows some new features released by OpenAI. Can you pinpoint the features in the photo and give more details using relevant tools?\",\n",
    "    \"Tell me more about code_interpreter and how it's used\",\n",
    "    extra_state={\"image_docs\": [image_document]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61acf913-7ff7-445f-afb9-537557457d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm_response = mm_llm.complete(\n",
    "#     \"Can you pinpoint the features in the photo and explain them?\",\n",
    "#     [image_document]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b61a9fb1-6942-46d0-b725-4a5b82688641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message_dicts = mm_llm._get_multi_modal_chat_messages(\n",
    "#     prompt=\"Can you pinpoint the features in the photo and explain them?\",\n",
    "#     role=\"user\",\n",
    "#     image_documents=[image_document]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "707f9e98-f2cd-48cc-be36-886509ca8e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(message_dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9bb15f0a-e401-42b4-809e-7c8628c47e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(mm_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18c30caf-2f3a-4202-8fde-b1c3099db726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_step(agent: AgentRunner, task: Task):\n",
    "    step_output = agent.run_step(task.task_id)\n",
    "    if step_output.is_last:\n",
    "        response = agent.finalize_response(task.task_id)\n",
    "        print(f\"> Agent finished: {str(response)}\")\n",
    "        return response\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def execute_steps(agent: AgentRunner, task: Task):\n",
    "    response = execute_step(agent, task)\n",
    "    while response is None:\n",
    "        response = execute_step(agent, task)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de20408f-a647-42cb-9915-fbec13d8b196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The image shows a code snippet in a code editor within the Playground interface. The code is written in Python and appears to be calculating the time it takes for light to travel from Earth to the Moon. The user is asking about the \"code_interpreter\" tool, which is likely a feature within the Playground interface that allows users to run code snippets and see the output. I need to use a tool to look up the latest information on the \"code_interpreter\" feature and how it is used.\n",
      "Action: vector_tool\n",
      "Action Input: {'input': 'code_interpreter feature OpenAI Playground'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: The Assistants API, which is part of OpenAI's platform, includes a feature called Code Interpreter. This feature allows developers to write and run Python code within a sandboxed execution environment. The Code Interpreter can generate graphs and charts, process files with diverse data and formatting, and solve challenging code and math problems iteratively. It provides a convenient way for assistants to incorporate code execution capabilities into their messages. You can try out the Code Interpreter feature in the OpenAI Playground, which is a web-based interface for creating high-quality assistants without writing any code.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# execute_steps(agent, task)\n",
    "execute_step(agent, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85783870-d51b-46cb-8588-3de1bdf1feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_step(agent, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c45113bd-73aa-44f5-b803-652231fcb92a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Observation: The latest features released by OpenAI include the GPT-4 Turbo model, the Assistants API, and multimodal capabilities such as vision, image creation (DALLÂ·E 3), and text-to-speech (TTS). These features were announced in a recent blog post and will be rolled out to OpenAI customers starting at 1pm PT today.\n"
     ]
    }
   ],
   "source": [
    "print(str(task.extra_state[\"input_chat\"][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a5bafb-48e8-4ba9-88f4-20311b8991d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
