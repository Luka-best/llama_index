{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "368686b4-f487-4dd4-aeff-37823976529d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/multi_modal/anthropic_multi_modal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Multi-Modal LLM using Anthropic model for image reasoning\n",
    "\n",
    "Anthropic has recently released its latest Multi modal models: Claude 3 Opus, Claude 3 Sonnet.\n",
    "\n",
    "1. Claude 3 Opus - claude-3-opus-20240229\n",
    "\n",
    "2. Claude 3 Sonnet - claude-3-sonnet-20240229\n",
    "\n",
    "In this notebook, we show how to use Anthropic MultiModal LLM class/abstraction for image understanding/reasoning.\n",
    "\n",
    "We also show several functions we are now supporting for Anthropic MultiModal LLM:\n",
    "* `complete` (both sync and async): for a single prompt and list of images\n",
    "* `chat` (both sync and async): for multiple chat messages\n",
    "* `stream complete` (both sync and async): for steaming output of complete\n",
    "* `stream chat` (both sync and async): for steaming output of chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-multi-modal-llms-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc691ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4479bf64",
   "metadata": {},
   "source": [
    "##  Use Anthropic to understand Images from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\n",
    "    \"ANTHROPIC_API_KEY\"\n",
    "] = \"YOUR ANTROPIC API KEY\"  # Your ANTHROPIC API key here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d0d083e",
   "metadata": {},
   "source": [
    "## Initialize `AnthropicMultiModal` and Load Images from URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627c8a1",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.anthropic import AnthropicMultiModal\n",
    "\n",
    "from llama_index.core.multi_modal_llms.generic_utils import load_image_urls\n",
    "\n",
    "\n",
    "image_urls = [\n",
    "    \"https://res.cloudinary.com/hello-tickets/image/upload/c_limit,f_auto,q_auto,w_1920/v1640835927/o3pfl41q7m5bj8jardk0.jpg\",\n",
    "]\n",
    "\n",
    "image_documents = load_image_urls(image_urls)\n",
    "\n",
    "anthropic_mm_llm = AnthropicMultiModal(max_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d94bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_response = requests.get(image_urls[0])\n",
    "print(image_urls[0])\n",
    "img = Image.open(BytesIO(img_response.content))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9c116",
   "metadata": {},
   "source": [
    "### Complete a prompt with a bunch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ab53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_response = anthropic_mm_llm.complete(\n",
    "    prompt=\"Describe the images as an alternative text\",\n",
    "    image_documents=image_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff28b6",
   "metadata": {},
   "source": [
    "### Steam Complete a prompt with a bunch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab28aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_complete_response = anthropic_mm_llm.stream_complete(\n",
    "    prompt=\"give me more context for this image\",\n",
    "    image_documents=image_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fd47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in stream_complete_response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bb503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.multi_modal_llms.openai_utils import (\n",
    "    generate_openai_multi_modal_chat_message,\n",
    ")\n",
    "\n",
    "chat_msg_1 = generate_openai_multi_modal_chat_message(\n",
    "    prompt=\"Describe the images as an alternative text\",\n",
    "    role=\"user\",\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "\n",
    "chat_msg_2 = generate_openai_multi_modal_chat_message(\n",
    "    prompt=\"The image is a graph showing the surge in US mortgage rates. It is a visual representation of data, with a title at the top and labels for the x and y-axes. Unfortunately, without seeing the image, I cannot provide specific details about the data or the exact design of the graph.\",\n",
    "    role=\"assistant\",\n",
    ")\n",
    "\n",
    "chat_msg_3 = generate_openai_multi_modal_chat_message(\n",
    "    prompt=\"can I know more?\",\n",
    "    role=\"user\",\n",
    ")\n",
    "\n",
    "chat_messages = [chat_msg_1, chat_msg_2, chat_msg_3]\n",
    "chat_response = anthropic_mm_llm.chat(\n",
    "    # prompt=\"Describe the images as an alternative text\",\n",
    "    messages=chat_messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb2d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in chat_messages:\n",
    "    print(msg.role, msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b719f",
   "metadata": {},
   "source": [
    "### Stream Chat through a list of chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d7140",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_chat_response = anthropic_mm_llm.stream_chat(\n",
    "    prompt=\"Describe the images as an alternative text\",\n",
    "    messages=chat_messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd055085",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in stream_chat_response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c86c542",
   "metadata": {},
   "source": [
    "### Async Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e75a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_acomplete = await anthropic_mm_llm.acomplete(\n",
    "    prompt=\"Describe the images as an alternative text\",\n",
    "    image_documents=image_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_acomplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8cfeec",
   "metadata": {},
   "source": [
    "### Async Steam Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c2850",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_astream_complete = await anthropic_mm_llm.astream_complete(\n",
    "    prompt=\"Describe the images as an alternative text\",\n",
    "    image_documents=image_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b35a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for delta in response_astream_complete:\n",
    "    print(delta.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1767dee",
   "metadata": {},
   "source": [
    "### Async Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "achat_response = await anthropic_mm_llm.achat(\n",
    "    messages=chat_messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240fda11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(achat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693bec3",
   "metadata": {},
   "source": [
    "### Async stream Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fef9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "astream_chat_response = await anthropic_mm_llm.astream_chat(\n",
    "    messages=chat_messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a99f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for delta in astream_chat_response:\n",
    "    print(delta.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096fb75",
   "metadata": {},
   "source": [
    "## Complete with Two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = [\n",
    "    \"https://www.visualcapitalist.com/wp-content/uploads/2023/10/US_Mortgage_Rate_Surge-Sept-11-1.jpg\",\n",
    "    \"https://www.sportsnet.ca/wp-content/uploads/2023/11/CP1688996471-1040x572.jpg\",\n",
    "]\n",
    "\n",
    "image_documents_1 = load_image_urls(image_urls)\n",
    "\n",
    "response_multi = anthropic_mm_llm.complete(\n",
    "    prompt=\"is there any relationship between those images?\",\n",
    "    image_documents=image_documents_1,\n",
    ")\n",
    "print(response_multi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8738293",
   "metadata": {},
   "source": [
    "##  Use Anthropic Multi Model to understand images from local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# put your local directore here\n",
    "image_documents = SimpleDirectoryReader(\"./data/\").load_data()\n",
    "\n",
    "response = anthropic_mm_llm.complete(\n",
    "    prompt=\"Describe the images as an alternative text\",\n",
    "    image_documents=image_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open(\"./data/1.jpg\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anthropic_env",
   "language": "python",
   "name": "anthropic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd48f70dff03045e65d79a07690526a58f67f0a1f6ad1b3133ba75880aff4bd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
