{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenInference Callback Handler\n",
    "\n",
    "[OpenInference](https://github.com/Arize-ai/open-inference-spec) is an open standard for capturing and storing AI model inferences. It enables production LLMapp servers to seamlessly integrate with LLM observability solutions such as [Arize](https://arize.com/) and [Phoenix](https://github.com/Arize-ai/phoenix).\n",
    "\n",
    "The `OpenInferenceCallbackHandler` saves data from retrieval-augmented generation (RAG) systems for downstream analysis and debugging. In particular, it saves the following data in columnar format:\n",
    "\n",
    "- query IDs\n",
    "- query text\n",
    "- query embeddings\n",
    "- scores (e.g., cosine similarity)\n",
    "- retrieved document IDs\n",
    "\n",
    "This tutorial demonstrates the callback handler's use for both in-notebook experimentation and lightweight production logging.\n",
    "\n",
    "⚠️ The `OpenInferenceCallbackHandler` is in beta and its APIs are subject to change.\n",
    "\n",
    "ℹ️ The callback maintainers are actively working to support additional RAG use-cases. If you find that your particular query engine or use-case is not supported, email the maintainers at phoenix-devs@arize.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies and Import Libraries\n",
    "\n",
    "Install notebook dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install html2text llama-index pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleWebPageReader,\n",
    "    ServiceContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.callbacks import CallbackManager, OpenInferenceCallbackHandler\n",
    "from llama_index.callbacks.open_inference_callback import DataBuffer, QueryData\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Parse Documents\n",
    "\n",
    "Load documents from Paul Graham's essay \"What I Worked On\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleWebPageReader().load_data(\n",
    "    [\n",
    "        \"http://raw.githubusercontent.com/jerryjliu/llama_index/main/examples/paul_graham_essay/data/paul_graham_essay.txt\"\n",
    "    ]\n",
    ")\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the document into nodes. Display the first node's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print(nodes[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access RAG Data as a Pandas Dataframe\n",
    "\n",
    "When experimenting with chatbots and LLMapps in a notebook, it's often useful to run your chatbot against a small collection of user queries and collect and analyze the data for iterative improvement. The `OpenInferenceCallbackHandler` stores RAG data in columnar format and provides convenient access to the data as a pandas dataframe.\n",
    "\n",
    "Instantiate the OpenInference callback handler and attach to the service context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_handler = OpenInferenceCallbackHandler()\n",
    "callback_manager = CallbackManager([callback_handler])\n",
    "service_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the index and instantiate the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your query engine across a collection of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_characters_per_line = 80\n",
    "queries = [\n",
    "    \"What did Paul Graham do growing up?\",\n",
    "    \"When and how did Paul Graham's mother die?\",\n",
    "    \"What, in Paul Graham's opinion, is the most distinctive thing about YC?\",\n",
    "    \"When and how did Paul Graham meet Jessica Livingston?\",\n",
    "    \"What is Bel, and when and where was it written?\",\n",
    "]\n",
    "for query in queries:\n",
    "    response = query_engine.query(query)\n",
    "    print(\"Query\")\n",
    "    print(\"=====\")\n",
    "    print(textwrap.fill(query, max_characters_per_line))\n",
    "    print()\n",
    "    print(\"Response\")\n",
    "    print(\"========\")\n",
    "    print(textwrap.fill(str(response), max_characters_per_line))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from your query engine runs can be accessed as a pandas dataframe for analysis and iterative improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_handler.query_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe column names conform to the OpenInference spec, which specifies the category, data type, and intent of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Production RAG Data\n",
    "\n",
    "In a production setting, LlamaIndex application maintainers can log the data generated by their system by implementing and passing a custom `logger` to `OpenInferenceCallbackHandler`. The logger is a callable that accepts a buffer of data from the `OpenInferenceCallbackHandler`, persists the data (e.g., by uploading to cloud storage or sending to a data ingestion service), and flushes the buffer after data is persisted. A reference implementation is included below that periodically writes data in OpenInference format to local Parquet files when the buffer exceeds a certain size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalParquetLogger:\n",
    "    def __init__(self, data_path: str, file_size_threshold_in_bytes: int = 1000):\n",
    "        \"\"\"Writes query data to Parquet files.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Directory where parquet files will be written. If the directory does\n",
    "            not exist, it will be created.\n",
    "\n",
    "            file_size_threshold_in_bytes (int): File-size threshold in bytes. Once this threshold is\n",
    "            exceeded, the query data will be written and the buffer will be flushed.\n",
    "        \"\"\"\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        self._data_path = data_path\n",
    "        self._max_size_in_bytes = file_size_threshold_in_bytes\n",
    "\n",
    "    def __call__(self, query_data_buffer: DataBuffer[QueryData]) -> None:\n",
    "        \"\"\"Writes the query data buffer to a Parquet file if the buffer size exceeds the configured\n",
    "           file size threshold.\n",
    "\n",
    "        Args:\n",
    "            query_data_buffer (DataBuffer[QueryData]): A buffer of query data.\n",
    "        \"\"\"\n",
    "        if query_data_buffer.size_in_bytes > self._max_size_in_bytes:\n",
    "            self._flush_buffer(query_data_buffer)\n",
    "\n",
    "    def _flush_buffer(self, query_data_buffer: DataBuffer[QueryData]) -> None:\n",
    "        \"\"\"Writes the query data buffer to a Parquet file and empties the buffer.\n",
    "\n",
    "        Args:\n",
    "            data_buffer (DataBuffer[QueryData]): A buffer of query data.\n",
    "        \"\"\"\n",
    "        query_dataframe = query_data_buffer.dataframe\n",
    "        query_ids_string = str(query_dataframe[\":id.str:\"].tolist())\n",
    "        batch_id = self._hash_string(query_ids_string)\n",
    "        file_path = os.path.join(self._data_path, f\"log-{batch_id}.parquet\")\n",
    "        query_dataframe.to_parquet(file_path)\n",
    "        query_data_buffer.clear()\n",
    "\n",
    "    @staticmethod\n",
    "    def _hash_string(string: str) -> str:\n",
    "        \"\"\"Hashes a string using SHA256.\n",
    "\n",
    "        Args:\n",
    "            string (str): Input string.\n",
    "\n",
    "        Returns:\n",
    "            str: Hashed string.\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(string.encode()).hexdigest()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ In a production setting, it's important to configure the `max_size_in_bytes` parameter on `OpenInferenceCallbackHandler` or periodically flush the buffer via your own custom logger logic, otherwise, the callback handler will indefinitely accumulate data in memory and eventually cause your system to crash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach the logger to your callback and re-run the query engine. The RAG data will be saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"rag_data\"\n",
    "logger = LocalParquetLogger(\n",
    "    data_path=data_path,\n",
    "    # this parameter is set artificially low for demonstration purposes\n",
    "    # to force a flush to disk, in practice it would be much larger\n",
    "    file_size_threshold_in_bytes=40,\n",
    ")\n",
    "callback_handler = OpenInferenceCallbackHandler(\n",
    "    logger=logger,\n",
    "    # this parameter is not necessary since the logger flushes to disk\n",
    "    # but is recommended as a safeguard\n",
    "    max_size_in_bytes=1000,\n",
    ")\n",
    "callback_manager = CallbackManager([callback_handler])\n",
    "service_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    query_engine.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and display saved Parquet data from disk to verify that the logger is working. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataframes = []\n",
    "for file_name in os.listdir(data_path):\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    query_dataframes.append(pd.read_parquet(file_path))\n",
    "query_dataframe = pd.concat(query_dataframes)\n",
    "query_dataframe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenixdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
