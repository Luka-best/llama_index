{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "034e355d-83a0-4bd2-877e-0f493c5f713d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 10K Analysis\n",
    "In this demo, we explore answering compare an contrast queries with two different approaches:\n",
    "1. ReAct agent, using prompting over a set of Tools\n",
    "2. OpenAIAgent, using the function calling API to choose Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fdc69e-4c3a-4b4a-babf-3733a0ba2d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # uncomment to turn on logging\n",
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c225e522-1ebd-436f-9c47-8738eb513880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1e705a-36f2-4272-8f98-7f3785e76e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, LLMPredictor, ServiceContext, GPTVectorStoreIndex\n",
    "from llama_index.response.pprint_utils import pprint_response\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35364259-f1c3-4df0-b8c9-79e0afca7436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=-1, streaming=True))\n",
    "# llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True))\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\", streaming=True))\n",
    "llm_predictor_gpt4 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\", streaming=True))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm_predictor=llm_predictor_gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7113434-0e41-46b6-a74e-284ce211fd38",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d036819a-754e-460b-8734-8af7071287e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "march_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_march_2022.pdf\"]).load_data()\n",
    "june_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_june_2022.pdf\"]).load_data()\n",
    "sept_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_sept_2022.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd541b68-c67f-4cbf-b579-5437d48e5b8f",
   "metadata": {},
   "source": [
    "# Build indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e6c3178-6aab-4fdc-99f6-c820661e7a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "march_index = GPTVectorStoreIndex.from_documents(march_2022)\n",
    "june_index = GPTVectorStoreIndex.from_documents(june_2022)\n",
    "sept_index = GPTVectorStoreIndex.from_documents(sept_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3af29f86-9a18-4b8e-af38-8ddc24b550e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "march_engine = march_index.as_query_engine(similarity_top_k=3, service_context=service_context)\n",
    "june_engine = june_index.as_query_engine(similarity_top_k=3, service_context=service_context)\n",
    "sept_engine = sept_index.as_query_engine(similarity_top_k=3, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6471ed-5645-4bb0-b8db-1b964ff7cd23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OpenAI Function Agent with a Query Plan Tool\n",
    "\n",
    "Use OpenAIAgent, built on top of the OpenAI tool use interface.\n",
    "\n",
    "Feed it our QueryPlanTool, which is a Tool that takes in other tools. And the agent to generate a query plan DAG over these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a89972cc-f7b8-4ebd-9c39-935e8a3671ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "query_tool_sept = QueryEngineTool.from_defaults(\n",
    "    query_engine=sept_engine,\n",
    "    name=\"sept_2022\",\n",
    "    description=f\"Provides information about Uber quarterly financials ending September 2022\",\n",
    ")\n",
    "query_tool_june = QueryEngineTool.from_defaults(\n",
    "    query_engine=june_engine,\n",
    "    name=\"june_2022\",\n",
    "    description=f\"Provides information about Uber quarterly financials ending June 2022\",\n",
    ")\n",
    "query_tool_march = QueryEngineTool.from_defaults(\n",
    "    query_engine=march_engine,\n",
    "    name=\"march_2022\",\n",
    "    description=f\"Provides information about Uber quarterly financials ending March 2022\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ae6d0bd-bd85-4f99-8363-96e203156933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define query plan tool\n",
    "from llama_index.tools import QueryPlanTool\n",
    "# from llama_index.tools.query_plan_v2 import QueryPlanTool\n",
    "from llama_index import ResponseSynthesizer\n",
    "\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    service_context=service_context\n",
    ")\n",
    "query_plan_tool = QueryPlanTool.from_defaults(\n",
    "    query_engine_tools=[query_tool_sept, query_tool_june, query_tool_march],\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "320e8bde-6afc-4896-94bf-7a186f94fa49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'query_plan_tool',\n",
       " 'description': '        This is a query plan tool that takes in a list of tools and executes a query plan over these tools to answer a query. The query plan is a DAG of query nodes.\\n\\nGiven a list of tool names and the query plan schema, you can choose to generate a query plan to answer a question.\\n\\nThe tool names and descriptions are as follows:\\n\\n\\n\\n        Tool Name: sept_2022\\nTool Description: Provides information about Uber quarterly financials ending September 2022 \\n\\nTool Name: june_2022\\nTool Description: Provides information about Uber quarterly financials ending June 2022 \\n\\nTool Name: march_2022\\nTool Description: Provides information about Uber quarterly financials ending March 2022 \\n        ',\n",
       " 'parameters': {'title': 'QueryPlan',\n",
       "  'description': \"Query plan.\\n\\nContains a list of QueryNode objects (which is a recursive object).\\nOut of the list of QueryNode objects, one of them must be the root node.\\nThe root node is the one that isn't a dependency of any other node.\",\n",
       "  'type': 'object',\n",
       "  'properties': {'nodes': {'title': 'Nodes',\n",
       "    'description': 'The original question we are asking.',\n",
       "    'type': 'array',\n",
       "    'items': {'$ref': '#/definitions/QueryNode'}}},\n",
       "  'required': ['nodes'],\n",
       "  'definitions': {'QueryNode': {'title': 'QueryNode',\n",
       "    'description': 'Query node.\\n\\nA query node represents a query (query_str) that must be answered.\\nIt can either be answered by a tool (tool_name), or by a list of child nodes\\n(child_nodes).\\nThe tool_name and child_nodes fields are mutually exclusive.',\n",
       "    'type': 'object',\n",
       "    'properties': {'id': {'title': 'Id',\n",
       "      'description': 'ID of the query node.',\n",
       "      'type': 'integer'},\n",
       "     'query_str': {'title': 'Query Str',\n",
       "      'description': 'Question we are asking. This is the query string that will be executed. ',\n",
       "      'type': 'string'},\n",
       "     'tool_name': {'title': 'Tool Name',\n",
       "      'description': 'Name of the tool to execute the `query_str`.',\n",
       "      'type': 'string'},\n",
       "     'dependencies': {'title': 'Dependencies',\n",
       "      'description': 'List of sub-questions that need to be answered in order to answer the question given by `query_str`.Should be blank if there are no sub-questions to be specified, in which case `tool_name` is specified.',\n",
       "      'type': 'array',\n",
       "      'items': {'type': 'integer'}}},\n",
       "    'required': ['id', 'query_str']}}}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_plan_tool.metadata.to_openai_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6466d4bf-59bd-4916-bf28-c1eb71a0a650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index.selectors.llm_selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [query_plan_tool],\n",
    "    max_function_calls=10,\n",
    "    llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-0613\"),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb85a5-8539-4218-b337-521ba77771a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = agent.query(\"What were the risk factors in sept 2022?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4778a8b-c2de-494c-b825-d0fe3c1d4aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.tools.query_plan import QueryPlan\n",
    "\n",
    "query_plan = QueryPlan(**{\n",
    "  \"root\": {\n",
    "    \"query_str\": \"risk factors\",\n",
    "    \"tool_name\": \"sept_2022\",\n",
    "    \"child_nodes\": []\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f056a81-cf16-4c05-aa3b-4439f2dc808d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'QueryPlan',\n",
       " 'description': 'Query plan.\\n\\nContains the root QueryNode (which is a recursive object).\\nThe root node should contain the original query string to be executed.\\n\\nExample query plan in JSON format:\\n\\n```json\\n{\\n    \"root\": {\\n        \"query_str\": \"Compare the demographics of France and Italy.\",\\n        \"child_nodes\": [\\n            {\\n                \"query_str\": \"What are the demographics of France?\",\\n                \"tool_name\": \"france_demographics\",\\n                \"child_nodes\": []\\n            },\\n            {\\n                \"query_str\": \"What are the demographics of Italy?\",\\n                \"tool_name\": \"italy_demographics\",\\n                \"child_nodes\": []\\n            }\\n        ]\\n    }\\n}\\n```',\n",
       " 'type': 'object',\n",
       " 'properties': {'root': {'title': 'Root',\n",
       "   'description': 'Root node of the query plan. Should contain the original query string to be executed.',\n",
       "   'allOf': [{'$ref': '#/definitions/QueryNode'}]}},\n",
       " 'required': ['root'],\n",
       " 'definitions': {'QueryNode': {'title': 'QueryNode',\n",
       "   'description': 'Query node.\\n\\nA query node represents a query (query_str) that must be answered.\\nIt can either be answered by a tool (tool_name), or by a list of child nodes\\n(child_nodes).\\nThe tool_name and child_nodes fields are mutually exclusive.',\n",
       "   'type': 'object',\n",
       "   'properties': {'query_str': {'title': 'Query Str',\n",
       "     'description': 'Question we are asking. This is the query string that will be executed. We will either provide a tool to execute the query, or a list of child nodes containing sub-questions that will be executed first, and the results of which will be used as context to execute the current query string.',\n",
       "     'type': 'string'},\n",
       "    'tool_name': {'title': 'Tool Name',\n",
       "     'description': 'Name of the tool to execute the `query_str`.',\n",
       "     'type': 'string'},\n",
       "    'child_nodes': {'title': 'Child Nodes',\n",
       "     'description': 'List of child nodes representing sub-questions that need to be answered in order to answer the question given by `query_str`.Should be blank if `tool_name` is specified.',\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/definitions/QueryNode'}}},\n",
       "   'required': ['query_str', 'child_nodes']}}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryPlan.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "feeb0afc-96c8-4be5-839c-ce8a64fb9e71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:01:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '51', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249968', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '7ms', 'x-request-id': '5dafb0dff6ba884387b7bf138e04a485', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da2249e9961171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:01:57 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '29', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249968', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '7ms', 'x-request-id': 'c36e9db37187b70565a09502fb7fd824', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da224b76df1171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: query_plan_tool with args: {\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"query_str\": \"What is Uber's revenue for March 2022?\",\n",
      "      \"tool_name\": \"march_2022\",\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"query_str\": \"What is Uber's revenue for June 2022?\",\n",
      "      \"tool_name\": \"june_2022\",\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"query_str\": \"What is Uber's revenue for September 2022?\",\n",
      "      \"tool_name\": \"sept_2022\",\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"query_str\": \"Calculate the revenue growth from March to June and from June to September\",\n",
      "      \"tool_name\": \"revenue_growth_calculator\",\n",
      "      \"dependencies\": [1, 2, 3]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3mExecuting node {\"id\": 4, \"query_str\": \"Calculate the revenue growth from March to June and from June to September\", \"tool_name\": \"revenue_growth_calculator\", \"dependencies\": [1, 2, 3]}\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mExecuting 3 child nodes\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mExecuting node {\"id\": 1, \"query_str\": \"What is Uber's revenue for March 2022?\", \"tool_name\": \"march_2022\", \"dependencies\": []}\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mSelected Tool: ToolMetadata(description='Provides information about Uber quarterly financials ending March 2022', name='march_2022', fn_schema=None)\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mExecuted query, got response.\n",
      "Query: What is Uber's revenue for March 2022?\n",
      "Response: Uber's revenue for the three months ended March 31, 2022, was $6.854 billion.\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mExecuting node {\"id\": 2, \"query_str\": \"What is Uber's revenue for June 2022?\", \"tool_name\": \"june_2022\", \"dependencies\": []}\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mSelected Tool: ToolMetadata(description='Provides information about Uber quarterly financials ending June 2022', name='june_2022', fn_schema=None)\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mExecuted query, got response.\n",
      "Query: What is Uber's revenue for June 2022?\n",
      "Response: Uber's revenue for June 2022 cannot be determined from the provided information. However, the revenue for the three months ended June 30, 2022, was $8,073 million.\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mExecuting node {\"id\": 3, \"query_str\": \"What is Uber's revenue for September 2022?\", \"tool_name\": \"sept_2022\", \"dependencies\": []}\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mSelected Tool: ToolMetadata(description='Provides information about Uber quarterly financials ending September 2022', name='sept_2022', fn_schema=None)\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mExecuted query, got response.\n",
      "Query: What is Uber's revenue for September 2022?\n",
      "Response: Uber's revenue for the three months ended September 30, 2022, was $8.343 billion.\n",
      "\u001b[0mGot output: To calculate the revenue growth from March to June, we can use the formula:\n",
      "\n",
      "((Revenue in June - Revenue in March) / Revenue in March) * 100\n",
      "\n",
      "Revenue growth from March to June:\n",
      "((8,073 - 6,854) / 6,854) * 100 = 17.8%\n",
      "\n",
      "To calculate the revenue growth from June to September, we can use the same formula:\n",
      "\n",
      "((Revenue in September - Revenue in June) / Revenue in June) * 100\n",
      "\n",
      "Revenue growth from June to September:\n",
      "((8,343 - 8,073) / 8,073) * 100 = 3.34%\n",
      "\n",
      "So, the revenue growth from March to June is 17.8%, and from June to September, it is 3.34%.\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Analyze Uber revenue growth in March, June, and September\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53bfd23d-cbb8-40ce-8bb7-feb3001398dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, we can analyze Uber's revenue growth for the three-month periods ending in March, June, and September.\n",
      "\n",
      "1. For the three months ended March 31, 2022, Uber's revenue was $6.854 billion.\n",
      "2. For the three months ended June 30, 2022, Uber's revenue was $8.073 billion.\n",
      "3. For the three months ended September 30, 2022, Uber's revenue was $8.343 billion.\n",
      "\n",
      "To analyze the growth, we can compare the revenue figures for each period:\n",
      "\n",
      "- From March to June, Uber's revenue increased by $1.219 billion ($8.073 billion - $6.854 billion), which represents a growth of approximately 17.8% (($1.219 billion / $6.854 billion) * 100).\n",
      "- From June to September, Uber's revenue increased by $0.270 billion ($8.343 billion - $8.073 billion), which represents a growth of approximately 3.3% (($0.270 billion / $8.073 billion) * 100).\n",
      "\n",
      "In summary, Uber experienced significant revenue growth of 17.8% between the three-month periods ending in March and June, followed by a smaller growth of 3.3% between the periods ending in June and September.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5bae0b65-59f5-4b5b-b5a7-1d8c5c4aa308",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 06:54:24 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '46', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249965', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '8ms', 'x-request-id': '06e14ea8510e7611c4bf57d9107ec007', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da219acef66171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 06:54:26 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '43', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249965', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '8ms', 'x-request-id': '0427bb42fb41fce447d44b0aee96bba4', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da219b4082a171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: query_plan_tool with args: {\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"query_str\": \"What were the risk factors in March 2022?\",\n",
      "      \"tool_name\": \"march_2022\",\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"query_str\": \"What were the risk factors in June 2022?\",\n",
      "      \"tool_name\": \"june_2022\",\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"query_str\": \"What were the risk factors in September 2022?\",\n",
      "      \"tool_name\": \"sept_2022\",\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"query_str\": \"Analyze changes in risk factors between March and June 2022\",\n",
      "      \"tool_name\": \"risk_analysis_tool\",\n",
      "      \"dependencies\": [1, 2]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"query_str\": \"Analyze changes in risk factors between June and September 2022\",\n",
      "      \"tool_name\": \"risk_analysis_tool\",\n",
      "      \"dependencies\": [2, 3]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"query_str\": \"Summarize the overall changes in risk factors from March to September 2022\",\n",
      "      \"tool_name\": \"summary_tool\",\n",
      "      \"dependencies\": [4, 5]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3mExecuting node {\"id\": 6, \"query_str\": \"Summarize the overall changes in risk factors from March to September 2022\", \"tool_name\": \"summary_tool\", \"dependencies\": [4, 5]}\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mExecuting 2 child nodes\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child_nodes' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnalyze changes in risk factors in march, june, and september for Uber\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/indices/query/base.py:23\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     22\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/openai_agent.py:155\u001b[0m, in \u001b[0;36mBaseOpenAIAgent._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatMessageHistory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/openai_agent.py:102\u001b[0m, in \u001b[0;36mBaseOpenAIAgent.chat\u001b[0;34m(self, message, chat_history)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExceeded max function calls: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_function_calls\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m function_message \u001b[38;5;241m=\u001b[39m \u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verbose\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m chat_history\u001b[38;5;241m.\u001b[39madd_message(function_message)\n\u001b[1;32m    106\u001b[0m n_function_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/openai_agent.py:50\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(tools, function_call, verbose)\u001b[0m\n\u001b[1;32m     48\u001b[0m tool \u001b[38;5;241m=\u001b[39m get_function_by_name(tools, name)\n\u001b[1;32m     49\u001b[0m argument_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(arguments_str)\n\u001b[0;32m---> 50\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margument_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/tools/query_plan_v2.py:206\u001b[0m, in \u001b[0;36mQueryPlanTool.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(root_nodes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery plan should have exactly one root node.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_nodes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/tools/query_plan_v2.py:169\u001b[0m, in \u001b[0;36mQueryPlanTool._execute_node\u001b[0;34m(self, node, nodes_dict)\u001b[0m\n\u001b[1;32m    165\u001b[0m     print_text(\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mdependencies)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m child nodes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpink\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m     child_nodes \u001b[38;5;241m=\u001b[39m [nodes_dict[dep] \u001b[38;5;28;01mfor\u001b[39;00m dep \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mdependencies]\n\u001b[0;32m--> 169\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# this is a leaf request, execute the query string using the specified tool\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     tool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_tools_dict[node\u001b[38;5;241m.\u001b[39mtool_name]\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/tools/query_plan_v2.py:149\u001b[0m, in \u001b[0;36mQueryPlanTool._execute_nodes\u001b[0;34m(self, query_str, nodes, nodes_dict)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute nodes.\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# execute the child nodes first\u001b[39;00m\n\u001b[1;32m    148\u001b[0m child_responses: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_node(child, nodes_dict) \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchild_nodes\u001b[49m\n\u001b[1;32m    150\u001b[0m ]\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# use ResponseSynthesizer to combine results\u001b[39;00m\n\u001b[1;32m    153\u001b[0m child_nodes \u001b[38;5;241m=\u001b[39m [Node(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m child_responses]\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child_nodes' referenced before assignment"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Analyze changes in risk factors in march, june, and september for Uber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01703941-f016-47d3-9cf9-fefac0fb54e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd52c07-0e46-4edd-8201-caa72b25ae1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# response = agent.query(\"Analyze both Uber revenue growth and risk factors over march, june, and september\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c47e41be-d61c-4992-b1c7-e038e1860d33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, we can analyze Uber's revenue growth for the three-month periods ending in March, June, and September.\n",
      "\n",
      "1. For the three months ended March 31, 2022, Uber's revenue was $6.854 billion.\n",
      "2. For the three months ended June 30, 2022, Uber's revenue was $8.073 billion.\n",
      "3. For the three months ended September 30, 2022, Uber's revenue was $8.343 billion.\n",
      "\n",
      "To analyze the growth, we can compare the revenue figures for each period:\n",
      "\n",
      "- From March to June, Uber's revenue increased by $1.219 billion ($8.073 billion - $6.854 billion), which represents a growth of approximately 17.8% (($1.219 billion / $6.854 billion) * 100).\n",
      "- From June to September, Uber's revenue increased by $0.270 billion ($8.343 billion - $8.073 billion), which represents a growth of approximately 3.3% (($0.270 billion / $8.073 billion) * 100).\n",
      "\n",
      "In summary, Uber experienced significant revenue growth of 17.8% between the three-month periods ending in March and June, followed by a smaller growth of 3.3% between the periods ending in June and September.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba9d02e4-4c5c-47d2-8cf4-62bcc967e6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:07:41 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '206', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249942', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '13ms', 'x-request-id': 'f3949fdbc166d0c2c5660d2ef3e8e8d5', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da22d217b71171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:07:43 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '140', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249942', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '13ms', 'x-request-id': 'a20d3c29417235332f6e60b831d96d14', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da22d298ac4171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:07:45 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '50', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249942', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '13ms', 'x-request-id': '6848341d437154c0592c36b3339d1b11', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da22d388925171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: query_plan_tool with args: {\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"query_str\": \"What is Uber's revenue growth and risk factors in March 2022?\",\n",
      "      \"tool_name\": \"march_2022\",\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"query_str\": \"What is Uber's revenue growth and risk factors in September 2022?\",\n",
      "      \"tool_name\": \"sept_2022\",\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"query_str\": \"Compare and contrast the revenue growth and risk factors of Uber in March 2022 and September 2022.\",\n",
      "      \"tool_name\": \"comparison_tool\",\n",
      "      \"dependencies\": [1, 2]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3mExecuting node {\"id\": 3, \"query_str\": \"Compare and contrast the revenue growth and risk factors of Uber in March 2022 and September 2022.\", \"tool_name\": \"comparison_tool\", \"dependencies\": [1, 2]}\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mExecuting 2 child nodes\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mExecuting node {\"id\": 1, \"query_str\": \"What is Uber's revenue growth and risk factors in March 2022?\", \"tool_name\": \"march_2022\", \"dependencies\": []}\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mSelected Tool: ToolMetadata(description='Provides information about Uber quarterly financials ending March 2022', name='march_2022', fn_schema=None)\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mExecuted query, got response.\n",
      "Query: What is Uber's revenue growth and risk factors in March 2022?\n",
      "Response: In March 2022, Uber's revenue was $6.9 billion, up 136% year-over-year. The company's growth and revenue have been affected by various risk factors, including the COVID-19 pandemic, which has caused significant fluctuations in their operating results. Other risk factors include seasonal fluctuations, competition in the markets they operate in, their ability to expand operations in new and existing markets, keeping pace with technological changes, changes in governmental or other regulations affecting their business, and harm to their brand or reputation.\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mExecuting node {\"id\": 2, \"query_str\": \"What is Uber's revenue growth and risk factors in September 2022?\", \"tool_name\": \"sept_2022\", \"dependencies\": []}\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mSelected Tool: ToolMetadata(description='Provides information about Uber quarterly financials ending September 2022', name='sept_2022', fn_schema=None)\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mExecuted query, got response.\n",
      "Query: What is Uber's revenue growth and risk factors in September 2022?\n",
      "Response: In September 2022, Uber's revenue was $8.3 billion, up 72% year-over-year. The revenue growth outpaced Gross Bookings growth primarily due to a $1.3 billion increase in their Freight business and the net favorable impact to Mobility revenue of $1.1 billion as a result of business model changes in the UK.\n",
      "\n",
      "Some risk factors for Uber include the duration and severity of the COVID-19 pandemic, their ability to grow supply and demand on their platform, increase existing platform users' activity, compete with competitors' products and offerings, develop new products and technologies, maintain existing local regulations in key markets, and increase positive perception of their brand. Additionally, they may continue to experience driver supply constraints due to the ongoing impacts of COVID-19.\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:09:35 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '39', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249644', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '85ms', 'x-request-id': 'd27c5633828daa6f4401bf8117cf5272', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da22fea8f7d171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got output: In March 2022, Uber's revenue was $6.9 billion, with a growth of 136% year-over-year, while in September 2022, the revenue increased to $8.3 billion, with a growth of 72% year-over-year. The revenue growth in September 2022 outpaced Gross Bookings growth mainly due to a significant increase in their Freight business and the net favorable impact of business model changes in the UK.\n",
      "\n",
      "The risk factors for Uber in both March and September 2022 were somewhat similar. In both periods, the COVID-19 pandemic played a significant role in affecting the company's growth and revenue. Other common risk factors included competition in the markets, the ability to expand operations, keeping pace with technological changes, changes in regulations, and harm to their brand or reputation.\n",
      "\n",
      "However, in September 2022, some additional risk factors were highlighted, such as the duration and severity of the COVID-19 pandemic, their ability to grow supply and demand on their platform, increase existing platform users' activity, and compete with competitors' products and offerings. Furthermore, they may continue to experience driver supply constraints due to the ongoing impacts of COVID-19.\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:09:37 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '78', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249644', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '85ms', 'x-request-id': '89b019974a5152523430626465117347', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da22ff1aeab171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:09:39 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '38', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249644', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '85ms', 'x-request-id': '071bcbe372d326d94565b5492109f3dc', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da22fff2d1b171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:09:43 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '50', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249644', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '85ms', 'x-request-id': 'd5296e46c4145940ddd38121159b545b', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da23018feb1171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 16.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:09:51 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '64', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249644', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '85ms', 'x-request-id': 'd291e9b84a05b945e272fa6684df5cee', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da2304bcc8b171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "The server had an error while processing your request. Sorry about that! {\n  \"error\": {\n    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:10:07 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '51', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249644', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '85ms', 'x-request-id': '9ba31bbdd7ee39e4aa74b77f56e53deb', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da230b15a17171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFirst look at Uber\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms revenue growth and risk factors in March, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthen revenue growth and risk factors in September, and then compare and contrast the two documents?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/indices/query/base.py:23\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     22\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/openai_agent.py:155\u001b[0m, in \u001b[0;36mBaseOpenAIAgent._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatMessageHistory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/openai_agent.py:109\u001b[0m, in \u001b[0;36mBaseOpenAIAgent.chat\u001b[0;34m(self, message, chat_history)\u001b[0m\n\u001b[1;32m    106\u001b[0m n_function_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# send function call & output back to get another response\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m ai_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m chat_history\u001b[38;5;241m.\u001b[39madd_message(ai_message)\n\u001b[1;32m    113\u001b[0m function_call \u001b[38;5;241m=\u001b[39m ai_message\u001b[38;5;241m.\u001b[39madditional_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:245\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:195\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    190\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    194\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 195\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:95\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     94\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     96\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[1;32m     97\u001b[0m generations \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39mgenerations \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:87\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m     92\u001b[0m     ]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     94\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 88\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m     92\u001b[0m     ]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     94\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:354\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m     message \u001b[38;5;241m=\u001b[39m _convert_dict_to_message(\n\u001b[1;32m    347\u001b[0m         {\n\u001b[1;32m    348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: inner_completion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m         }\n\u001b[1;32m    352\u001b[0m     )\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[ChatGeneration(message\u001b[38;5;241m=\u001b[39mmessage)])\n\u001b[0;32m--> 354\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:302\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m     retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:300\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAPIError\u001b[0m: The server had an error while processing your request. Sorry about that! {\n  \"error\": {\n    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 20 Jun 2023 07:10:07 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-4-0613', 'openai-organization': 'llamaindex', 'openai-processing-ms': '51', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249644', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '85ms', 'x-request-id': '9ba31bbdd7ee39e4aa74b77f56e53deb', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7da230b15a17171a-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"First look at Uber's revenue growth and risk factors in March, \" +\n",
    "    \"then revenue growth and risk factors in September, and then compare and contrast the two documents?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f6f3c-e5ac-401b-925a-7a6cce0620bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb580ce-b9f8-4cec-af16-58d0c13f7446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
