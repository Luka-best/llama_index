{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebf73ae-a632-4d09-b941-6739e35b760d",
   "metadata": {},
   "source": [
    "# An Introduction to LlamaIndex Query Pipelines\n",
    "\n",
    "## Overview\n",
    "LlamaIndex provides a declarative query API that allows you to chain together different modules in order to orchestrate simple-to-advanced workflows over your data.\n",
    "\n",
    "This is centered around our `QueryPipeline` abstraction. Load in a variety of modules (from LLMs to prompts to retrievers to other pipelines), connect them all together into a sequential chain or DAG, and run it end2end.\n",
    "\n",
    "**NOTE**: You can orchestrate all these workflows without the declarative pipeline abstraction (by using the modules imperatively and writing your own functions). So what are the advantages of `QueryPipeline`? \n",
    "\n",
    "- Express common workflows with fewer lines of code/boilerplate\n",
    "- Greater readability\n",
    "- Greater parity / better integration points with common low-code / no-code solutions (e.g. LangFlow)\n",
    "- [In the future] A declarative interface allows easy serializability of pipeline components, providing portability of pipelines/easier deployment to different systems.\n",
    "\n",
    "## Cookbook\n",
    "\n",
    "In this cookbook we give you an introduction to our `QueryPipeline` interface and show you some basic workflows you can tackle.\n",
    "\n",
    "- Chain together prompt and LLM\n",
    "- Chain together query rewriting (prompt + LLM) with retrieval\n",
    "- Chain together a full RAG query pipeline (query rewriting, retrieval, reranking, response synthesis)\n",
    "- Setting up a custom query component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144b2d4-adbc-44da-8c12-bdb5fe4b18bb",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we setup some data + indexes (from PG's essay) that we'll be using in the rest of the cookbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c272d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-voyageai\n",
    "%pip install llama-index-postprocessor-voyageai-rerank\n",
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc82744-965a-4d79-b357-faf3de7ba2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://127.0.0.1:6006/\n",
      "📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# setup Arize Phoenix for logging/observability\n",
    "import phoenix as px\n",
    "\n",
    "px.launch_app()\n",
    "import llama_index.core\n",
    "\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4208b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.voyageai import VoyageEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = VoyageEmbedding(model_name=\"voyage-large-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009af96-59e3-4d14-8272-382203c8b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(\"../data/paul_graham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d390b-38ca-4176-8cdb-8c2a0af1add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ddc81-0783-4fa5-ade0-60700c918011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"storage\"):\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    # save index to disk\n",
    "    index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(\"./storage\")\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c59b5c-9b18-4dfa-97ef-e39b8069b73c",
   "metadata": {},
   "source": [
    "## 1. Chain Together Prompt and LLM \n",
    "\n",
    "In this section we show a super simple workflow of chaining together a prompt with LLM.\n",
    "\n",
    "We simply define `chain` on initialization. This is a special case of a query pipeline where the components are purely sequential, and we automatically convert outputs into the right format for the next inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb233a0f-2993-4780-a241-6a2299047598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# try chaining basic prompts\n",
    "prompt_str = \"Please generate related movies to {movie_name}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26c0c6-b886-42a6-b524-b19b18d1c01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 1331b23f-d409-4b21-b9f7-a45b3c3c7031 with input: \n",
      "movie_name: The Departed\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module bfe6afb2-25ed-429f-a180-1c7581b1d5fa with input: \n",
      "messages: Please generate related movies to The Departed\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "output = p.run(movie_name=\"The Departed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987554f-4214-409f-97a8-40d2b13c25ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: 1. Infernal Affairs (2002) - The original Hong Kong film that inspired The Departed\n",
      "2. The Town (2010) - A crime thriller directed by Ben Affleck\n",
      "3. Mystic River (2003) - A crime drama directed by Clint Eastwood\n",
      "4. Goodfellas (1990) - A classic crime film directed by Martin Scorsese\n",
      "5. The Irishman (2019) - Another crime film directed by Martin Scorsese, starring Robert De Niro and Al Pacino\n",
      "6. The Godfather (1972) - A classic crime film directed by Francis Ford Coppola\n",
      "7. Heat (1995) - A crime thriller directed by Michael Mann, starring Al Pacino and Robert De Niro\n",
      "8. The Departed (2006) - A crime thriller directed by Martin Scorsese, starring Leonardo DiCaprio and Matt Damon\n",
      "9. Casino (1995) - A crime film directed by Martin Scorsese, starring Robert De Niro and Joe Pesci\n",
      "10. American Gangster (2007) - A crime film directed by Ridley Scott, starring Denzel Washington and Russell Crowe.\n"
     ]
    }
   ],
   "source": [
    "print(str(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44bac2-fff8-4578-8179-2cf68d075429",
   "metadata": {},
   "source": [
    "### Try Output Parsing\n",
    "\n",
    "Let's parse the outputs into a structured Pydantic object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc9aa6-d74e-4d02-8101-83ee03d68d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"Object representing a single movie.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"Name of the movie.\")\n",
    "    year: int = Field(..., description=\"Year of the movie.\")\n",
    "\n",
    "\n",
    "class Movies(BaseModel):\n",
    "    \"\"\"Object representing a list of movies.\"\"\"\n",
    "\n",
    "    movies: List[Movie] = Field(..., description=\"List of movies.\")\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = PydanticOutputParser(Movies)\n",
    "json_prompt_str = \"\"\"\\\n",
    "Please generate related movies to {movie_name}. Output with the following JSON format: \n",
    "\"\"\"\n",
    "json_prompt_str = output_parser.format(json_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b55ca-a7bd-43e4-a837-20e29b3bebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module e16375ef-6d2a-4297-a2de-26376a7af0b9 with input: \n",
      "movie_name: Toy Story\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 4acbc86f-36de-45e5-b9fb-1f79e7149f3a with input: \n",
      "messages: Please generate related movies to Toy Story. Output with the following JSON format: \n",
      "\n",
      "\n",
      "\n",
      "Here's a JSON schema to follow:\n",
      "{\"$defs\": {\"Movie\": {\"description\": \"Object representing a single movie.\", \"prop...\n",
      "\n",
      "> Running module 4e3c0f02-e277-4eab-9131-9ed0d04471d6 with input:\n",
      "input: assistant: {\n",
      "\"movies\": [\n",
      "{\n",
      "\"name\": \"Finding Nemo\",\n",
      "\"year\": 2003\n",
      "},\n",
      "{\n",
      "\"name\": \"Monsters, Inc.\",\n",
      "\"year\": 2001\n",
      "},\n",
      "{\n",
      "\"name\": \"The Incredibles\",\n",
      "\"y...\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# add JSON spec to prompt template\n",
    "json_prompt_tmpl = PromptTemplate(json_prompt_str)\n",
    "\n",
    "p = QueryPipeline(chain=[json_prompt_tmpl, llm, output_parser], verbose=True)\n",
    "output = p.run(movie_name=\"Toy Story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15bef7-399b-4dcc-94d6-6a4ea0066a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Movies(movies=[Movie(name='Finding Nemo', year=2003), Movie(name='Monsters, Inc.', year=2001), Movie(name='The Incredibles', year=2004)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
