{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cea58c-48bc-4af6-8358-df9695659983",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build your own OpenAI Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673df1fe-eb6c-46ea-9a73-a96e7ae7942e",
   "metadata": {
    "tags": []
   },
   "source": [
    "With the [new OpenAI API](https://openai.com/blog/function-calling-and-other-api-updates) that supports function calling, it's never been easier to build your own agent!\n",
    "\n",
    "In this notebook tutorial, we showcase how to write your own OpenAI agent in **under 50 lines of code**! It is minimal, yet feature complete (with ability to carry on a conversation and use tools)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b7bc2e-606f-411a-9490-fcfab9236dfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initial Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e80e5b-aaee-4f23-b338-7ae62b08141f",
   "metadata": {},
   "source": [
    "Let's start by importing some simple building blocks.  \n",
    "\n",
    "The main thing we need is:\n",
    "1. the OpenAI API (using our own `llama_index` LLM class)\n",
    "2. a place to keep conversation history \n",
    "3. a definition for tools that our agent can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d47283b-025e-4874-88ed-76245b22f82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "from typing import Sequence, List\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms import OpenAI, ChatMessage\n",
    "from llama_index.tools import BaseTool, FunctionTool\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "level = logging.WARNING\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=level)\n",
    "logger.setLevel(level)\n",
    "logging.getLogger(\"llama_index\").setLevel(level)\n",
    "logging.getLogger(\"asyncio\").setLevel(level)\n",
    "\n",
    "load_dotenv(\"../../../.env\")\n",
    "assert (key := os.getenv(\"OPENAI_API_KEY\"))\n",
    "openai.api_key = key\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe08eb1-e638-4c00-9103-5c305bfacccf",
   "metadata": {},
   "source": [
    "Let's define some very simple calculator tools for our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd3c4a6-f3e0-46f9-ad3b-7ba57d1bc992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfcfb78b-7d4f-48d9-8d4c-ffcded23e7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d30b8-6405-4187-a9ed-6146dcc42167",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Our (Slightly Better) `OpenAIAgent` Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ca3fd-6711-4c0c-a853-d868dd14b484",
   "metadata": {},
   "source": [
    "We provide a (slightly better) `OpenAIAgent` implementation in LlamaIndex, which you can directly use as follows.  \n",
    "\n",
    "In comparison to the simplified version above:\n",
    "* it implements the `BaseChatEngine` and `BaseQueryEngine` interface, so you can more seamlessly use it in the LlamaIndex framework. \n",
    "* it supports multiple function calls per conversation turn\n",
    "* it supports streaming\n",
    "* it supports async endpoints\n",
    "* it supports callback and tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ab3938-1138-43ea-b085-f430b42f5377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index.agent.experimental_openai_agent import ExperimentalOpenAIAgent\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d852ece7-e5a1-4368-9d59-c7014e0b5b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "agent = OpenAIAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\n",
    "exp_agent = ExperimentalOpenAIAgent.from_tools(\n",
    "    [multiply_tool, add_tool], llm=llm, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500cbee4",
   "metadata": {},
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd1cad5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 3\n",
      "}\n",
      "Got output: 363\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\n",
      "  \"a\": 363,\n",
      "  \"b\": 42\n",
      "}\n",
      "Got output: 405\n",
      "========================\n",
      "The result of (121 * 3) + 42 is 405.\n"
     ]
    }
   ],
   "source": [
    "response = exp_agent.chat(\"What is (121 * 3) + 42?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538bf32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect sources\n",
    "print(response.sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33983c",
   "metadata": {},
   "source": [
    "### Async Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d1fc974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 3\n",
      "}\n",
      "Got output: 363\n",
      "========================\n",
      "121 multiplied by 3 is equal to 363.\n"
     ]
    }
   ],
   "source": [
    "response = await exp_agent.achat(\"What is 121 * 3?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae035cb",
   "metadata": {},
   "source": [
    "### Streaming Chat\n",
    "Here, every LLM response is returned as a generator. You can stream every incremental step, or only the last response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14217fb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 2\n",
      "}\n",
      "Got output: 242\n",
      "========================\n",
      "121 multiplied by 2 is equal to 242.\n",
      "\n",
      "In a cozy corner, where shadows dance,\n",
      "A group of mice embark on a grand romance.\n",
      "With tiny paws and whiskers so fine,\n",
      "They scurry and play, in a world so divine.\n",
      "\n",
      "Together they gather, a family so tight,\n",
      "In their tiny kingdom, they find delight.\n",
      "With nimble steps, they explore the unknown,\n",
      "In search of adventure, they've always shown.\n",
      "\n",
      "Their fur, a tapestry of colors so bright,\n",
      "Each mouse unique, a captivating sight.\n",
      "They scamper and frolic, with joy in their eyes,\n",
      "Creating memories, as time swiftly flies.\n",
      "\n",
      "In their tiny world, they build their nests,\n",
      "A sanctuary where they can truly rest.\n",
      "They share their stories, their laughter and fears,\n",
      "Supporting each other, through joys and tears.\n",
      "\n",
      "With hearts so brave, they face the unknown,\n",
      "Unafraid to venture where others haven't flown.\n",
      "In unity they stand, a formidable force,\n",
      "A group of mice, their bond never to divorce.\n",
      "\n",
      "So let us celebrate these creatures so small,\n",
      "For in their presence, we find joy in all.\n",
      "In their unity and resilience, we find inspiration,\n",
      "A reminder of the power of collaboration."
     ]
    }
   ],
   "source": [
    "response = exp_agent.stream_chat(\n",
    "    \"What is 121 * 2? Once you have the answer, use that number to write a poem about a group of mice.\"\n",
    ")\n",
    "\n",
    "response_gen = response.response_gen\n",
    "\n",
    "for ix, token in enumerate(response_gen):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac119f",
   "metadata": {},
   "source": [
    "### Async Streaming Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1218cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 2\n",
      "}\n",
      "Got output: 242\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\n",
      "  \"a\": 242,\n",
      "  \"b\": 8\n",
      "}\n",
      "Got output: 250\n",
      "========================\n",
      "121 multiplied by 2 is 242. Adding 8 to that gives us 250.\n",
      "\n",
      "There once was a group of mice,\n",
      "Living in a world so nice.\n",
      "With paws so small,\n",
      "They would scurry and crawl,\n",
      "In their cozy little paradise.\n",
      "\n",
      "They danced and played all day,\n",
      "In their own mischievous way.\n",
      "With whiskers so fine,\n",
      "They would dine and dine,\n",
      "On crumbs they found along the way.\n",
      "\n",
      "In unity, they stood so strong,\n",
      "Facing challenges all along.\n",
      "With hearts full of glee,\n",
      "They lived wild and free,\n",
      "A group of mice, forever bonded in song.\n",
      "\n",
      "So let's raise a toast to these mice,\n",
      "Living their lives with such spice.\n",
      "In their tiny abode,\n",
      "They found love and joy,\n",
      "A group of mice, oh so wise."
     ]
    }
   ],
   "source": [
    "response = await exp_agent.astream_chat(\n",
    "    \"What is 121 * 2 + 8? Once you have the answer, use that number to write a limerick about a group of mice.\"\n",
    ")\n",
    "\n",
    "async for token in response.async_response_gen():\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe399c5-6d07-4926-b701-b612efd56b30",
   "metadata": {},
   "source": [
    "### Agent with Personality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47c034-f948-4604-a8d8-828b617ea245",
   "metadata": {},
   "source": [
    "You can specify a system prompt to give the agent additional instruction or personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef36d1e-c26e-4b07-b3d0-3b7f314a45f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.prompts.system import SHAKESPEARE_WRITING_ASSISTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba7fa46-1173-42f2-885c-0cc28df1cd2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [multiply_tool, add_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    system_prompt=SHAKESPEARE_WRITING_ASSISTANT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4841778-7008-4b61-afcc-995b6b64e91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"Hi\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a83768-1203-4485-a346-2fa78089afb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"Tell me a story\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c322b-bf5d-4140-9dcf-341062a4151b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
