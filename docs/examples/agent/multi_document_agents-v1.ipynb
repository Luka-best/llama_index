{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43497beb-817d-4366-9156-f4d7f0d44942",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multi-Document Agents (V1)\n",
    "\n",
    "In this guide, you learn towards setting up a multi-document agent over the LlamaIndex documentation.\n",
    "\n",
    "This is an extension of V0 multi-document agents with the additional features:\n",
    "- Reranking during document (tool) retrieval\n",
    "- Query planning tool that the agent can use to plan \n",
    "\n",
    "\n",
    "We do this with the following architecture:\n",
    "\n",
    "- setup a \"document agent\" over each Document: each doc agent can do QA/summarization within its doc\n",
    "- setup a top-level agent over this set of document agents. Do tool retrieval and then do CoT over the set of tools to answer a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0e47ac-ec6d-48eb-93a3-0e1fcab22112",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4680c94e-0bc3-469c-928e-9d26ed19af9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://127.0.0.1:6060/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x105cd82b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import phoenix as px\n",
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98830aff-8649-44d0-8ba3-e25f53ce518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index\n",
    "llama_index.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be00aba-b6c5-4940-9825-81c5d2cd2f0b",
   "metadata": {},
   "source": [
    "## Setup and Download Data\n",
    "\n",
    "In this section, we'll load in the LlamaIndex documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49893d69-c106-4169-92c3-6b5b751066e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"docs.llamaindex.ai\"\n",
    "docs_url = \"https://docs.llamaindex.ai/en/latest/\"\n",
    "!wget -e robots=off --recursive --no-clobber --page-requisites --html-extension --convert-links --restrict-file-names=windows --domains {domain} --no-parent {docs_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c661cb62-1e18-410c-bc2e-e707b66596a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_hub.file.unstructured.base import UnstructuredReader\n",
    "from pathlib import Path\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2452e8-061b-45b0-990d-36aa39ae02a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jerryliu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jerryliu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "reader = UnstructuredReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44feebd5-0430-4d73-9cb1-a3de73c1f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_gen = Path(\"./docs.llamaindex.ai/\").rglob(\"*\")\n",
    "all_files = [f.resolve() for f in all_files_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d837b4b-130c-493c-b62e-6662904c20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_html_files = [f for f in all_files if f.suffix.lower() == \".html\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cddf0f5-3c5f-4d42-868d-54bedb12d02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a1dd0cf-5da2-4ac0-bfd1-8f48921518c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx 0/418\n",
      "path: /Users/jerryliu/Programming/gpt_index/docs/examples/agent/docs.llamaindex.ai/en/latest/index.html\n",
      "\n",
      "Welcome to LlamaIndex ü¶ô !ÔÉÅ\n",
      "\n",
      "LlamaIndex (formerly GPT Index) is a data framework for LLM applica\n",
      "Idx 1/418\n",
      "path: /Users/jerryliu/Programming/gpt_index/docs/examples/agent/docs.llamaindex.ai/en/latest/deprecated_terms.html\n",
      "\n",
      "Deprecated TermsÔÉÅ\n",
      "\n",
      "As LlamaIndex continues to evolve, many class names and APIs have\n",
      "Idx 2/418\n",
      "path: /Users/jerryliu/Programming/gpt_index/docs/examples/agent/docs.llamaindex.ai/en/latest/genindex.html\n",
      "\n",
      "A |\n",
      "\n",
      "B |\n",
      "\n",
      "C |\n",
      "\n",
      "D |\n",
      "\n",
      "E |\n",
      "\n",
      "F |\n",
      "\n",
      "G |\n",
      "\n",
      "H |\n",
      "\n",
      "I |\n",
      "\n",
      "J |\n",
      "\n",
      "K |\n",
      "\n",
      "L |\n",
      "\n",
      "M |\n",
      "\n",
      "N |\n",
      "\n",
      "O |\n",
      "\n",
      "P |\n",
      "\n",
      "Q |\n",
      "\n",
      "R |\n",
      "\n",
      "S \n",
      "Idx 3/418\n",
      "path: /Users/jerryliu/Programming/gpt_index/docs/examples/agent/docs.llamaindex.ai/en/latest/search.html\n",
      "\n",
      "Please activate JavaScript to enable the search functionality.\n",
      "\n",
      "Copyright ¬© 2022, Jerry Liu Ma\n",
      "Idx 4/418\n",
      "path: /Users/jerryliu/Programming/gpt_index/docs/examples/agent/docs.llamaindex.ai/en/latest/development/documentation.html\n",
      "\n",
      "Documentation GuideÔÉÅ\n",
      "\n",
      "A guide for docs contributorsÔÉÅ\n",
      "\n",
      "The docs directory co\n",
      "Idx 5/418\n",
      "path: /Users/jerryliu/Programming/gpt_index/docs/examples/agent/docs.llamaindex.ai/en/latest/development/changelog.html\n",
      "\n",
      "ChangeLogÔÉÅ\n",
      "\n",
      "UnreleasedÔÉÅ\n",
      "\n",
      "New FeaturesÔÉÅ\n",
      "\n",
      "LocalAI more intuitive module-level var\n",
      "Idx 6/418\n",
      "path: /Users/jerryliu/Programming/gpt_index/docs/examples/agent/docs.llamaindex.ai/en/latest/development/contributing.html\n",
      "\n",
      "Contributing to LlamaIndexÔÉÅ\n",
      "\n",
      "Interested in contributing to LlamaIndex? Here‚Äô\n",
      "Idx 7/418\n",
      "path: /Users/jerryliu/Programming/gpt_index/docs/examples/agent/docs.llamaindex.ai/en/latest/development/privacy.html\n",
      "\n",
      "Privacy and SecurityÔÉÅ\n",
      "\n",
      "By default, LLamaIndex sends your data to OpenAI for gener\n",
      "Idx 8/418\n",
      "path: /Users/jerryliu/Programming/gpt_index/docs/examples/agent/docs.llamaindex.ai/en/latest/api_reference/evaluation.html\n",
      "\n",
      "EvaluationÔÉÅ\n",
      "\n",
      "We have modules for both LLM-based evaluation and retrieval-bas\n",
      "Idx 9/418\n",
      "path: /Users/jerryliu/Programming/gpt_index/docs/examples/agent/docs.llamaindex.ai/en/latest/api_reference/node.html\n",
      "\n",
      "NodeÔÉÅ\n",
      "\n",
      "Base schema for data structures.\n",
      "\n",
      "pydantic\n",
      "\n",
      "model\n",
      "\n",
      "llama_index.schema.\n",
      "\n",
      "Bas\n",
      "Idx 10/418\n",
      "path: /Users/jerryliu/Programming/gpt_index/docs/examples/agent/docs.llamaindex.ai/en/latest/api_reference/callbacks.html\n",
      "\n",
      "CallbacksÔÉÅ\n",
      "\n",
      "class\n",
      "\n",
      "llama_index.callbacks.\n",
      "\n",
      "AimCallback\n",
      "\n",
      "repo\n",
      "\n",
      "Optional[str]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "docs = []\n",
    "for idx, f in enumerate(all_html_files):\n",
    "    if idx > 10:\n",
    "        break\n",
    "    print(f\"Idx {idx}/{len(all_html_files)}\")\n",
    "    loaded_docs = reader.load_data(file=f, split_documents=True)\n",
    "    # Hardcoded Index. Everything before this is ToC\n",
    "    start_idx = 72\n",
    "    loaded_doc = Document(text=\"\\n\\n\".join([d.get_content() for d in loaded_docs[72:]]), metadata={\"path\": str(f)})\n",
    "    print(loaded_doc.get_content(metadata_mode=\"all\")[:200])\n",
    "    docs.append(loaded_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189aaf4-2eb7-40bc-9e83-79ce4f221b4b",
   "metadata": {},
   "source": [
    "Define LLM + Service Context + Callback Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd6e5e48-91b9-4701-a85d-d98c92323350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeef31a-fc25-4367-a5ba-945f81d04cf9",
   "metadata": {},
   "source": [
    "## Building Multi-Document Agents\n",
    "\n",
    "In this section we show you how to construct the multi-document agent. We first build a document agent for each document, and then define the top-level parent agent with an object index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9c3c2a9-c546-410d-9fbd-1a76f8da4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SummaryIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2a26f16-cfe9-4221-b169-57df91b197da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cd798-2e8d-474c-922a-51b12c5c6f36",
   "metadata": {},
   "source": [
    "### Build Document Agent for each Document\n",
    "\n",
    "In this section we define \"document agents\" for each document.\n",
    "\n",
    "We define both a vector index (for semantic search) and summary index (for summarization) for each document. The two query engines are then converted into tools that are passed to an OpenAI function calling agent.\n",
    "\n",
    "This document agent can dynamically choose to perform semantic search or summarization within a given document.\n",
    "\n",
    "We create a separate document agent for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eacdf3a7-cfe3-4c2b-9037-b28a065ed148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index import load_index_from_storage, StorageContext\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "async def build_agent_per_doc(nodes, file_base):\n",
    "    print(file_base)\n",
    "\n",
    "    vi_out_path = f\"./data/llamaindex_docs/{file_base}\"\n",
    "    summary_out_path = f\"./data/llamaindex_docs/{file_base}_summary.pkl\"\n",
    "    if not os.path.exists(vi_out_path):\n",
    "        Path(\"./data/llamaindex_docs/\").mkdir(parents=True, exist_ok=True)\n",
    "        # build vector index\n",
    "        vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "        vector_index.storage_context.persist(persist_dir=vi_out_path)\n",
    "    else:\n",
    "        vector_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=vi_out_path),\n",
    "            service_context=service_context,\n",
    "        )\n",
    "\n",
    "    # build summary index\n",
    "    summary_index = SummaryIndex(nodes, service_context=service_context)\n",
    "    \n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "    summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "\n",
    "    # extract a summary\n",
    "    if not os.path.exists(summary_out_path):\n",
    "        Path(summary_out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        summary = str(await summary_query_engine.aquery(\"Extract a concise 1-2 line summary of this document\"))\n",
    "        pickle.dump(summary, open(summary_out_path, 'wb'))\n",
    "    else:\n",
    "        summary = pickle.load(open(summary_out_path, 'rb'))\n",
    "\n",
    "    # define tools\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"vector_tool_{file_base}\",\n",
    "                description=f\"Useful for questions related to specific facts\",\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=summary_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"summary_tool_{file_base}\",\n",
    "                description=f\"Useful for summarization questions\",\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # build agent\n",
    "    function_llm = OpenAI(model=\"gpt-4\")\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "        system_prompt=f\"\"\"\\\n",
    "You are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.\n",
    "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    return agent, summary\n",
    "\n",
    "\n",
    "async def build_agents(docs):\n",
    "\n",
    "    node_parser = SimpleNodeParser.from_defaults()\n",
    "    \n",
    "    # Build agents dictionary\n",
    "    agents_dict = {}\n",
    "    extra_info_dict = {}\n",
    "    \n",
    "    # # this is for the baseline\n",
    "    # all_nodes = []\n",
    "    \n",
    "    for idx, doc in enumerate(tqdm(docs)):\n",
    "        nodes = node_parser.get_nodes_from_documents([doc])\n",
    "        # all_nodes.extend(nodes)\n",
    "\n",
    "        file_base = Path(doc.metadata[\"path\"]).stem\n",
    "        agent, summary = await build_agent_per_doc(nodes, file_base)\n",
    "    \n",
    "        agents_dict[file_base] = agent\n",
    "        extra_info_dict[file_base] = {\n",
    "            \"summary\": summary,\n",
    "            \"nodes\": nodes\n",
    "        }\n",
    "        \n",
    "    return agents_dict, extra_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44748b46-dd6b-4d4f-bc70-7022ae96413f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005494117736816406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 11,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872010244d694ee4ab96f1ad09cdbe3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "deprecated_terms\n",
      "genindex\n",
      "search\n",
      "documentation\n",
      "changelog\n",
      "contributing\n",
      "privacy\n",
      "evaluation\n",
      "node\n",
      "callbacks\n"
     ]
    }
   ],
   "source": [
    "agents_dict, extra_info_dict = await build_agents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ca55b-0c02-429b-a765-8e4f806d503f",
   "metadata": {},
   "source": [
    "### Build Retriever-Enabled OpenAI Agent\n",
    "\n",
    "We build a top-level agent that can orchestrate across the different document agents to answer any user query.\n",
    "\n",
    "This agent takes in all document agents as tools. This specific agent `RetrieverOpenAIAgent` performs tool retrieval before tool use (unlike a default agent that tries to put all tools in the prompt).\n",
    "\n",
    "Here we use a top-k retriever, but we encourage you to customize the tool retriever method!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6884ff15-bf40-4bdd-a1e3-58cbd056a12a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define tool for each document agent\n",
    "all_tools = []\n",
    "for file_base, agent in agents_dict.items():\n",
    "    summary = extra_info_dict[file_base][\"summary\"]\n",
    "    doc_tool = QueryEngineTool(\n",
    "        query_engine=agent,\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"tool_{file_base}\",\n",
    "            description=summary,\n",
    "        ),\n",
    "    )\n",
    "    all_tools.append(doc_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b266ad43-c3fd-41cb-9e3b-4cb2bb2c2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.objects import ObjectIndex, SimpleToolNodeMapping, ObjectRetriever\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from llama_index.indices.postprocessor import CohereRerank\n",
    "from llama_index.tools import QueryPlanTool\n",
    "\n",
    "tool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    tool_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "# vector_node_retriever = obj_index.as_node_retriever(similarity_top_k=10)\n",
    "vector_node_retriever = obj_index.as_node_retriever(similarity_top_k=2)\n",
    "\n",
    "\n",
    "# define a custom retriever with reranking\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, postprocessor = None):\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._postprocessor = postprocessor or CohereRerank(top_n=5)\n",
    "        \n",
    "    def _retrieve(self, query_bundle):\n",
    "        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        filtered_nodes = self._postprocessor.postprocess_nodes(\n",
    "            retrieved_nodes, query_bundle=query_bundle\n",
    "        )\n",
    "        \n",
    "        return filtered_nodes\n",
    "\n",
    "# # define a custom object retriever that adds in a query planning tool \n",
    "# class CustomObjectRetriever(ObjectRetriever):\n",
    "#     def __init__(self, retriever, object_node_mapping, all_tools):\n",
    "#         self._retriever = retriever\n",
    "#         self._object_node_mapping = object_node_mapping\n",
    "\n",
    "#         self._query_plan_tool = QueryPlanTool.from_defaults(\n",
    "#             query_engine_tools=all_tools,\n",
    "#             # include_tool_description=False,\n",
    "#             include_tool_description=True,\n",
    "#             name=\"tool_queryplan\",\n",
    "#             # description_prefix=\"ONLY use this tool for compare/contrast queries (NEVER use any other tool for that purpose)\"\n",
    "#             description_prefix=\"\"\"\\\n",
    "# This is a query plan tool. It takes in a query plan DAG \\\n",
    "# in the form of QueryNode objects (each containing tool_name, query_str, and dependencies).\n",
    "# It will then execute this query plan to answer the user question.\n",
    "\n",
    "# You should ALWAYS use this tool to answer any question over multiple documents (e.g. document comparisons).\\\n",
    "# \"\"\"\n",
    "#         )\n",
    "        \n",
    "#     def retrieve(self, query_bundle):\n",
    "#         nodes = self._retriever.retrieve(query_bundle)\n",
    "#         tools = [self._object_node_mapping.from_node(n.node) for n in nodes]\n",
    "#         return tools + [self._query_plan_tool]\n",
    "#         # return [self._query_plan_tool]\n",
    "\n",
    "\n",
    "# define a custom object retriever that adds in a query planning tool \n",
    "class CustomObjectRetriever(ObjectRetriever):\n",
    "    def __init__(self, retriever, object_node_mapping, all_tools):\n",
    "        self._retriever = retriever\n",
    "        self._object_node_mapping = object_node_mapping\n",
    "\n",
    "#         self._query_plan_tool = QueryPlanTool.from_defaults(\n",
    "#             query_engine_tools=all_tools,\n",
    "#             # include_tool_description=False,\n",
    "#             include_tool_description=True,\n",
    "#             name=\"tool_queryplan\",\n",
    "#             # description_prefix=\"ONLY use this tool for compare/contrast queries (NEVER use any other tool for that purpose)\"\n",
    "#             description_prefix=\"\"\"\\\n",
    "# This is a query plan tool. It takes in a query plan DAG \\\n",
    "# in the form of QueryNode objects (each containing tool_name, query_str, and dependencies).\n",
    "# It will then execute this query plan to answer the user question.\n",
    "\n",
    "# You should ALWAYS use this tool to answer any question over multiple documents (e.g. document comparisons).\\\n",
    "# \"\"\"\n",
    "#         )\n",
    "        \n",
    "    def retrieve(self, query_bundle):\n",
    "        nodes = self._retriever.retrieve(query_bundle)\n",
    "        tools = [self._object_node_mapping.from_node(n.node) for n in nodes]\n",
    "\n",
    "        query_plan_tool = QueryPlanTool.from_defaults(\n",
    "            query_engine_tools=tools,\n",
    "            # include_tool_description=False,\n",
    "            include_tool_description=True,\n",
    "            name=\"tool_queryplan\",\n",
    "            # description_prefix=\"ONLY use this tool for compare/contrast queries (NEVER use any other tool for that purpose)\"\n",
    "#             description_prefix=\"\"\"\\\n",
    "# This is a query plan tool. It takes in a query plan DAG \\\n",
    "# in the form of QueryNode objects (each containing tool_name, query_str, and dependencies).\n",
    "# It will then execute this query plan to answer the user question.\n",
    "\n",
    "# You should ALWAYS use this tool to answer any question over multiple documents (e.g. document comparisons).\\\n",
    "# \"\"\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # return tools + [self._query_plan_tool]\n",
    "        return [query_plan_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ba0d1a6-e324-4faa-b72b-d340904e65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_node_retriever = CustomRetriever(vector_node_retriever)\n",
    "\n",
    "# wrap it with ObjectRetriever to return objects\n",
    "custom_obj_retriever = CustomObjectRetriever(custom_node_retriever, tool_mapping, all_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8654ce2a-cce7-44fc-8445-8bbcfdf7ee91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'tool_queryplan',\n",
       " 'description': '            This is a query plan tool that takes in a list of tools and executes a query plan over these tools to answer a query. The query plan is a DAG of query nodes.\\n\\nGiven a list of tool names and the query plan schema, you can choose to generate a query plan to answer a question.\\n\\nThe tool names and descriptions are as follows:\\n\\n\\n\\n            Tool Name: tool_index\\nTool Description: LlamaIndex is a data framework that allows LLM applications to ingest, structure, and access private or domain-specific data by providing tools such as data connectors, data indexes, engines, data agents, and application integrations. It is designed for beginners, advanced users, and everyone in between, and offers both high-level and lower-level APIs for customization. \\n\\nTool Name: tool_search\\nTool Description: This document is a web page that provides information about enabling JavaScript for search functionality and credits the creators of Sphinx and Furo. \\n            ',\n",
       " 'parameters': {'title': 'QueryPlan',\n",
       "  'description': \"Query plan.\\n\\nContains a list of QueryNode objects (which is a recursive object).\\nOut of the list of QueryNode objects, one of them must be the root node.\\nThe root node is the one that isn't a dependency of any other node.\",\n",
       "  'type': 'object',\n",
       "  'properties': {'nodes': {'title': 'Nodes',\n",
       "    'description': 'The original question we are asking.',\n",
       "    'type': 'array',\n",
       "    'items': {'$ref': '#/definitions/QueryNode'}}},\n",
       "  'required': ['nodes'],\n",
       "  'definitions': {'QueryNode': {'title': 'QueryNode',\n",
       "    'description': 'Query node.\\n\\nA query node represents a query (query_str) that must be answered.\\nIt can either be answered by a tool (tool_name), or by a list of child nodes\\n(child_nodes).\\nThe tool_name and child_nodes fields are mutually exclusive.',\n",
       "    'type': 'object',\n",
       "    'properties': {'id': {'title': 'Id',\n",
       "      'description': 'ID of the query node.',\n",
       "      'type': 'integer'},\n",
       "     'query_str': {'title': 'Query Str',\n",
       "      'description': 'Question we are asking. This is the query string that will be executed. ',\n",
       "      'type': 'string'},\n",
       "     'tool_name': {'title': 'Tool Name',\n",
       "      'description': 'Name of the tool to execute the `query_str`.',\n",
       "      'type': 'string'},\n",
       "     'dependencies': {'title': 'Dependencies',\n",
       "      'description': 'List of sub-questions that need to be answered in order to answer the question given by `query_str`.Should be blank if there are no sub-questions to be specified, in which case `tool_name` is specified.',\n",
       "      'type': 'array',\n",
       "      'items': {'type': 'integer'}}},\n",
       "    'required': ['id', 'query_str']}}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmps = custom_obj_retriever.retrieve(\"hello\")\n",
    "display(tmps[0].metadata.to_openai_function())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fed38942-1e37-4c61-89fa-d2ef41151831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import FnRetrieverOpenAIAgent, ReActAgent\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-4-0613\")\n",
    "# top_agent = FnRetrieverOpenAIAgent.from_retriever(\n",
    "#     custom_obj_retriever,\n",
    "#     system_prompt=\"\"\" \\\n",
    "# You are an agent designed to answer queries about the documentation.\n",
    "# Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "# \"\"\",\n",
    "#     llm=llm,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# top_agent = ReActAgent.from_tools(\n",
    "#     custom_obj_retriever,\n",
    "#     system_prompt=\"\"\" \\\n",
    "# You are an agent designed to answer queries about the documentation.\n",
    "# Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "# \"\"\",\n",
    "#     llm=llm,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32b97c-6779-4b60-823d-6ca3be6f358a",
   "metadata": {},
   "source": [
    "### Define Baseline Vector Store Index\n",
    "\n",
    "As a point of comparison, we define a \"naive\" RAG pipeline which dumps all docs into a single vector index collection.\n",
    "\n",
    "We set the top_k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2f54834-1597-46ce-b0d3-0456bfa0d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = [n for extra_info in extra_info_dict.values() for n in extra_info[\"nodes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60dfc88f-6f47-4ef2-9ae6-74abde06a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = VectorStoreIndex(all_nodes)\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedb927-a992-4f21-a0fb-4ce4361adcb3",
   "metadata": {},
   "source": [
    "## Running Example Queries\n",
    "\n",
    "Let's run some example queries, ranging from QA / summaries over a single document to QA / summarization over multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e743c62-7dd8-4ac9-85a5-f1cbc112a79c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'tool_queryplan', 'description': '            This is a query plan tool that takes in a list of tools and executes a query plan over these tools to answer a query. The query plan is a DAG of query nodes.\\n\\nGiven a list of tool names and the query plan schema, you can choose to generate a query plan to answer a question.\\n\\nThe tool names and descriptions are as follows:\\n\\n\\n\\n            Tool Name: tool_evaluation\\nTool Description: This document provides information about evaluation modules and classes, dataset generation, evaluators and metrics, and the evaluation process for a retriever in the llama_index library. It also includes details about model construction, validation, and duplication, as well as methods for parsing, validating, and updating objects. \\n\\nTool Name: tool_index\\nTool Description: LlamaIndex is a data framework that allows LLM applications to ingest, structure, and access private or domain-specific data by providing tools such as data connectors, data indexes, engines, data agents, and application integrations. It is designed for beginners, advanced users, and everyone in between, and offers both high-level and lower-level APIs for customization. \\n            ', 'parameters': {'title': 'QueryPlan', 'description': \"Query plan.\\n\\nContains a list of QueryNode objects (which is a recursive object).\\nOut of the list of QueryNode objects, one of them must be the root node.\\nThe root node is the one that isn't a dependency of any other node.\", 'type': 'object', 'properties': {'nodes': {'title': 'Nodes', 'description': 'The original question we are asking.', 'type': 'array', 'items': {'$ref': '#/definitions/QueryNode'}}}, 'required': ['nodes'], 'definitions': {'QueryNode': {'title': 'QueryNode', 'description': 'Query node.\\n\\nA query node represents a query (query_str) that must be answered.\\nIt can either be answered by a tool (tool_name), or by a list of child nodes\\n(child_nodes).\\nThe tool_name and child_nodes fields are mutually exclusive.', 'type': 'object', 'properties': {'id': {'title': 'Id', 'description': 'ID of the query node.', 'type': 'integer'}, 'query_str': {'title': 'Query Str', 'description': 'Question we are asking. This is the query string that will be executed. ', 'type': 'string'}, 'tool_name': {'title': 'Tool Name', 'description': 'Name of the tool to execute the `query_str`.', 'type': 'string'}, 'dependencies': {'title': 'Dependencies', 'description': 'List of sub-questions that need to be answered in order to answer the question given by `query_str`.Should be blank if there are no sub-questions to be specified, in which case `tool_name` is specified.', 'type': 'array', 'items': {'type': 'integer'}}}, 'required': ['id', 'query_str']}}}}]\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "'            This is a query plan tool that takes in a list of tools and executes a query plan over these tools to answer a query. The query plan is a DAG of query nodes.\\n\\nGiven a list of tool names and the query plan schema, you can choose to generate a query plan to answer a question.\\n\\nThe tool names and descriptions are as follows:\\n\\n\\n\\n            Tool Name: tool_evaluation\\nTool Description: This document provides information about evaluation modules and classes, dataset generation, evaluators and metrics, and the evaluation process for a retriever in the llama_index library. It also includes details about model construction, validation, and duplication, as well as methods for parsing, validating, and updating objects. \\n\\nTool Name: tool_index\\nTool Description: LlamaIndex is a data framework that allows LLM applications to ingest, structure, and access private or domain-specific data by providing tools such as data connectors, data indexes, engines, data agents, and application integrations. It is designed for beginners, advanced users, and everyone in between, and offers both high-level and lower-level APIs for customization. \\n            ' is too long - 'functions.0.description'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# should use Boston agent -> vector tool\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mtop_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me about the different types of evaluation in LlamaIndex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/indices/query/base.py:23\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     22\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/callbacks/utils.py:39\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/types.py:18\u001b[0m, in \u001b[0;36mBaseAgent._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[0;32m---> 18\u001b[0m     agent_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Response(response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(agent_response))\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/callbacks/utils.py:39\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/openai_agent.py:343\u001b[0m, in \u001b[0;36mchat\u001b[0;34m(self, message, chat_history, function_call)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     function_call: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentChatResponse:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    341\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mAGENT_STEP,\n\u001b[1;32m    342\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mMESSAGES: [message]},\n\u001b[0;32m--> 343\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    344\u001b[0m         chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat(\n\u001b[1;32m    345\u001b[0m             message, chat_history, function_call, mode\u001b[38;5;241m=\u001b[39mChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT\n\u001b[1;32m    346\u001b[0m         )\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/openai_agent.py:290\u001b[0m, in \u001b[0;36mBaseOpenAIAgent._chat\u001b[0;34m(self, message, chat_history, function_call, mode)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     llm_chat_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_llm_chat_kwargs(functions, current_func)\n\u001b[0;32m--> 290\u001b[0m     agent_chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_agent_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mllm_chat_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_function_call, n_function_calls):\n\u001b[1;32m    292\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBreak: should continue False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/openai_agent.py:257\u001b[0m, in \u001b[0;36mBaseOpenAIAgent._get_agent_response\u001b[0;34m(self, mode, **llm_chat_kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_agent_response\u001b[39m(\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode: ChatResponseMode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_chat_kwargs: Any\n\u001b[1;32m    255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AGENT_CHAT_RESPONSE_TYPE:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT:\n\u001b[0;32m--> 257\u001b[0m         chat_response: ChatResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mllm_chat_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_message(chat_response)\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mSTREAM:\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/llms/base.py:151\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self) \u001b[38;5;28;01mas\u001b[39;00m callback_manager:\n\u001b[1;32m    143\u001b[0m     event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    144\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    145\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m         },\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 151\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/llms/openai.py:124\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/llms/openai.py:190\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: Sequence[ChatMessage], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponse:\n\u001b[1;32m    189\u001b[0m     message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 190\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_chat_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_all_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     message_dict \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    198\u001b[0m     message \u001b[38;5;241m=\u001b[39m from_openai_message_dict(message_dict)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/llms/openai_utils.py:139\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(is_chat_model, max_retries, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     client \u001b[38;5;241m=\u001b[39m get_completion_endpoint(is_chat_model)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/llms/openai_utils.py:137\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    136\u001b[0m     client \u001b[38;5;241m=\u001b[39m get_completion_endpoint(is_chat_model)\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: '            This is a query plan tool that takes in a list of tools and executes a query plan over these tools to answer a query. The query plan is a DAG of query nodes.\\n\\nGiven a list of tool names and the query plan schema, you can choose to generate a query plan to answer a question.\\n\\nThe tool names and descriptions are as follows:\\n\\n\\n\\n            Tool Name: tool_evaluation\\nTool Description: This document provides information about evaluation modules and classes, dataset generation, evaluators and metrics, and the evaluation process for a retriever in the llama_index library. It also includes details about model construction, validation, and duplication, as well as methods for parsing, validating, and updating objects. \\n\\nTool Name: tool_index\\nTool Description: LlamaIndex is a data framework that allows LLM applications to ingest, structure, and access private or domain-specific data by providing tools such as data connectors, data indexes, engines, data agents, and application integrations. It is designed for beginners, advanced users, and everyone in between, and offers both high-level and lower-level APIs for customization. \\n            ' is too long - 'functions.0.description'"
     ]
    }
   ],
   "source": [
    "# should use Boston agent -> vector tool\n",
    "response = top_agent.query(\"Tell me about the different types of evaluation in LlamaIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4ce2a76-5779-4acf-9337-69109dae7fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are two types of evaluation in LlamaIndex:\n",
      "\n",
      "1. LLM-based evaluation: This type of evaluation focuses on evaluating the performance of the LLM (Language Learning Model) in understanding and generating responses. It involves measuring metrics such as perplexity, BLEU score, and F1 score to assess the quality of the generated responses.\n",
      "\n",
      "2. Retrieval-based evaluation: This type of evaluation focuses on evaluating the performance of the retrieval system in retrieving relevant documents or responses. It involves measuring metrics such as precision, recall, and mean average precision (MAP) to assess the effectiveness of the retrieval system.\n",
      "\n",
      "Both types of evaluation are important in assessing the performance and effectiveness of the LlamaIndex system.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af28b422-fb73-4b59-9e77-3ba3afa87795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex utilizes various types of evaluation methods to assess its performance and effectiveness. These evaluation methods include RelevancyEvaluator, RetrieverEvaluator, SemanticSimilarityEvaluator, PairwiseComparisonEvaluator, CorrectnessEvaluator, FaithfulnessEvaluator, and GuidelineEvaluator. Each of these evaluators serves a specific purpose in evaluating different aspects of the LlamaIndex system.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "response = base_query_engine.query(\"Tell me about the different types of evaluation in LlamaIndex\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee6ef20c-3ccc-46c3-ad87-667138d78d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'tool_queryplan', 'description': '            This is a query plan tool. It takes in a query plan DAG in the form of QueryNode objects (each containing tool_name, query_str, and dependencies).\\nIt will then execute this query plan to answer the user question.\\n\\nYou should ALWAYS use this tool to answer any question over multiple documents (e.g. document comparisons).\\n\\n\\n            Tool Name: tool_index\\nTool Description: LlamaIndex is a data framework that allows LLM applications to ingest, structure, and access private or domain-specific data by providing tools such as data connectors, data indexes, engines, data agents, and application integrations. It is designed for beginners, advanced users, and everyone in between, and offers both high-level and lower-level APIs for customization. \\n\\nTool Name: tool_deprecated_terms\\nTool Description: This document provides a list of deprecated terms in LlamaIndex, including class names and APIs that have been adjusted, improved, or replaced. It also provides links to their replacements and additional details on usage. \\n\\nTool Name: tool_genindex\\nTool Description: This document provides a list of methods, classes, attributes, and properties related to the Llama Index system, including various modules and components such as vector stores, document readers, chat engines, and more. \\n\\nTool Name: tool_search\\nTool Description: This document is a web page that provides information about enabling JavaScript for search functionality and credits the creators of Sphinx and Furo. \\n\\nTool Name: tool_documentation\\nTool Description: This document provides a guide for contributors interested in running and making changes to the LlamaIndex documentation locally. It includes instructions on cloning the GitHub repo, installing dependencies, building the Sphinx docs, and using sphinx-autobuild for live-reloading during development. \\n\\nTool Name: tool_changelog\\nTool Description: This document provides a summary of the new features, bug fixes, and other changes in different versions of a software package or project. \\n\\nTool Name: tool_contributing\\nTool Description: This document provides guidelines for contributing to LlamaIndex, including extending core modules, fixing bugs, adding usage examples, adding experimental features, and improving code quality and documentation. It also includes information on the environment setup, validating changes, formatting and linting, testing, creating example notebooks, and creating a pull request. \\n\\nTool Name: tool_privacy\\nTool Description: This document provides information about the privacy and security features of LLamaIndex, including the option to configure data handling preferences, use custom embedding models, and connect with other vector stores. \\n\\nTool Name: tool_evaluation\\nTool Description: This document provides information about evaluation modules and classes, dataset generation, evaluators and metrics, and the evaluation process for a retriever in the llama_index library. It also includes details about model construction, validation, and duplication, as well as methods for parsing, validating, and updating objects. \\n\\nTool Name: tool_node\\nTool Description: This document provides information about various classes, methods, and attributes related to node objects, including base components, base nodes, documents, image documents, metadata modes, node relationships, and node with scores. \\n\\nTool Name: tool_callbacks\\nTool Description: This document provides information about the callback handlers and event handling in the LlamaIndex library. \\n            ', 'parameters': {'title': 'QueryPlan', 'description': \"Query plan.\\n\\nContains a list of QueryNode objects (which is a recursive object).\\nOut of the list of QueryNode objects, one of them must be the root node.\\nThe root node is the one that isn't a dependency of any other node.\", 'type': 'object', 'properties': {'nodes': {'title': 'Nodes', 'description': 'The original question we are asking.', 'type': 'array', 'items': {'$ref': '#/definitions/QueryNode'}}}, 'required': ['nodes'], 'definitions': {'QueryNode': {'title': 'QueryNode', 'description': 'Query node.\\n\\nA query node represents a query (query_str) that must be answered.\\nIt can either be answered by a tool (tool_name), or by a list of child nodes\\n(child_nodes).\\nThe tool_name and child_nodes fields are mutually exclusive.', 'type': 'object', 'properties': {'id': {'title': 'Id', 'description': 'ID of the query node.', 'type': 'integer'}, 'query_str': {'title': 'Query Str', 'description': 'Question we are asking. This is the query string that will be executed. ', 'type': 'string'}, 'tool_name': {'title': 'Tool Name', 'description': 'Name of the tool to execute the `query_str`.', 'type': 'string'}, 'dependencies': {'title': 'Dependencies', 'description': 'List of sub-questions that need to be answered in order to answer the question given by `query_str`.Should be blank if there are no sub-questions to be specified, in which case `tool_name` is specified.', 'type': 'array', 'items': {'type': 'integer'}}}, 'required': ['id', 'query_str']}}}}]\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "'            This is a query plan tool. It takes in a query plan DAG in the form of QueryNode objects (each containing tool_name, query_str, and dependencies).\\nIt will then execute this query plan to answer the user question.\\n\\nYou should ALWAYS use this tool to answer any question over multiple documents (e.g. document comparisons).\\n\\n\\n            Tool Name: tool_index\\nTool Description: LlamaIndex is a data framework that allows LLM applications to ingest, structure, and access private or domain-specific data by providing tools such as data connectors, data indexes, engines, data agents, and application integrations. It is designed for beginners, advanced users, and everyone in between, and offers both high-level and lower-level APIs for customization. \\n\\nTool Name: tool_deprecated_terms\\nTool Description: This document provides a list of deprecated terms in LlamaIndex, including class names and APIs that have been adjusted, improved, or replaced. It also provides links to their replacements and additional details on usage. \\n\\nTool Name: tool_genindex\\nTool Description: This document provides a list of methods, classes, attributes, and properties related to the Llama Index system, including various modules and components such as vector stores, document readers, chat engines, and more. \\n\\nTool Name: tool_search\\nTool Description: This document is a web page that provides information about enabling JavaScript for search functionality and credits the creators of Sphinx and Furo. \\n\\nTool Name: tool_documentation\\nTool Description: This document provides a guide for contributors interested in running and making changes to the LlamaIndex documentation locally. It includes instructions on cloning the GitHub repo, installing dependencies, building the Sphinx docs, and using sphinx-autobuild for live-reloading during development. \\n\\nTool Name: tool_changelog\\nTool Description: This document provides a summary of the new features, bug fixes, and other changes in different versions of a software package or project. \\n\\nTool Name: tool_contributing\\nTool Description: This document provides guidelines for contributing to LlamaIndex, including extending core modules, fixing bugs, adding usage examples, adding experimental features, and improving code quality and documentation. It also includes information on the environment setup, validating changes, formatting and linting, testing, creating example notebooks, and creating a pull request. \\n\\nTool Name: tool_privacy\\nTool Description: This document provides information about the privacy and security features of LLamaIndex, including the option to configure data handling preferences, use custom embedding models, and connect with other vector stores. \\n\\nTool Name: tool_evaluation\\nTool Description: This document provides information about evaluation modules and classes, dataset generation, evaluators and metrics, and the evaluation process for a retriever in the llama_index library. It also includes details about model construction, validation, and duplication, as well as methods for parsing, validating, and updating objects. \\n\\nTool Name: tool_node\\nTool Description: This document provides information about various classes, methods, and attributes related to node objects, including base components, base nodes, documents, image documents, metadata modes, node relationships, and node with scores. \\n\\nTool Name: tool_callbacks\\nTool Description: This document provides information about the callback handlers and event handling in the LlamaIndex library. \\n            ' is too long - 'functions.0.description'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mtop_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCompare the contributions page vs. the home/index page\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/indices/query/base.py:23\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     22\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/callbacks/utils.py:39\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/types.py:18\u001b[0m, in \u001b[0;36mBaseAgent._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[0;32m---> 18\u001b[0m     agent_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Response(response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(agent_response))\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/callbacks/utils.py:39\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/openai_agent.py:343\u001b[0m, in \u001b[0;36mchat\u001b[0;34m(self, message, chat_history, function_call)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     function_call: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentChatResponse:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    341\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mAGENT_STEP,\n\u001b[1;32m    342\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mMESSAGES: [message]},\n\u001b[0;32m--> 343\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    344\u001b[0m         chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat(\n\u001b[1;32m    345\u001b[0m             message, chat_history, function_call, mode\u001b[38;5;241m=\u001b[39mChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT\n\u001b[1;32m    346\u001b[0m         )\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/openai_agent.py:290\u001b[0m, in \u001b[0;36mBaseOpenAIAgent._chat\u001b[0;34m(self, message, chat_history, function_call, mode)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     llm_chat_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_llm_chat_kwargs(functions, current_func)\n\u001b[0;32m--> 290\u001b[0m     agent_chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_agent_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mllm_chat_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_function_call, n_function_calls):\n\u001b[1;32m    292\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBreak: should continue False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/agent/openai_agent.py:257\u001b[0m, in \u001b[0;36mBaseOpenAIAgent._get_agent_response\u001b[0;34m(self, mode, **llm_chat_kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_agent_response\u001b[39m(\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode: ChatResponseMode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_chat_kwargs: Any\n\u001b[1;32m    255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AGENT_CHAT_RESPONSE_TYPE:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT:\n\u001b[0;32m--> 257\u001b[0m         chat_response: ChatResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mllm_chat_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_message(chat_response)\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mSTREAM:\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/llms/base.py:151\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self) \u001b[38;5;28;01mas\u001b[39;00m callback_manager:\n\u001b[1;32m    143\u001b[0m     event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    144\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    145\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m         },\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 151\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/llms/openai.py:124\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/llms/openai.py:190\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: Sequence[ChatMessage], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponse:\n\u001b[1;32m    189\u001b[0m     message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 190\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_chat_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_all_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     message_dict \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    198\u001b[0m     message \u001b[38;5;241m=\u001b[39m from_openai_message_dict(message_dict)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/llms/openai_utils.py:139\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(is_chat_model, max_retries, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     client \u001b[38;5;241m=\u001b[39m get_completion_endpoint(is_chat_model)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama_index/llms/openai_utils.py:137\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    136\u001b[0m     client \u001b[38;5;241m=\u001b[39m get_completion_endpoint(is_chat_model)\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: '            This is a query plan tool. It takes in a query plan DAG in the form of QueryNode objects (each containing tool_name, query_str, and dependencies).\\nIt will then execute this query plan to answer the user question.\\n\\nYou should ALWAYS use this tool to answer any question over multiple documents (e.g. document comparisons).\\n\\n\\n            Tool Name: tool_index\\nTool Description: LlamaIndex is a data framework that allows LLM applications to ingest, structure, and access private or domain-specific data by providing tools such as data connectors, data indexes, engines, data agents, and application integrations. It is designed for beginners, advanced users, and everyone in between, and offers both high-level and lower-level APIs for customization. \\n\\nTool Name: tool_deprecated_terms\\nTool Description: This document provides a list of deprecated terms in LlamaIndex, including class names and APIs that have been adjusted, improved, or replaced. It also provides links to their replacements and additional details on usage. \\n\\nTool Name: tool_genindex\\nTool Description: This document provides a list of methods, classes, attributes, and properties related to the Llama Index system, including various modules and components such as vector stores, document readers, chat engines, and more. \\n\\nTool Name: tool_search\\nTool Description: This document is a web page that provides information about enabling JavaScript for search functionality and credits the creators of Sphinx and Furo. \\n\\nTool Name: tool_documentation\\nTool Description: This document provides a guide for contributors interested in running and making changes to the LlamaIndex documentation locally. It includes instructions on cloning the GitHub repo, installing dependencies, building the Sphinx docs, and using sphinx-autobuild for live-reloading during development. \\n\\nTool Name: tool_changelog\\nTool Description: This document provides a summary of the new features, bug fixes, and other changes in different versions of a software package or project. \\n\\nTool Name: tool_contributing\\nTool Description: This document provides guidelines for contributing to LlamaIndex, including extending core modules, fixing bugs, adding usage examples, adding experimental features, and improving code quality and documentation. It also includes information on the environment setup, validating changes, formatting and linting, testing, creating example notebooks, and creating a pull request. \\n\\nTool Name: tool_privacy\\nTool Description: This document provides information about the privacy and security features of LLamaIndex, including the option to configure data handling preferences, use custom embedding models, and connect with other vector stores. \\n\\nTool Name: tool_evaluation\\nTool Description: This document provides information about evaluation modules and classes, dataset generation, evaluators and metrics, and the evaluation process for a retriever in the llama_index library. It also includes details about model construction, validation, and duplication, as well as methods for parsing, validating, and updating objects. \\n\\nTool Name: tool_node\\nTool Description: This document provides information about various classes, methods, and attributes related to node objects, including base components, base nodes, documents, image documents, metadata modes, node relationships, and node with scores. \\n\\nTool Name: tool_callbacks\\nTool Description: This document provides information about the callback handlers and event handling in the LlamaIndex library. \\n            ' is too long - 'functions.0.description'"
     ]
    }
   ],
   "source": [
    "response = top_agent.query(\n",
    "    \"Compare the contributions page vs. the home/index page\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfe1dd4c-8bfd-43d0-99bc-ca60861dc418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Houston has a diverse population with a demographic makeup that includes non-Hispanic whites (23.3%), Hispanics and Latino Americans (45.8%), Blacks or African Americans (22.4%), and Asian Americans (6.5%). The largest Hispanic or Latino American ethnic group in Houston is Mexican Americans. Houston is also home to the largest African American community west of the Mississippi River and has a growing Muslim population.\n",
      "\n",
      "On the other hand, Chicago is also known for its diverse demographics. The city has a significant non-Hispanic White population, along with a substantial Black population and Hispanic population. Chicago is celebrated for its cultural diversity and has a significant LGBT population.\n",
      "\n",
      "Both Houston and Chicago have diverse populations, with a mix of different racial and ethnic groups contributing to their vibrant communities.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "843c79cb-fc7a-4eac-ba7f-6d25c7a8f08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Houston is the most populous city in Texas and the fourth-most populous city in the United States. It has a population of 2,304,580 as of the 2020 U.S. census. The city is known for its diversity, with a significant proportion of minorities. In 2019, non-Hispanic whites made up 23.3% of the population, Hispanics and Latino Americans 45.8%, Blacks or African Americans 22.4%, and Asian Americans 6.5%. The largest Hispanic or Latino American ethnic group in Houston is Mexican Americans, comprising 31.6% of the population.\n",
      "\n",
      "In comparison, Chicago is the third-most populous city in the United States. According to the 2020 U.S. census, Chicago has a population of 2,746,388. The demographics of Chicago are different from Houston, with non-Hispanic whites making up 32.7% of the population, Hispanics and Latino Americans 29.9%, Blacks or African Americans 29.8%, and Asian Americans 7.6%. The largest Hispanic or Latino American ethnic group in Chicago is Mexican Americans, comprising 21.6% of the population.\n",
      "\n",
      "Overall, both Houston and Chicago have diverse populations, but the specific demographic composition differs between the two cities.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "response = base_query_engine.query(\n",
    "    \"Tell the demographics of Houston, and then compare that with the demographics of Chicago\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f99183-bdd9-4d49-8980-44919cf86c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline: the response tells you nothing about Chicago...\n",
    "response.source_nodes[3].get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5b7e0a4c-b7be-4797-9c82-1577693bd117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: tool_Shanghai with args: {\n",
      "  \"input\": \"history\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"history\"\n",
      "}\n",
      "Got output: Shanghai has a rich history that dates back to ancient times. However, in the context provided, the history of Shanghai is mainly discussed in relation to its modern development. After the war, Shanghai's economy experienced significant growth, with increased agricultural and industrial output. The city's administrative divisions were rearranged, and it became a center for radical leftism during the 1950s and 1960s. The Cultural Revolution had a severe impact on Shanghai's society, but the city maintained economic production with a positive growth rate. Shanghai also played a significant role in China's Third Front campaign and has been a major contributor of tax revenue to the central government. Economic reforms were initiated in Shanghai in 1990, leading to the development of the Pudong district and its classification as an Alpha+ city.\n",
      "========================\n",
      "Got output: Shanghai's history is rich and complex, dating back to ancient times. However, its modern development is particularly noteworthy. After the war, Shanghai experienced significant economic growth, with a boost in both agricultural and industrial output. The city's administrative divisions were restructured, and it became a hub for radical leftism during the 1950s and 1960s.\n",
      "\n",
      "The Cultural Revolution had a profound impact on Shanghai's society, but despite this, the city managed to maintain economic production with a positive growth rate. Shanghai also played a significant role in China's Third Front campaign and has been a major contributor of tax revenue to the central government.\n",
      "\n",
      "In 1990, economic reforms were initiated in Shanghai, leading to the development of the Pudong district. This has helped Shanghai to be classified as an Alpha+ city, indicating its influence on the global economic stage.\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: tool_Beijing with args: {\n",
      "  \"input\": \"history\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"history\"\n",
      "}\n",
      "Got output: Beijing has a rich history that spans several dynasties. It was the capital of the Ming dynasty, during which the city took its current shape and many of its major attractions, such as the Forbidden City and the Temple of Heaven, were constructed. The Qing dynasty succeeded the Ming dynasty and made Beijing its sole capital. During this time, the Imperial residence and the general layout of the city remained largely unchanged. However, the city faced challenges during the Second Opium War and the Boxer Rebellion, resulting in the looting and destruction of important structures. In the early 20th century, Beijing saw the signing of a peace agreement between the Eight-Nation Alliance and the Chinese government, which led to the restoration of Qing dynasty rule. However, the dynasty eventually collapsed in 1911.\n",
      "========================\n",
      "Got output: Beijing has a rich and complex history that spans several dynasties. It served as the capital during the Ming dynasty, during which the city took its current shape and many of its major attractions, such as the Forbidden City and the Temple of Heaven, were constructed. The Qing dynasty succeeded the Ming dynasty and made Beijing its sole capital. During this time, the Imperial residence and the general layout of the city remained largely unchanged.\n",
      "\n",
      "However, the city faced significant challenges during the Second Opium War and the Boxer Rebellion, which resulted in the looting and destruction of important structures. In the early 20th century, Beijing saw the signing of a peace agreement between the Eight-Nation Alliance and the Chinese government, leading to the restoration of Qing dynasty rule. However, the dynasty eventually collapsed in 1911. Despite these tumultuous events, Beijing has managed to preserve its historical heritage while also evolving into a modern metropolis.\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: tool_Shanghai with args: {\n",
      "  \"input\": \"current economy\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"current economy\"\n",
      "}\n",
      "Got output: The current economy of Shanghai is strong and thriving. It is a global center for finance and innovation, and a national center for commerce, trade, and transportation. The city has a diverse economy, with its six largest industries comprising about half of its GDP. Shanghai has experienced rapid development and has been one of the fastest-developing cities in the world. It has recorded double-digit GDP growth in almost every year between 1992 and 2008. As of 2021, Shanghai had a GDP of CN¬•4.46 trillion ($1.106 trillion in PPP), making it one of the wealthiest cities in China. It is also the most expensive city in mainland China to live in. Shanghai is a major player in the global financial industry, ranking first in Asia and third globally in the Global Financial Centres Index. It is home to the Shanghai Stock Exchange, the largest stock exchange in China and the fourth-largest in the world. The city has attracted significant foreign investment and has been a hub for the technology industry and startups. Overall, the current economy of Shanghai is robust and continues to grow.\n",
      "========================\n",
      "Got output: The current economy of Shanghai is robust and thriving. It is a global center for finance and innovation, and a national center for commerce, trade, and transportation. The city has a diverse economy, with its six largest industries comprising about half of its GDP. \n",
      "\n",
      "Shanghai has experienced rapid development and has been one of the fastest-developing cities in the world. It has recorded double-digit GDP growth in almost every year between 1992 and 2008. As of 2021, Shanghai had a GDP of CN¬•4.46 trillion ($1.106 trillion in PPP), making it one of the wealthiest cities in China. \n",
      "\n",
      "Shanghai is also the most expensive city in mainland China to live in. It is a major player in the global financial industry, ranking first in Asia and third globally in the Global Financial Centres Index. The city is home to the Shanghai Stock Exchange, the largest stock exchange in China and the fourth-largest in the world. \n",
      "\n",
      "The city has attracted significant foreign investment and has been a hub for the technology industry and startups. Overall, the current economy of Shanghai is robust and continues to grow.\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: tool_Beijing with args: {\n",
      "  \"input\": \"current economy\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"current economy\"\n",
      "}\n",
      "Got output: The current economy of Beijing is dominated by the tertiary sector, which includes services such as professional services, wholesale and retail, information technology, commercial real estate, scientific research, and residential real estate. This sector generated 83.8% of the city's output in 2022. The secondary sector, which includes manufacturing and construction, accounted for 15.8% of output, while the primary sector, which includes agriculture and mining, contributed only 0.26%. The city has also identified six high-end economic output zones that are driving local economic growth, including Zhongguancun, Beijing Financial Street, Beijing Central Business District (CBD), Beijing Economic and Technological Development Area (Yizhuang), Beijing Airport Economic Zone, and Beijing Olympic Center Zone. These zones are home to various industries and sectors, such as technology companies, financial institutions, office buildings, industrial parks, and entertainment and sports centers.\n",
      "========================\n",
      "Got output: The current economy of Beijing is primarily driven by the tertiary sector, which includes services such as professional services, wholesale and retail, information technology, commercial real estate, scientific research, and residential real estate. This sector generated 83.8% of the city's output in 2022. The secondary sector, which includes manufacturing and construction, accounted for 15.8% of output, while the primary sector, which includes agriculture and mining, contributed only 0.26%.\n",
      "\n",
      "Beijing has also identified six high-end economic output zones that are driving local economic growth. These include Zhongguancun, Beijing Financial Street, Beijing Central Business District (CBD), Beijing Economic and Technological Development Area (Yizhuang), Beijing Airport Economic Zone, and Beijing Olympic Center Zone. These zones are home to various industries and sectors, such as technology companies, financial institutions, office buildings, industrial parks, and entertainment and sports centers.\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "response = top_agent.query(\n",
    "    \"Tell me the differences between Shanghai and Beijing in terms of history and current economy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "98540ec5-093d-41a0-888b-246ee7093cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In terms of history, both Shanghai and Beijing have rich and complex pasts. Shanghai's history dates back to ancient times, but its modern development is particularly noteworthy. It experienced significant economic growth after the war and played a major role in China's economic reforms. Beijing, on the other hand, has a history that spans several dynasties and served as the capital during the Ming and Qing dynasties. It has preserved its historical heritage while evolving into a modern metropolis.\n",
      "\n",
      "In terms of current economy, Shanghai is a global center for finance and innovation. It has a diverse economy and has experienced rapid development, with a high GDP and significant foreign investment. It is a major player in the global financial industry and is home to the Shanghai Stock Exchange. Beijing's economy is primarily driven by the tertiary sector, with a focus on services such as professional services, information technology, and commercial real estate. It has identified high-end economic output zones that are driving local economic growth.\n",
      "\n",
      "Overall, both cities have thriving economies, but Shanghai has a stronger focus on finance and global influence, while Beijing has a diverse economy with a focus on services and high-end economic zones.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0f49e28-e85e-4c93-8eed-68122ad9f537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shanghai and Beijing have distinct differences in terms of history and current economy. Historically, Shanghai was the largest and most prosperous city in East Asia during the 1930s, while Beijing served as the capital of the Republic of China and later the People's Republic of China. Shanghai experienced significant growth and redevelopment in the 1990s, while Beijing expanded its urban area and underwent rapid development in the last two decades.\n",
      "\n",
      "In terms of the current economy, Shanghai is considered the \"showpiece\" of China's booming economy. It is a global center for finance and innovation, with a strong focus on industries such as retail, finance, IT, real estate, machine manufacturing, and automotive manufacturing. Shanghai is also home to the world's busiest container port, the Port of Shanghai. The city has a high GDP and is classified as an Alpha+ city by the Globalization and World Cities Research Network.\n",
      "\n",
      "On the other hand, Beijing is a global financial center and ranks third globally in the Global Financial Centres Index. It is also a hub for the Chinese and global technology industry, with a large startup ecosystem. Beijing has a strong presence in industries such as finance, technology, and pharmaceuticals. The city is home to the headquarters of large state banks and insurance companies, as well as the country's financial regulatory agencies.\n",
      "\n",
      "Overall, while both Shanghai and Beijing are important economic centers in China, Shanghai has a stronger focus on industries such as finance, retail, and manufacturing, while Beijing has a strong presence in finance, technology, and pharmaceuticals.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "response = base_query_engine.query(\n",
    "    \"Tell me the differences between Shanghai and Beijing in terms of history and current economy\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d97266-8e22-43a8-adfe-b9a7f833c06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
