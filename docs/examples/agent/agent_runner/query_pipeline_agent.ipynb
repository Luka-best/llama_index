{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7973eef-7e5e-4ca4-844a-1d016f5a638b",
   "metadata": {},
   "source": [
    "# Building an Agent around a Query Pipeline\n",
    "\n",
    "In this cookbook we show you how to build an agent around a query pipeline.\n",
    "\n",
    "Agents offer the ability to do complex, sequential reasoning on top of any query DAG that you have setup. Conceptually this is also one of the ways you can add a \"loop\" to the graph.\n",
    "\n",
    "Here we show you how to add an agent on top of a query pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122dce22-6d3a-4d4a-a265-a6a3d3f90d26",
   "metadata": {},
   "source": [
    "## Setup Data\n",
    "\n",
    "We use the chinook database as sample data. [Source](https://www.sqlitetutorial.net/sqlite-sample-database/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "926b79ba-1868-46ca-bb7e-2bd3c907773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  298k  100  298k    0     0  3756k      0 --:--:-- --:--:-- --:--:-- 3926k\n",
      "curl: (6) Could not resolve host: .\n",
      "Archive:  ./chinook.zip\n",
      "  inflating: chinook.db              \n"
     ]
    }
   ],
   "source": [
    "!curl \"https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip\" -O ./chinook.zip\n",
    "!unzip ./chinook.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763a442a-cfcd-4e63-9121-e3a45dc3acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SQLDatabase\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column\n",
    "\n",
    "engine = create_engine(\"sqlite:///chinook.db\")\n",
    "sql_database = SQLDatabase(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aacdcad-f0c1-40f4-b319-6c4cf3b309c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline import QueryPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc38d8-2a78-4457-888d-cfe8960a9c6b",
   "metadata": {},
   "source": [
    "## Setup Text-to-SQL Query Engine / Tool\n",
    "\n",
    "Now we setup a simple text-to-SQL tool: given a query, translate text to SQL, execute against database, and get back a result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d72eb79e-05b4-4260-b086-e837b01cce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import NLSQLTableQueryEngine\n",
    "from llama_index.tools.query_engine import QueryEngineTool\n",
    "\n",
    "sql_query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database, \n",
    "    tables=[\"albums\", \"tracks\", \"artists\"], \n",
    "    verbose=True\n",
    ")\n",
    "sql_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=sql_query_engine,\n",
    "    name=\"sql_tool\",\n",
    "    description=(\n",
    "        \"Useful for translating a natural language query into a SQL query\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a024201f-a39b-4e23-a567-9737026dd771",
   "metadata": {},
   "source": [
    "### Define I/O Agent Components\n",
    "\n",
    "To setup a query pipeline in an agent setting, we define two components:\n",
    "- An `AgentInputComponent` that allows you to convert the agent inputs (Task, state dictionary) into a set of inputs for the query pipeline.\n",
    "- An `AgentFnComponent` as an output processor: a general processor that allows you to take in the current Task, state, as well as any arbitrary inputs, and returns an output. In this cookbook we put this at the end to process the output and return a response. However, you can put this anywhere, wherever you need access to the Task and state.\n",
    "\n",
    "Note that any function passed into `AgentFnComponent` and `AgentInputComponent` MUST include `task` and `state` as input variables, as these are inputs passed from the agent. \n",
    "The `AgentFnComponent` can also contain an arbitrary number of extra arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b1a9252-57df-47cc-9fcf-84d87b1e0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.react.types import (\n",
    "    ActionReasoningStep,\n",
    "    ObservationReasoningStep,\n",
    "    ResponseReasoningStep,\n",
    ")\n",
    "from llama_index.agent import Task, AgentChatResponse\n",
    "from llama_index.query_pipeline import (\n",
    "    AgentInputComponent,\n",
    "    AgentFnComponent\n",
    ")\n",
    "from llama_index.llms import MessageRole\n",
    "from typing import Dict, Any, Optional, Tuple, List, cast\n",
    "\n",
    "## Agent Input Component\n",
    "## This is the component that produces agent inputs to the rest of the components\n",
    "## Can also put initialization logic here.\n",
    "def agent_input_fn(task: Task, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Agent input function.\n",
    "\n",
    "    Returns:\n",
    "        A Dictionary of output keys and values. If you are specifying\n",
    "        src_key when defining links between this component and other\n",
    "        components, make sure the src_key matches the specified output_key.\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize current_reasoning\n",
    "    if \"current_reasoning\" not in state:\n",
    "        state[\"current_reasoning\"] = []\n",
    "    reasoning_step = ObservationReasoningStep(observation=task.input)\n",
    "    state[\"current_reasoning\"].append(reasoning_step)\n",
    "    return {\"input\": task.input}\n",
    "    \n",
    "agent_input_component = AgentInputComponent(fn=agent_input_fn)\n",
    "\n",
    "\n",
    "## Agent Output Component\n",
    "## Process reasoning step/tool outputs, and return agent response\n",
    "def finalize_fn(\n",
    "    task: Task, \n",
    "    state: Dict[str, Any], \n",
    "    reasoning_step: Any, \n",
    "    is_done: bool = False,\n",
    "    tool_output: Optional[Any] = None\n",
    ") -> Tuple[AgentChatResponse, bool]:\n",
    "    \"\"\"Finalize function.\n",
    "\n",
    "    Here we take the latest reasoning step, and a tool output (if provided),\n",
    "    and return the agent output (and decide if agent is done).\n",
    "    \n",
    "    \"\"\"\n",
    "    current_reasoning = state[\"current_reasoning\"]\n",
    "    current_reasoning.append(reasoning_step)\n",
    "    # if tool_output is not None, add to current reasoning\n",
    "    if tool_output is not None:\n",
    "        observation_step = ObservationReasoningStep(observation=str(tool_output))\n",
    "        current_reasoning.append(observation_step)\n",
    "    if isinstance(current_reasoning[-1], ResponseReasoningStep):\n",
    "        response_step = cast(ResponseReasoningStep, current_reasoning[-1])\n",
    "        response_str = response_step.response\n",
    "    else:\n",
    "        response_str = current_reasoning[-1].get_content()\n",
    "\n",
    "    # if is_done, add to memory\n",
    "    # NOTE: memory is a reserved keyword in `state`, but you can add your own too\n",
    "    if is_done:\n",
    "        memory = state[\"memory\"]\n",
    "        memory.put(ChatMessage(content=task.input, role=MessageRole.USER))\n",
    "        memory.put(ChatMessage(content=response_str, role=MessageRole.ASSISTANT))\n",
    "\n",
    "    return AgentChatResponse(response=response_str), is_done\n",
    "\n",
    "\n",
    "agent_output_component = AgentFnComponent(fn=finalize_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd914690-ee4d-4b3b-bb95-9cce0994b9c1",
   "metadata": {},
   "source": [
    "### Define Agent Prompt\n",
    "\n",
    "Here we define the agent component that generates a ReAct prompt, and after the output is generated from the LLM, parses into a structured object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a1957da-bc2b-4186-abf0-11148a23dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.react.formatter import ReActChatFormatter\n",
    "from llama_index.query_pipeline import InputComponent, Link\n",
    "from llama_index.llms import ChatMessage\n",
    "from llama_index.tools import BaseTool\n",
    "\n",
    "## define prompt function\n",
    "def react_prompt_fn(\n",
    "    task: Task,\n",
    "    state: Dict[str, Any],\n",
    "    input: str,\n",
    "    tools: List[BaseTool]\n",
    ") -> List[ChatMessage]:\n",
    "    # Add input to reasoning\n",
    "    chat_formatter = ReActChatFormatter()\n",
    "    input_chat = chat_formatter.format(\n",
    "        tools,\n",
    "        chat_history=task.memory.get() + state[\"memory\"].get_all(),\n",
    "        current_reasoning=state[\"current_reasoning\"]\n",
    "    )\n",
    "    print(f\"CURRENT CHAT: {state['current_reasoning']}\")\n",
    "    return input_chat\n",
    "\n",
    "react_prompt_component = AgentFnComponent(fn=react_prompt_fn, partial_dict={\"tools\": [sql_tool]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd7088-7804-48c5-8c7f-0c5c112d27f0",
   "metadata": {},
   "source": [
    "### Define Agent Output Parser + Tool Pipeline\n",
    "\n",
    "Once the LLM gives an output, we have a decision tree:\n",
    "1. If an answer is given, then we're done. Process the output\n",
    "2. If an action is given, we need to execute the specified tool with the specified args, and then process the output.\n",
    "\n",
    "Tool calling can be done via the `ToolRunnerComponent` module. This is a standalone module that takes in a list of tools, and can be \"executed\" with the specified tool name (every tool has a name) and tool action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d68a12f7-73cd-4614-8238-a1b4de58abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline import ToolRunnerComponent, IfElseComponent\n",
    "from llama_index.llms import ChatMessage, ChatResponse\n",
    "from llama_index.agent.react.output_parser import ReActOutputParser\n",
    "\n",
    "## define tool pipeline\n",
    "## Tool Runner Component\n",
    "tool_runner_component = ToolRunnerComponent([sql_tool])\n",
    "tool_qp = QueryPipeline(modules={\n",
    "    \"input\": InputComponent(),\n",
    "    \"tool_runner\": tool_runner_component,\n",
    "    \"agent_output\": agent_output_component\n",
    "}, verbose=True)\n",
    "tool_qp.add_links([\n",
    "    Link(\"input\", \"tool_runner\", src_key=\"tool_name\", dest_key=\"tool_name\"),\n",
    "    Link(\"input\", \"tool_runner\", src_key=\"tool_input\", dest_key=\"tool_input\"),\n",
    "    Link(\"input\", \"agent_output\", src_key=\"reasoning_step\", dest_key=\"reasoning_step\"),\n",
    "    Link(\"input\", \"agent_output\", src_key=\"is_done\", dest_key=\"is_done\"),\n",
    "    Link(\"tool_runner\", \"agent_output\", dest_key=\"tool_output\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "## define simple if-else decision module to only call tool if the agent isn't done\n",
    "## otherwise skip and directly process output \n",
    "def react_output_fn(\n",
    "    chat_response: ChatResponse,\n",
    ") -> Tuple[bool, Dict[str, Any]]:\n",
    "    \"\"\"ReAct output function.\"\"\"\n",
    "    output_parser = ReActOutputParser()\n",
    "    reasoning_step = output_parser.parse(chat_response.message.content)\n",
    "    if reasoning_step.is_done:\n",
    "        return True, {\"reasoning_step\": reasoning_step, \"is_done\": True}\n",
    "    else:\n",
    "        return False, {\n",
    "            \"tool_name\": reasoning_step.action,\n",
    "            \"tool_input\": reasoning_step.action_input,\n",
    "            \"reasoning_step\": reasoning_step,\n",
    "            \"is_done\": False\n",
    "        }\n",
    "\n",
    "react_output_component = IfElseComponent(\n",
    "    fn=react_output_fn,\n",
    "    choice1=agent_output_component,\n",
    "    choice2=tool_qp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "219ec2ed-474e-450e-866c-15772055b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_pipeline import QueryPipeline as QP\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "qp = QP(\n",
    "    modules={\n",
    "        \"agent_input\": agent_input_component,\n",
    "        \"react_prompt\": react_prompt_component,\n",
    "        \"llm\": OpenAI(model=\"gpt-4-1106-preview\"),\n",
    "        \"react_output\": react_output_component,\n",
    "    },\n",
    "    verbose=True,\n",
    ")\n",
    "qp.add_link(\"agent_input\", \"react_prompt\", dest_key=\"input\")\n",
    "qp.add_chain([\"react_prompt\", \"llm\", \"react_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ff8b0-efc3-482d-8224-f3b82c9a1c2c",
   "metadata": {},
   "source": [
    "### Visualize Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd5e739b-ef99-44dd-91a3-a495fa11e4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_dag.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"agent_dag.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x157d17c70>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(qp.dag)\n",
    "net.show(\"agent_dag.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2526dac0-6209-421b-8d0d-7cfcc0947f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOL Pipeline\n",
    "# net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "# net.from_nx(tool_qp.dag)\n",
    "# net.show(\"agent_dag.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7785354-1fe0-49b4-8b48-08ce51853a4e",
   "metadata": {},
   "source": [
    "## Setup Agent around Text-to-SQL Query Pipeline\n",
    "\n",
    "This is our way to setup an agent around a text-to-SQL Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d01ea94b-3d3f-4b26-b71a-eea3d1d1ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import QueryPipelineAgentWorker, AgentRunner\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "agent_worker = QueryPipelineAgentWorker(qp)\n",
    "agent = AgentRunner(agent_worker, callback_manager=CallbackManager([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdd8835d-1044-4cca-a6aa-59f7635096ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AgentFnComponent(partial_dict={'tools': [<llama_index.tools.query_engine.QueryEngineTool object at 0x157cef070>]}, fn=<function react_prompt_fn at 0x157cc29e0>, async_fn=None),\n",
       " AgentFnComponent(partial_dict={}, fn=<function finalize_fn at 0x1680031c0>, async_fn=None),\n",
       " AgentFnComponent(partial_dict={}, fn=<function finalize_fn at 0x1680031c0>, async_fn=None)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_worker.agent_fn_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373bd661-4a6b-4c5b-b35b-0082921cc3b9",
   "metadata": {},
   "source": [
    "## Run the Agent\n",
    "\n",
    "Let's try the agent on some sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53c5d9bb-bc73-4b4d-b966-1966ba26b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = agent.chat(\n",
    "#     \"What are the top modes of transporation fo the city with the higehest population?\"\n",
    "# )\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ea55110-98fb-4acf-b2f8-eee2b15c087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start task\n",
    "task = agent.create_task(\"What are some tracks from the artist AC/DC? Limit it to 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f35e816-e76b-43ba-8eb4-ddda8ee97a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module agent_input with input: \n",
      "state: {'sources': [], 'memory': ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'), chat_store=SimpleChatSto...\n",
      "task: task_id='e93cfd04-1dec-4121-9c32-c48285776de6' input='What are some tracks from the artist AC/DC? Limit it to 3' memory=ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method ...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_prompt with input: \n",
      "input: What are some tracks from the artist AC/DC? Limit it to 3\n",
      "\n",
      "\u001b[0mCURRENT CHAT: [ObservationReasoningStep(observation='What are some tracks from the artist AC/DC? Limit it to 3')]\n",
      "\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
      "messages: [ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Too...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_output with input: \n",
      "chat_response: assistant: Thought: I need to use a tool to help me answer the question.\n",
      "Action: sql_tool\n",
      "Action Input: {\"input\": \"Select track_name from music_database where artist_name = 'AC/DC' limit 3\"}\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "tool_name: sql_tool\n",
      "tool_input: {'input': \"Select track_name from music_database where artist_name = 'AC/DC' limit 3\"}\n",
      "reasoning_step: thought='I need to use a tool to help me answer the question.' action='sql_tool' action_input={'input': \"Select track_name from music_database where artist_name = 'AC/DC' limit 3\"}\n",
      "is_done: False\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module tool_runner with input: \n",
      "tool_name: sql_tool\n",
      "tool_input: {'input': \"Select track_name from music_database where artist_name = 'AC/DC' limit 3\"}\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module agent_output with input: \n",
      "reasoning_step: thought='I need to use a tool to help me answer the question.' action='sql_tool' action_input={'input': \"Select track_name from music_database where artist_name = 'AC/DC' limit 3\"}\n",
      "is_done: False\n",
      "tool_output: The top 3 tracks by AC/DC are \"For Those About To Rock (We Salute You)\", \"Put The Finger On You\", and \"Let's Get It Up\".\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a13e449-db6c-43a3-99b7-70d23687e8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module agent_input with input: \n",
      "state: {'sources': [], 'memory': ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'), chat_store=SimpleChatSto...\n",
      "task: task_id='e93cfd04-1dec-4121-9c32-c48285776de6' input='What are some tracks from the artist AC/DC? Limit it to 3' memory=ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method ...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_prompt with input: \n",
      "input: What are some tracks from the artist AC/DC? Limit it to 3\n",
      "\n",
      "\u001b[0mCURRENT CHAT: [ObservationReasoningStep(observation='What are some tracks from the artist AC/DC? Limit it to 3'), ActionReasoningStep(thought='I need to use a tool to help me answer the question.', action='sql_tool', action_input={'input': \"Select track_name from music_database where artist_name = 'AC/DC' limit 3\"}), ObservationReasoningStep(observation='The top 3 tracks by AC/DC are \"For Those About To Rock (We Salute You)\", \"Put The Finger On You\", and \"Let\\'s Get It Up\".'), ObservationReasoningStep(observation='What are some tracks from the artist AC/DC? Limit it to 3')]\n",
      "\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
      "messages: [ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Too...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_output with input: \n",
      "chat_response: assistant: Thought: The user has repeated the request, but since I already have the information from the previous observation, I can provide the answer without using any more tools.\n",
      "Answer: The top 3 ...\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22dffeec-651e-43e7-8929-827566c8f6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_output.is_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44372a37-0b1e-4d90-8f42-9816546cc66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.finalize_response(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ba842bd-cc86-422b-8c17-2ff87c1962b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 3 tracks by AC/DC are \"For Those About To Rock (We Salute You)\", \"Put The Finger On You\", and \"Let's Get It Up\".\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8458f-4fab-420f-bb80-214e1f11c0a8",
   "metadata": {},
   "source": [
    "## Setup Agent with Tool Use\n",
    "\n",
    "User query -> reasoning -> output parsing + tool use -> add to agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f729fc-882d-4466-95b6-254da23a3724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
