{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e5f9aa4",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dc07d6a",
   "metadata": {},
   "source": [
    "# Evaluation using [Prometheus](https://huggingface.co/TheBloke/prometheus-13B-v1.0-GPTQ) model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c142b766",
   "metadata": {},
   "source": [
    "Evaluation is a crucial aspect of iterating over your RAG (Retrieval-Augmented Generation) pipeline. This process has relied heavily on GPT-4. However, a new open-source model named [Prometheus](https://arxiv.org/abs/2310.08491) has recently emerged as an alternative for evaluation purposes.\n",
    "\n",
    "In this notebook, we will demonstrate how you can utilize the Prometheus model for evaluation, integrating it with the LlamaIndex abstractions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36fc6fe6",
   "metadata": {},
   "source": [
    "We will demonstrate the correctness evaluation using the Prometheus model with two datasets from the Llama Datasets. If you haven't yet explored Llama Datasets, I recommend taking some time to read about them [here](https://blog.llamaindex.ai/introducing-llama-datasets-aadb9994ad9e).\n",
    "\n",
    "1. Paul Graham Essay\n",
    "2. Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the same event-loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17449837",
   "metadata": {},
   "source": [
    "## Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b44ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llama_dataset import download_llama_dataset\n",
    "\n",
    "paul_graham_rag_dataset, paul_graham_documents = download_llama_dataset(\n",
    "    \"PaulGrahamEssayDataset\", \"./data/paul_graham\"\n",
    ")\n",
    "\n",
    "llama2_rag_dataset, llama2_documents = download_llama_dataset(\n",
    "    \"Llama2PaperDataset\", \"./data/llama2\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f08c135c",
   "metadata": {},
   "source": [
    "## Define Prometheus LLM hosted on HuggingFace and prompt template.\n",
    "\n",
    "We hosted the model on HF Inference endpoint using Nvidia A10G GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e165956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import HuggingFaceInferenceAPI\n",
    "\n",
    "HF_TOKEN = \"YOUR HF TOKEN\"\n",
    "HF_ENDPOINT_URL = (\n",
    "    \"https://lj6l3d9g2zwx5gfn.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    ")\n",
    "\n",
    "prometheus_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=HF_ENDPOINT_URL,\n",
    "    token=HF_TOKEN,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e522fbf1",
   "metadata": {},
   "source": [
    "### Correctness Evaluation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8648503",
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_correctness_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query, a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. \n",
    "\t\t\t1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general. \n",
    "\t\t\t2. After writing a feedback, write a score that is an integer of 1 or 5. You should refer to the score rubric. \n",
    "\t\t\t3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer of 1 or 5)\" \n",
    "\t\t\t4. Please do not generate any other opening, closing, and explanations. \n",
    "\n",
    "\t\t\t###The instruction to evaluate: Your task is to evaluate the generated answer and reference answer for the query: {query}\n",
    "\n",
    "\t\t\t###Generate answer to evaluate: {generated_answer} \n",
    "\n",
    "            ###Reference Answer (Score 5): {reference_answer}\n",
    "            \n",
    "    \t\t###Score Rubrics: \n",
    "            Score 1: If the generated answer is not relevant to the user query and reference answer.\n",
    "            Score 2: If the generated answer is according to reference answer but not relevant to user query.\n",
    "            Score 3: If the generated answer is relevant to the user query and reference answer but contains mistakes.\n",
    "    \t\tScore 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\n",
    "            Score 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.\n",
    "    \n",
    "    \t\t###Feedback:\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "240e82ad",
   "metadata": {},
   "source": [
    "### Faithfulness Evaluation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11597ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_faithfulness_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), an information, a context, and a score rubric representing evaluation criteria are given. \n",
    "\t        1. You are provided with evaluation task with the help of information, context information to give result based on score rubrics.\n",
    "            2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n",
    "\t\t\t3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n",
    "            4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)‚Äù \n",
    "            5. Please do not generate any other opening, closing, and explanations. \n",
    "\n",
    "        ###The instruction to evaluate: Your task is to evaluate if the given piece of information is supported by context. You are provided with some examples for reference.\n",
    "\n",
    "        ###Information: {query_str} \n",
    "\n",
    "        ###Context: {context_str}\n",
    "            \n",
    "        ###Score Rubrics: \n",
    "        Score YES: If the given piece of information is supported by context.\n",
    "        Score NO: If the given piece of information is not supported by context\n",
    "    \n",
    "        ###Feedback: \"\"\"\n",
    "\n",
    "prometheus_faithfulness_refine_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a information, a context information, an existing answer, and a score rubric representing a evaluation criteria are given. \n",
    "\t\t\t1. You are provided with evaluation task with the help of information, context information and an existing answer.\n",
    "            2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n",
    "\t\t\t3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n",
    "\t\t\t4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\" \n",
    "\t\t\t5. Please do not generate any other opening, closing, and explanations. \n",
    "\n",
    "\t\t\t###The instruction to evaluate: If the information is present in the context and also provided with an existing answer.\n",
    "\n",
    "\t\t\t###Existing answer: {existing_answer} \n",
    "\n",
    "            ###Information: {query_str}\n",
    "\n",
    "            ###Context: {context_msg}\n",
    "            \n",
    "    \t\t###Score Rubrics: \n",
    "            Score YES: If the existing answer is already YES or If the Information is present in the context.\n",
    "            Score NO: If the existing answer is NO and If the Information is not present in the context.\n",
    "    \n",
    "    \t\t###Feedback: \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00b50daa",
   "metadata": {},
   "source": [
    "### Relevancy Evaluation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_relevancy_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, and a score rubric representing evaluation criteria are given. \n",
    "            1. You are provided with evaluation task with the help of a query with response and context.\n",
    "            2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n",
    "\t\t\t3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n",
    "            4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)‚Äù \n",
    "            5. Please do not generate any other opening, closing, and explanations. \n",
    "\n",
    "        ###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n",
    "\n",
    "        ###Query and Response: {query_str} \n",
    "\n",
    "        ###Context: {context_str}\n",
    "            \n",
    "        ###Score Rubrics: \n",
    "        Score YES: If the response for the query is in line with the context information provided.\n",
    "        Score NO: If the response for the query is not in line with the context information provided.\n",
    "    \n",
    "        ###Feedback: \"\"\"\n",
    "\n",
    "prometheus_relevancy_refine_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, an existing answer, and a score rubric representing a evaluation criteria are given. \n",
    "\t\t\t1. You are provided with evaluation task with the help of a query with response and context and an existing answer.\n",
    "            2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n",
    "\t\t\t3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n",
    "\t\t\t4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\" \n",
    "\t\t\t5. Please do not generate any other opening, closing, and explanations. \n",
    "\n",
    "\t\t\t###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n",
    "\n",
    "\t\t\t###Query and Response: {query_str} \n",
    "\n",
    "            ###Context: {context_str}\n",
    "            \n",
    "    \t\t###Score Rubrics: \n",
    "            Score YES: If the existing answer is already YES or If the response for the query is in line with the context information provided.\n",
    "            Score NO: If the existing answer is NO and If the response for the query is in line with the context information provided.\n",
    "    \n",
    "    \t\t###Feedback: \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fb0de17",
   "metadata": {},
   "source": [
    "Set OpenAI Key for indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30f967ea",
   "metadata": {},
   "source": [
    "## Define parser function \n",
    "\n",
    "It will be used in correctness evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import re\n",
    "\n",
    "\n",
    "def parser_function(output_str: str) -> Tuple[float, str]:\n",
    "    # Pattern to match the feedback and response\n",
    "    # This pattern looks for any text ending with '[RESULT]' followed by a number\n",
    "    pattern = r\"(.+?) \\[RESULT\\] (\\d)\"\n",
    "\n",
    "    # Using regex to find all matches\n",
    "    matches = re.findall(pattern, output_str)\n",
    "\n",
    "    # Check if any match is found\n",
    "    if matches:\n",
    "        # Assuming there's only one match in the text, extract feedback and response\n",
    "        feedback, score = matches[0]\n",
    "        score = float(score.strip()) if score is not None else score\n",
    "        return score, feedback.strip()\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c72df564",
   "metadata": {},
   "source": [
    "## Define Correctness, FaithFulness, Relevancy Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea574259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    ")\n",
    "\n",
    "# Provide Prometheus model in service_context\n",
    "prometheus_service_context = ServiceContext.from_defaults(llm=prometheus_llm)\n",
    "\n",
    "# CorrectnessEvaluator with Prometheus model\n",
    "prometheus_correctness_evaluator = CorrectnessEvaluator(\n",
    "    service_context=prometheus_service_context,\n",
    "    parser_function=parser_function,\n",
    "    eval_template=prometheus_correctness_eval_prompt_template,\n",
    ")\n",
    "\n",
    "# FaithfulnessEvaluator with Prometheus model\n",
    "prometheus_faithfulness_evaluator = FaithfulnessEvaluator(\n",
    "    service_context=prometheus_service_context,\n",
    "    eval_template=prometheus_faithfulness_eval_prompt_template,\n",
    "    refine_template=prometheus_faithfulness_refine_prompt_template,\n",
    ")\n",
    "\n",
    "# RelevancyEvaluator with Prometheus model\n",
    "prometheus_relevancy_evaluator = RelevancyEvaluator(\n",
    "    service_context=prometheus_service_context,\n",
    "    eval_template=prometheus_relevancy_eval_prompt_template,\n",
    "    refine_template=prometheus_relevancy_refine_prompt_template,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d14dac0",
   "metadata": {},
   "source": [
    "## Let's create a function to create `query_engine` and `rag_dataset` for different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c99c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llama_dataset import LabelledRagDataset\n",
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "\n",
    "def create_query_engine_rag_dataset(dataset_path):\n",
    "    rag_dataset = LabelledRagDataset.from_json(\n",
    "        f\"{dataset_path}/rag_dataset.json\"\n",
    "    )\n",
    "    documents = SimpleDirectoryReader(\n",
    "        input_dir=f\"{dataset_path}/source_files\"\n",
    "    ).load_data()\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(documents=documents)\n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    return query_engine, rag_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5a5b50e",
   "metadata": {},
   "source": [
    "## Function to check the distribution of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d39f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def get_scores_distribution(scores: List[float]) -> Dict[str, float]:\n",
    "    # Counting the occurrences of each score\n",
    "    score_counts = Counter(scores)\n",
    "\n",
    "    # Total number of scores\n",
    "    total_scores = len(scores)\n",
    "\n",
    "    # Calculating the percentage distribution\n",
    "    percentage_distribution = {\n",
    "        score: (count / total_scores) * 100\n",
    "        for score, count in score_counts.items()\n",
    "    }\n",
    "\n",
    "    return percentage_distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c8a631a",
   "metadata": {},
   "source": [
    "## Function to check faithfulness and relevancy evaluation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_results(key, eval_results):\n",
    "    results = eval_results[key]\n",
    "    correct = 0\n",
    "    for result in results:\n",
    "        if result.passing:\n",
    "            correct += 1\n",
    "    score = correct / len(results)\n",
    "    print(f\"{key} Score: {round(score, 2)}\")\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a90c5bc",
   "metadata": {},
   "source": [
    "## Function to compute correctness evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d07039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_sample(sample, evaluator):\n",
    "    query = sample.query\n",
    "    reference_answer = sample.reference_answer\n",
    "    response = query_engine.query(query).response\n",
    "\n",
    "    result = evaluator.evaluate(\n",
    "        query=query,\n",
    "        response=response,\n",
    "        reference=reference_answer,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def correctness_evaluation_scores(rag_dataset, evaluator):\n",
    "    scores = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Create a list to hold the future results\n",
    "        futures = [\n",
    "            executor.submit(process_sample, ex, evaluator)\n",
    "            for ex in rag_dataset.examples\n",
    "        ]\n",
    "\n",
    "        # Process the futures as they are completed\n",
    "        for future in tqdm(\n",
    "            as_completed(futures), total=len(rag_dataset.examples)\n",
    "        ):\n",
    "            result = future.result()\n",
    "            # Here, you need to extract scores from the result\n",
    "            scores.append(result.score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2300f843",
   "metadata": {},
   "source": [
    "### PaulGraham Essay text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae4d9b-66d8-4e30-be53-aae3b4d01343",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine, rag_dataset = create_query_engine_rag_dataset(\n",
    "    \"./data/paul_graham\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec19307",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [example.query for example in rag_dataset.examples]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90f21697",
   "metadata": {},
   "source": [
    "### Compute Correctness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffeb494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:21<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "prometheus_scores = correctness_evaluation_scores(\n",
    "    rag_dataset, prometheus_correctness_evaluator\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16b9ea6e",
   "metadata": {},
   "source": [
    "### Compute Faithfulness and Relevancy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d67e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import BatchEvalRunner\n",
    "\n",
    "batch_runner = BatchEvalRunner(\n",
    "    {\n",
    "        \"faithfulness\": prometheus_faithfulness_evaluator,\n",
    "        \"relevancy\": prometheus_relevancy_evaluator,\n",
    "    },\n",
    "    workers=8,\n",
    ")\n",
    "\n",
    "eval_results = await batch_runner.aevaluate_queries(\n",
    "    query_engine, queries=questions\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf51af00",
   "metadata": {},
   "source": [
    "### Correctness Evaluation score distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06baac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3.0: 59.09090909090909,\n",
       " 1.0: 34.090909090909086,\n",
       " 5.0: 2.272727272727273,\n",
       " None: 2.272727272727273,\n",
       " 4.0: 2.272727272727273}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores_distribution(prometheus_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d7cdab4",
   "metadata": {},
   "source": [
    "### Faithfulness Evaluation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9fb039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "score = get_eval_results(\"faithfulness\", eval_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07eadb57",
   "metadata": {},
   "source": [
    "### Relevancy Evaluation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f637a28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevancy Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "score = get_eval_results(\"relevancy\", eval_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8349f1a",
   "metadata": {},
   "source": [
    "### Llama2 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine, rag_dataset = create_query_engine_rag_dataset(\"./data/llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [example.query for example in rag_dataset.examples]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d632a084",
   "metadata": {},
   "source": [
    "### Compute Correctness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61725219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [03:05<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "prometheus_scores = correctness_evaluation_scores(\n",
    "    rag_dataset, prometheus_correctness_evaluator\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c26a012",
   "metadata": {},
   "source": [
    "### Compute Faithfulness and Relevancy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_runner = BatchEvalRunner(\n",
    "    {\n",
    "        \"faithfulness\": prometheus_faithfulness_evaluator,\n",
    "        \"relevancy\": prometheus_relevancy_evaluator,\n",
    "    },\n",
    "    workers=8,\n",
    ")\n",
    "\n",
    "eval_results = await batch_runner.aevaluate_queries(\n",
    "    query_engine, queries=questions\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bedaa07b",
   "metadata": {},
   "source": [
    "### Correctness Evaluation score distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92560236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0: 11.0, 3.0: 78.0, 4.0: 7.000000000000001, 5.0: 4.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores_distribution(prometheus_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fc3e64a",
   "metadata": {},
   "source": [
    "### Faithfulness Evaluation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a560dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness Score: 0.56\n"
     ]
    }
   ],
   "source": [
    "score = get_eval_results(\"faithfulness\", eval_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d9441b8",
   "metadata": {},
   "source": [
    "### Relevancy Evaluation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f7c757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevancy Score: 0.61\n"
     ]
    }
   ],
   "source": [
    "score = get_eval_results(\"relevancy\", eval_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbe28800",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "1. The cost for evaluation (approx.): `$0.433` for `144` queries (`44` for Paul Graham Essay and `100` for Llama2 paper) which accounts to `$0.003` per query.\n",
    "2. The higher percentage of examples with a score of `3.0` suggests that the model is making certain mistakes in relation to the reference answer. Additionally, in some cases, the model fails to provide a result, which is indicated by `None`.\n",
    "3. The faithfulness and relevancy scores indicate that there are instances of hallucination and a lack of answer relevancy in relation to the query and context in some examples.\n",
    "\n",
    "Note: The endpoint on HF is served on AWS Nvidia A10G ¬∑ 1x GPU ¬∑ 24 GB which costs $1.3/h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
