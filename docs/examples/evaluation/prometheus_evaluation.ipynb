{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dc07d6a",
   "metadata": {},
   "source": [
    "# Evaluation using [Prometheus](https://huggingface.co/TheBloke/prometheus-13B-v1.0-GPTQ) model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c142b766",
   "metadata": {},
   "source": [
    "Evaluation is a crucial aspect of iterating over your RAG (Retrieval-Augmented Generation) pipeline. This process has relied heavily on GPT-4. However, a new open-source model named [Prometheus](https://arxiv.org/abs/2310.08491) has recently emerged as an alternative for evaluation purposes.\n",
    "\n",
    "In this notebook, we will demonstrate how you can utilize the Prometheus model for evaluation, integrating it with the LlamaIndex abstractions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36fc6fe6",
   "metadata": {},
   "source": [
    "We will demonstrate the correctness evaluation using the Prometheus model with two datasets from the Llama Datasets. If you haven't yet explored Llama Datasets, I recommend taking some time to read about them [here](https://blog.llamaindex.ai/introducing-llama-datasets-aadb9994ad9e).\n",
    "\n",
    "1. Paul Graham Essay\n",
    "2. Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the same event-loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17449837",
   "metadata": {},
   "source": [
    "## Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b44ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llama_dataset import download_llama_dataset\n",
    "\n",
    "paul_graham_rag_dataset, paul_graham_documents = download_llama_dataset(\n",
    "    \"PaulGrahamEssayDataset\", \"./data/paul_graham\"\n",
    ")\n",
    "\n",
    "llama2_rag_dataset, llama2_documents = download_llama_dataset(\n",
    "    \"Llama2PaperDataset\", \"./data/llama2\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f08c135c",
   "metadata": {},
   "source": [
    "## Define Prometheus LLM hosted on HuggingFace and prompt template.\n",
    "\n",
    "We hosted the model on HF Inference endpoint using Nvidia A10G GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e165956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import HuggingFaceInferenceAPI\n",
    "\n",
    "HF_TOKEN = \"YOUR HF TOKEN\"\n",
    "HF_ENDPOINT_URL = \"YOUR MODEL ENDPOINT\"\n",
    "\n",
    "prometheus_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=HF_ENDPOINT_URL,\n",
    "    token=HF_TOKEN,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. \n",
    "\t\t\t1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general. \n",
    "\t\t\t2. After writing a feedback, write a score that is an integer of 1 or 5. You should refer to the score rubric. \n",
    "\t\t\t3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer of 1 or 5)\" \n",
    "\t\t\t4. Please do not generate any other opening, closing, and explanations. \n",
    "\n",
    "\t\t\t###The instruction to evaluate: Your task is to evaluate the generated answer and reference answer for the query: {query}\n",
    "\n",
    "\t\t\t###Generate answer to evaluate: {generated_answer} \n",
    "\n",
    "            ###Reference Answer (Score 5): {reference_answer}\n",
    "            \n",
    "    \t\t###Score Rubrics: [Is the model able to identify and react correctly to the emotional context of the user's input?] \n",
    "            Score 1: If the generated answer is not relevant to the user query.\n",
    "            Score 3: If the generated answer is relevant to the user query and reference answer but contains mistakes.\n",
    "    \t\tScore 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\n",
    "            Score 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.\n",
    "    \n",
    "    \t\t###Feedback:\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fb0de17",
   "metadata": {},
   "source": [
    "Set OpenAI Key for indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30f967ea",
   "metadata": {},
   "source": [
    "## Define parser function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import re\n",
    "\n",
    "\n",
    "def parser_function(output_str: str) -> Tuple[float, str]:\n",
    "    # Pattern to match the feedback and response\n",
    "    # This pattern looks for any text ending with '[RESULT]' followed by a number\n",
    "    pattern = r\"(.+?) \\[RESULT\\] (\\d)\"\n",
    "\n",
    "    # Using regex to find all matches\n",
    "    matches = re.findall(pattern, output_str)\n",
    "\n",
    "    # Check if any match is found\n",
    "    if matches:\n",
    "        # Assuming there's only one match in the text, extract feedback and response\n",
    "        feedback, score = matches[0]\n",
    "        score = float(score.strip()) if score is not None else score\n",
    "        return score, feedback.strip()\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c72df564",
   "metadata": {},
   "source": [
    "## Define Correctness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea574259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.evaluation import CorrectnessEvaluator\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# CorrectnessEvaluator with Prometheus model\n",
    "prometheus_service_context = ServiceContext.from_defaults(llm=prometheus_llm)\n",
    "prometheus_evaluator = CorrectnessEvaluator(\n",
    "    service_context=prometheus_service_context,\n",
    "    parser_function=parser_function,\n",
    "    eval_template=prometheus_eval_prompt_template,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d14dac0",
   "metadata": {},
   "source": [
    "## Let's create a function to create `query_engine` and `rag_dataset` for different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c99c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llama_dataset import LabelledRagDataset\n",
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "\n",
    "def create_query_engine_rag_dataset(dataset_path):\n",
    "    rag_dataset = LabelledRagDataset.from_json(\n",
    "        f\"{dataset_path}/rag_dataset.json\"\n",
    "    )\n",
    "    documents = SimpleDirectoryReader(\n",
    "        input_dir=f\"{dataset_path}/source_files\"\n",
    "    ).load_data()\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(documents=documents)\n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    return query_engine, rag_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5a5b50e",
   "metadata": {},
   "source": [
    "## Function to check the distribution of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d39f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def get_scores_distribution(scores: List[float]) -> Dict[float, float]:\n",
    "    # Counting the occurrences of each score\n",
    "    score_counts = Counter(scores)\n",
    "\n",
    "    # Total number of scores\n",
    "    total_scores = len(scores)\n",
    "\n",
    "    # Calculating the percentage distribution\n",
    "    percentage_distribution = {\n",
    "        score: (count / total_scores) * 100\n",
    "        for score, count in score_counts.items()\n",
    "    }\n",
    "\n",
    "    return percentage_distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a90c5bc",
   "metadata": {},
   "source": [
    "## Function to compute correctness evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d07039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_sample(sample, evaluator):\n",
    "    query = sample.query\n",
    "    reference_answer = sample.reference_answer\n",
    "    response = query_engine.query(query).response\n",
    "\n",
    "    result = evaluator.evaluate(\n",
    "        query=query,\n",
    "        response=response,\n",
    "        reference=reference_answer,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def correctness_evaluation_scores(rag_dataset, evaluator):\n",
    "    scores = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Create a list to hold the future results\n",
    "        futures = [\n",
    "            executor.submit(process_sample, ex, evaluator)\n",
    "            for ex in rag_dataset.examples\n",
    "        ]\n",
    "\n",
    "        # Process the futures as they are completed\n",
    "        for future in tqdm(\n",
    "            as_completed(futures), total=len(rag_dataset.examples)\n",
    "        ):\n",
    "            result = future.result()\n",
    "            # Here, you need to extract scores from the result\n",
    "            scores.append(result.score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2300f843",
   "metadata": {},
   "source": [
    "### PaulGraham Essay text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae4d9b-66d8-4e30-be53-aae3b4d01343",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine, rag_dataset = create_query_engine_rag_dataset(\n",
    "    \"./data/paul_graham\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffeb494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [01:19<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "prometheus_scores = correctness_evaluation_scores(\n",
    "    rag_dataset, prometheus_evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06baac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3.0: 88.63636363636364, 1.0: 9.090909090909092, 5.0: 2.272727272727273}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores_distribution(prometheus_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8349f1a",
   "metadata": {},
   "source": [
    "### Llama2 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine, rag_dataset = create_query_engine_rag_dataset(\"./data/llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc86d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:02<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "prometheus_scores = correctness_evaluation_scores(\n",
    "    rag_dataset, prometheus_evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92560236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0: 11.0, 3.0: 75.0, 4.0: 12.0, 5.0: 2.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores_distribution(prometheus_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbe28800",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "In both the datasets, the percentage of examples with score of `3.0` is higher which indicates there are certain mistakes that the model is generating wrt reference answer.\n",
    "\n",
    "The cost for evaluation (approx.): $0.108"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
