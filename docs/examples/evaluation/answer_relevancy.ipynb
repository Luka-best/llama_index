{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c693f512-0033-4bca-9824-261701e4a4d4",
   "metadata": {},
   "source": [
    "# Answer Relevancy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69792a3-24a1-424b-afc0-c28b13f2cb42",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to utilize the `AnswerRelevancyEvaluator` class to get a measure on the relevancy of a generated answer to given user query. This evaluator returns a `score` that is between 0 and 1 as well as a generated `feedback` explaining the score. Note that, higher score means higher relevancy. In particular, we prompt the judge LLM to take a step-by-step approach in providing a relevancy score, asking it to answer the following three questions of a generated answer to a query:\n",
    "\n",
    "1. Does the provided response match the subject matter of the user's query?\n",
    "2. Does the provided response attempt to address the focus or perspective on the subject matter taken on by the user's query?\n",
    "3. Does the provided response attempt to follow the instruction of the user's query?\n",
    "\n",
    "Each question is worth 1 point and so a perfect evaluation would yield a score of 3/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5d108-6cad-4b5f-848d-6f4edeef2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7436d4-8cf2-444f-a776-4544f666ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55957d59-44ab-45d3-9716-fb1aeb69633e",
   "metadata": {},
   "source": [
    "### Download the dataset (`LabelledRagDataset`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228c9ab-96d5-4e4c-8b8c-febe8375b649",
   "metadata": {},
   "source": [
    "For this demonstration, we will use a llama-dataset provided through our [llama-hub](https://llamahub.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1997b3-d3fc-4a95-b139-09c1fb256021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llama_dataset import download_llama_dataset\n",
    "from llama_index.llama_pack import download_llama_pack\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "# download and install dependencies for benchmark dataset\n",
    "rag_dataset, documents = download_llama_dataset(\n",
    "    \"EvaluatingLlmSurveyPaperDataset\", \"./data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e019d4-90ae-4e8c-91b6-7313493b4983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_answer_by</th>\n",
       "      <th>query_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the potential risks associated with l...</td>\n",
       "      <td>[Evaluating Large Language Models: A\\nComprehe...</td>\n",
       "      <td>According to the context information, the pote...</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the survey categorize the evaluation ...</td>\n",
       "      <td>[Evaluating Large Language Models: A\\nComprehe...</td>\n",
       "      <td>The survey categorizes the evaluation of LLMs ...</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the different types of reasoning disc...</td>\n",
       "      <td>[Contents\\n1 Introduction 4\\n2 Taxonomy and Ro...</td>\n",
       "      <td>The different types of reasoning discussed in ...</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How is toxicity evaluated in language models a...</td>\n",
       "      <td>[Contents\\n1 Introduction 4\\n2 Taxonomy and Ro...</td>\n",
       "      <td>Toxicity is evaluated in language models accor...</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the context of specialized LLMs evaluation,...</td>\n",
       "      <td>[5.1.3 Alignment Robustness . . . . . . . . . ...</td>\n",
       "      <td>In the context of specialized LLMs evaluation,...</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  What are the potential risks associated with l...   \n",
       "1  How does the survey categorize the evaluation ...   \n",
       "2  What are the different types of reasoning disc...   \n",
       "3  How is toxicity evaluated in language models a...   \n",
       "4  In the context of specialized LLMs evaluation,...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [Evaluating Large Language Models: A\\nComprehe...   \n",
       "1  [Evaluating Large Language Models: A\\nComprehe...   \n",
       "2  [Contents\\n1 Introduction 4\\n2 Taxonomy and Ro...   \n",
       "3  [Contents\\n1 Introduction 4\\n2 Taxonomy and Ro...   \n",
       "4  [5.1.3 Alignment Robustness . . . . . . . . . ...   \n",
       "\n",
       "                                    reference_answer reference_answer_by  \\\n",
       "0  According to the context information, the pote...  ai (gpt-3.5-turbo)   \n",
       "1  The survey categorizes the evaluation of LLMs ...  ai (gpt-3.5-turbo)   \n",
       "2  The different types of reasoning discussed in ...  ai (gpt-3.5-turbo)   \n",
       "3  Toxicity is evaluated in language models accor...  ai (gpt-3.5-turbo)   \n",
       "4  In the context of specialized LLMs evaluation,...  ai (gpt-3.5-turbo)   \n",
       "\n",
       "             query_by  \n",
       "0  ai (gpt-3.5-turbo)  \n",
       "1  ai (gpt-3.5-turbo)  \n",
       "2  ai (gpt-3.5-turbo)  \n",
       "3  ai (gpt-3.5-turbo)  \n",
       "4  ai (gpt-3.5-turbo)  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset.to_pandas()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ae70a-d1a2-4f86-909a-e9c6fb063930",
   "metadata": {},
   "source": [
    "Next, we build a RAG over the same source documents used to created the `rag_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6d215-1644-4da9-8f15-5c1c77ab35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88027888-102f-40df-a581-3047b35d4f11",
   "metadata": {},
   "source": [
    "With our RAG (i.e `query_engine`) defined, we can make predictions (i.e., generate responses to the query) with it over the `rag_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609c502-e2a7-4dfe-ba1d-5ee54fe59691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|████████████████████| 100/100 [00:06<00:00, 15.77it/s]\n",
      "Batch processing of predictions: 100%|████████████████████| 100/100 [00:06<00:00, 16.28it/s]\n",
      "Batch processing of predictions: 100%|██████████████████████| 76/76 [00:05<00:00, 13.11it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction_dataset = await rag_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=100, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db779e26-dad1-4380-a131-b906339a936e",
   "metadata": {},
   "source": [
    "### Evaluating Answer Relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96344bde-30c7-416f-a650-58e79b5abaff",
   "metadata": {},
   "source": [
    "We first need to define our evaluator (i.e. `AnswerRelevancyEvaluator`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb6c86-b88a-4344-b390-a4decfa71061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the gpt-4 judge\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.evaluation import AnswerRelevancyEvaluator\n",
    "\n",
    "judge = AnswerRelevancyEvaluator(\n",
    "    service_context=ServiceContext.from_defaults(\n",
    "        llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e5262-1fa0-411f-9f09-e2557f96116a",
   "metadata": {},
   "source": [
    "Now, we can use our evaluator to make evaluations by looping through all of the <example, prediction> pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d02ce-c7f3-49b7-b397-b94b0aa3fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tasks = []\n",
    "for example, prediction in zip(\n",
    "    rag_dataset.examples, prediction_dataset.predictions\n",
    "):\n",
    "    eval_tasks.append(\n",
    "        judge.aevaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "            sleep_time_in_seconds=1.0,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155634d-58c7-4dac-ac16-93a8537ddef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 276/276 [00:35<00:00,  7.80it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_results = await tqdm_asyncio.gather(*eval_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31a4ac-5871-4e78-856d-5055f249598f",
   "metadata": {},
   "source": [
    "### Taking a look at the evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d037782-b7fa-43f6-ad83-b80faad3606b",
   "metadata": {},
   "source": [
    "Here we use a utility function to convert the list of `EvaluationResult` objects into something more notebook friendly, that is a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa305d6-bba1-45ef-87a1-c4a2415fa54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation.notebook_utils import get_eval_results_df\n",
    "\n",
    "deep_df, mean_df = get_eval_results_df(\n",
    "    names=[\"baseline\"] * len(eval_results), results_arr=eval_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc452d5a-a2f5-4fb6-b41e-ff61cc7cb810",
   "metadata": {},
   "source": [
    "The above utility also provides the mean score across all of the evaluations in `mean_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c259a79-a1d6-4131-b986-8302931716be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rag</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.922101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            scores\n",
       "rag               \n",
       "baseline  0.922101"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f8b67c-8501-4ad2-ba71-d20ed24dc041",
   "metadata": {},
   "source": [
    "We can get a look at the raw distribution of the scores by invoking `value_counts()` on the `deep_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72768281-4fd2-480c-a1f2-5432606b0dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scores\n",
       "1.0    240\n",
       "0.5     29\n",
       "0.0      7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_df[\"scores\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4324ddbd-0b59-430a-89d5-c706bd55a184",
   "metadata": {},
   "source": [
    "It looks like for the most part, the default RAG does fairly well in terms of generating answers that are relevant to the query. Getting a closer look is made possible by viewing the records of `deep_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617540f-9eb7-47d7-8b96-7618236f1c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1457e_row0_col0, #T_1457e_row0_col1, #T_1457e_row0_col2, #T_1457e_row0_col3, #T_1457e_row0_col4, #T_1457e_row1_col0, #T_1457e_row1_col1, #T_1457e_row1_col2, #T_1457e_row1_col3, #T_1457e_row1_col4 {\n",
       "  inline-size: 300px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1457e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1457e_level0_col0\" class=\"col_heading level0 col0\" >rag</th>\n",
       "      <th id=\"T_1457e_level0_col1\" class=\"col_heading level0 col1\" >query</th>\n",
       "      <th id=\"T_1457e_level0_col2\" class=\"col_heading level0 col2\" >answer</th>\n",
       "      <th id=\"T_1457e_level0_col3\" class=\"col_heading level0 col3\" >scores</th>\n",
       "      <th id=\"T_1457e_level0_col4\" class=\"col_heading level0 col4\" >feedbacks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1457e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1457e_row0_col0\" class=\"data row0 col0\" >baseline</td>\n",
       "      <td id=\"T_1457e_row0_col1\" class=\"data row0 col1\" >What are the potential risks associated with large language models (LLMs) according to the context information?</td>\n",
       "      <td id=\"T_1457e_row0_col2\" class=\"data row0 col2\" >LLMs present potential risks such as private data leaks and the generation of inappropriate, harmful, or misleading content. The rapid progress of LLMs also raises concerns about the potential emergence of superintelligent systems without adequate safeguards.</td>\n",
       "      <td id=\"T_1457e_row0_col3\" class=\"data row0 col3\" >1.000000</td>\n",
       "      <td id=\"T_1457e_row0_col4\" class=\"data row0 col4\" >1. The response does match the subject matter of the user's query. It provides information about the potential risks associated with large language models (LLMs), which is exactly what the user asked for.\n",
       "2. The response also attempts to address the focus or perspective on the subject matter taken on by the user's query. It provides specific examples of risks, such as private data leaks and the generation of inappropriate, harmful, or misleading content, and also mentions concerns about the potential emergence of superintelligent systems without adequate safeguards.\n",
       "\n",
       "[RESULT] 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1457e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1457e_row1_col0\" class=\"data row1 col0\" >baseline</td>\n",
       "      <td id=\"T_1457e_row1_col1\" class=\"data row1 col1\" >How does the survey categorize the evaluation of LLMs and what are the three major groups mentioned?</td>\n",
       "      <td id=\"T_1457e_row1_col2\" class=\"data row1 col2\" >The survey categorizes the evaluation of LLMs by providing a well-structured taxonomy framework. The three major groups mentioned in the survey are knowledge and reasoning, alignment evaluation, and safety evaluation.</td>\n",
       "      <td id=\"T_1457e_row1_col3\" class=\"data row1 col3\" >1.000000</td>\n",
       "      <td id=\"T_1457e_row1_col4\" class=\"data row1 col4\" >1. The response does match the subject matter of the user's query. The user asked about how a survey categorizes the evaluation of LLMs and the response provides information on this, stating that the survey uses a well-structured taxonomy framework for this purpose.\n",
       "2. The response also addresses the focus or perspective on the subject matter taken on by the user's query. The user asked about the three major groups mentioned in the survey and the response provides this information, stating that the three major groups are knowledge and reasoning, alignment evaluation, and safety evaluation.\n",
       "\n",
       "[RESULT] 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17b175990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayify_df(deep_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a0d5f-253d-433b-85b5-abbd7d95bbe5",
   "metadata": {},
   "source": [
    "And, of course you can apply any filters as you like. For example, if you want to look at the examples that yielded less than perfect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cfaa1e-720b-4753-b8cc-4c76b6f21e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3bb92_row0_col0, #T_3bb92_row0_col1, #T_3bb92_row0_col2, #T_3bb92_row0_col3, #T_3bb92_row0_col4, #T_3bb92_row1_col0, #T_3bb92_row1_col1, #T_3bb92_row1_col2, #T_3bb92_row1_col3, #T_3bb92_row1_col4, #T_3bb92_row2_col0, #T_3bb92_row2_col1, #T_3bb92_row2_col2, #T_3bb92_row2_col3, #T_3bb92_row2_col4, #T_3bb92_row3_col0, #T_3bb92_row3_col1, #T_3bb92_row3_col2, #T_3bb92_row3_col3, #T_3bb92_row3_col4, #T_3bb92_row4_col0, #T_3bb92_row4_col1, #T_3bb92_row4_col2, #T_3bb92_row4_col3, #T_3bb92_row4_col4 {\n",
       "  inline-size: 300px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3bb92\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3bb92_level0_col0\" class=\"col_heading level0 col0\" >rag</th>\n",
       "      <th id=\"T_3bb92_level0_col1\" class=\"col_heading level0 col1\" >query</th>\n",
       "      <th id=\"T_3bb92_level0_col2\" class=\"col_heading level0 col2\" >answer</th>\n",
       "      <th id=\"T_3bb92_level0_col3\" class=\"col_heading level0 col3\" >scores</th>\n",
       "      <th id=\"T_3bb92_level0_col4\" class=\"col_heading level0 col4\" >feedbacks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3bb92_level0_row0\" class=\"row_heading level0 row0\" >35</th>\n",
       "      <td id=\"T_3bb92_row0_col0\" class=\"data row0 col0\" >baseline</td>\n",
       "      <td id=\"T_3bb92_row0_col1\" class=\"data row0 col1\" >In the evaluation of online shopping models, what is the notable difference between humans and language models in terms of performance?</td>\n",
       "      <td id=\"T_3bb92_row0_col2\" class=\"data row0 col2\" >The notable difference between humans and language models in the evaluation of online shopping models is that humans outperform language models in all metrics.</td>\n",
       "      <td id=\"T_3bb92_row0_col3\" class=\"data row0 col3\" >0.500000</td>\n",
       "      <td id=\"T_3bb92_row0_col4\" class=\"data row0 col4\" >1. The response does match the subject matter of the user's query, which is about the difference in performance between humans and language models in the context of online shopping models. \n",
       "2. The response attempts to address the focus of the user's query, but it does not provide specific details or examples to support the claim that humans outperform language models in all metrics. The user's query seems to be asking for a more detailed or nuanced explanation of the differences in performance.\n",
       "\n",
       "[RESULT] 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3bb92_level0_row1\" class=\"row_heading level0 row1\" >46</th>\n",
       "      <td id=\"T_3bb92_row1_col0\" class=\"data row1 col0\" >baseline</td>\n",
       "      <td id=\"T_3bb92_row1_col1\" class=\"data row1 col1\" >What are the four macroscopic perspectives used to categorize ethics and morality evaluations in the context of LLMs?</td>\n",
       "      <td id=\"T_3bb92_row1_col2\" class=\"data row1 col2\" >The four macroscopic perspectives used to categorize ethics and morality evaluations in the context of LLMs are expert-defined ethics and morality, evaluation with expert-defined ethics and morality, evaluation with crowdsourced ethics and morality, and evaluation with crowdsourced ethics and morality.</td>\n",
       "      <td id=\"T_3bb92_row1_col3\" class=\"data row1 col3\" >0.500000</td>\n",
       "      <td id=\"T_3bb92_row1_col4\" class=\"data row1 col4\" >1. The response does match the subject matter of the user's query. It provides information about the four macroscopic perspectives used to categorize ethics and morality evaluations in the context of LLMs.\n",
       "2. The response does attempt to address the focus or perspective on the subject matter taken on by the user's query. However, it seems to repeat the same two categories twice, which may be a mistake. The user asked for four distinct perspectives, but the response only provides two unique ones.\n",
       "\n",
       "[RESULT] 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3bb92_level0_row2\" class=\"row_heading level0 row2\" >61</th>\n",
       "      <td id=\"T_3bb92_row2_col0\" class=\"data row2 col0\" >baseline</td>\n",
       "      <td id=\"T_3bb92_row2_col1\" class=\"data row2 col1\" >How does the CBBQ evaluation method extend the BBQ approach? What additional categories are included in CBBQ?</td>\n",
       "      <td id=\"T_3bb92_row2_col2\" class=\"data row2 col2\" >The CBBQ evaluation method extends the BBQ approach by including additional categories in its evaluation framework. These additional categories are not explicitly mentioned in the given context information.</td>\n",
       "      <td id=\"T_3bb92_row2_col3\" class=\"data row2 col3\" >0.500000</td>\n",
       "      <td id=\"T_3bb92_row2_col4\" class=\"data row2 col4\" >1. The response does match the subject matter of the user's query, which is about how the CBBQ evaluation method extends the BBQ approach. The response correctly states that the CBBQ method extends the BBQ approach by including additional categories in its evaluation framework.\n",
       "2. However, the response does not fully address the focus or perspective on the subject matter taken on by the user's query. The user specifically asked about what additional categories are included in CBBQ, but the response does not provide this information.\n",
       "\n",
       "[RESULT] 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3bb92_level0_row3\" class=\"row_heading level0 row3\" >84</th>\n",
       "      <td id=\"T_3bb92_row3_col0\" class=\"data row3 col0\" >baseline</td>\n",
       "      <td id=\"T_3bb92_row3_col1\" class=\"data row3 col1\" >How does the BigToM benchmark align human Theory-of-Mind reasoning capabilities by controlling different variables and conditions in the causal graph?</td>\n",
       "      <td id=\"T_3bb92_row3_col2\" class=\"data row3 col2\" >The BigToM benchmark aligns human Theory-of-Mind reasoning capabilities by controlling different variables and conditions in the causal graph.</td>\n",
       "      <td id=\"T_3bb92_row3_col3\" class=\"data row3 col3\" >0.500000</td>\n",
       "      <td id=\"T_3bb92_row3_col4\" class=\"data row3 col4\" >1. The response does match the subject matter of the user's query. It mentions the BigToM benchmark, human Theory-of-Mind reasoning capabilities, and controlling different variables and conditions in the causal graph, which are all elements present in the query.\n",
       "2. However, the response does not attempt to address the focus or perspective on the subject matter taken on by the user's query. The user is asking for an explanation of how the BigToM benchmark aligns human Theory-of-Mind reasoning capabilities by controlling different variables and conditions in the causal graph. The response merely restates the query without providing any additional information or explanation.\n",
       "\n",
       "[RESULT] 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3bb92_level0_row4\" class=\"row_heading level0 row4\" >121</th>\n",
       "      <td id=\"T_3bb92_row4_col0\" class=\"data row4 col0\" >baseline</td>\n",
       "      <td id=\"T_3bb92_row4_col1\" class=\"data row4 col1\" >Who are the authors of the paper titled \"Frontier AI regulation: Managing emerging risks to public safety\"?</td>\n",
       "      <td id=\"T_3bb92_row4_col2\" class=\"data row4 col2\" >Based on the given context information, it is not possible to determine the authors of the paper titled \"Frontier AI regulation: Managing emerging risks to public safety.\"</td>\n",
       "      <td id=\"T_3bb92_row4_col3\" class=\"data row4 col3\" >0.500000</td>\n",
       "      <td id=\"T_3bb92_row4_col4\" class=\"data row4 col4\" >1. The response does match the subject matter of the user's query, which is about the authors of a specific paper. \n",
       "2. However, the response does not attempt to address the focus or perspective on the subject matter taken on by the user's query. The user is asking for specific information, namely the authors of a specific paper, and the response does not provide this information.\n",
       "\n",
       "[RESULT] 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2ae1d0c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayify_df(deep_df[deep_df[\"scores\"] < 1].head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
