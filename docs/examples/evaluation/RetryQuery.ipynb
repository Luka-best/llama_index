{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retry Query Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to add your OpenAI API key or enable debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\"\n",
    "\n",
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we ingest the Uber 10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.query_engine.retry_source_query_engine import RetrySourceQueryEngine\n",
    "from llama_index.readers.file.base import SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_docs = SimpleDirectoryReader(input_files=[\"../data/10k/uber_2021.pdf\"]).load_data()\n",
    "index = VectorStoreIndex.from_documents(uber_docs)\n",
    "query = \"How many drivers does Uber have?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will query with the default settings first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default query engine response: \n",
      "It is not possible to answer this question with the given context information.\n"
     ]
    }
   ],
   "source": [
    "# Default query engine\n",
    "default_query_engine = index.as_query_engine()\n",
    "response = default_query_engine.query(query)\n",
    "print(f\"Default query engine response: {response}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't give us a good result, so let's try with retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query engine with retry response: \n",
      "Here is a better answer.\n",
      "\n",
      "It is not possible to determine the exact number of drivers Uber has with the given context information. However, it is known that Uber has more than 70,000 Mobility drivers in the UK and has extended offers to all eligible drivers who are not already represented by an attorney. Additionally, it is known that in October and November of 2016, outside actors downloaded the personal data of approximately 57 million Drivers and consumers worldwide.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.query_engine.retry_query_engine import (\n",
    "    RetryQueryEngine,\n",
    ")\n",
    "from llama_index.evaluation.base import QueryResponseEvaluator\n",
    "\n",
    "# Query engine with retry\n",
    "query_response_evaluator = QueryResponseEvaluator()\n",
    "retry_query_engine = RetryQueryEngine(index.as_query_engine(), query_response_evaluator)\n",
    "retry_response = retry_query_engine.query(query)\n",
    "print(f\"Query engine with retry response: {retry_response}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working better. The following are more experimental modules that aren't fully working at the moment.\n",
    "\n",
    "The first tries to modify the source nodes by filtering the existing source nodes for the query based on llm node evaluation.\n",
    "\n",
    "A better solution would be to re-retrieve based on filtering out the nodes that didn't pass evaluation.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query engine with retry source response: \n",
      "It is not possible to answer this question with the given context information.\n"
     ]
    }
   ],
   "source": [
    "retry_source_query_engine = RetrySourceQueryEngine(index.as_query_engine(), query_response_evaluator)\n",
    "retry_source_response = retry_source_query_engine.query(query)\n",
    "print(f\"Query engine with retry source response: {retry_source_response}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module tries to use guidelines to direct the evaluator's behavior. You can customize your own guidelines.\n",
    "\n",
    "Currently this can lead to the llm hallucinating. WIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guideline eval evaluation result: The response should provide a specific number or statistic to answer the query. It should not be vague or ambiguous.\n",
      "Transformed query: Here is a previous bad answer.\n",
      "\n",
      "It is not possible to answer this question with the given context information.\n",
      "Here is some feedback from the evaluator about the response given.\n",
      "The response should provide a specific number or statistic to answer the query. It should not be vague or ambiguous.\n",
      "Now answer the question.\n",
      "\n",
      "What is the total number of Uber drivers?\n",
      "Query engine with retry guideline response: \n",
      "According to Uber's 2020 Annual Report, there were 3.9 million active drivers on the Uber platform worldwide as of December 31, 2020.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation.guideline_eval import GuidelineEvaluator, DEFAULT_GUIDELINES\n",
    "from llama_index.response.schema import Response\n",
    "from llama_index.indices.query.query_transform.feedback_transform import (\n",
    "    FeedbackQueryTransformation,\n",
    ")\n",
    "from llama_index.query_engine.retry_query_engine import (\n",
    "    RetryGuidelineQueryEngine,\n",
    ")\n",
    "\n",
    "# Guideline eval\n",
    "guideline_eval = GuidelineEvaluator(guidelines=DEFAULT_GUIDELINES) # just for example\n",
    "typed_response = response if isinstance(response, Response) else response.get_response()\n",
    "eval = guideline_eval.evaluate_response(query, typed_response)\n",
    "print(f\"Guideline eval evaluation result: {eval.feedback}\")\n",
    "feedback_query_transform = FeedbackQueryTransformation(resynthesize_query=True)\n",
    "transformed_query = feedback_query_transform.run(query, {\"evaluation\": eval})\n",
    "print(f\"Transformed query: {transformed_query.query_str}\")\n",
    "retry_guideline_query_engine = RetryGuidelineQueryEngine(index.as_query_engine(), guideline_eval, resynthesize_query=True)\n",
    "retry_guideline_response = retry_guideline_query_engine.query(query)\n",
    "print(f\"Query engine with retry guideline response: {retry_guideline_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
