{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Embeddings with IPEX-LLM on Intel GPU\n",
    "\n",
    "> [IPEX-LLM](https://github.com/intel-analytics/ipex-llm/) is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency.\n",
    "\n",
    "This example goes over how to use LlamaIndex to conduct embedding tasks with `ipex-llm` optimizations on Intel GPU. This would be helpful in applications such as RAG, document QA, etc.\n",
    "\n",
    "## Install Prerequisites\n",
    "To benefit from IPEX-LLM on Intel GPUs, there are several prerequisite steps for tools installation and environment preparation.\n",
    "\n",
    "If you are a Windows user, visit the [Install IPEX-LLM on Windows with Intel GPU Guide](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/install_windows_gpu.html), and follow [**Install Prerequisites**](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/install_windows_gpu.html#install-prerequisites) to install [Visual Studio](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/install_windows_gpu.html#install-visual-studio-2022), [GPU driver](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/install_windows_gpu.html#install-gpu-driver) and [Intel® oneAPI Base Toolkit 2024.0](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/install_windows_gpu.html#install-oneapi).\n",
    "\n",
    "If you are a Linux user, follow the guide [here](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Overview/install_gpu.html#id1) to install [GPU driver](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/install_linux_gpu.html#install-gpu-driver) and [Intel® oneAPI Base Toolkit 2024.0](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Overview/install_gpu.html#id1:~:text=Intel%C2%AE%20oneAPI%20Base%20Toolkit%202024.0%20installation%20methods%3A) **through PIP installer**.\n",
    "\n",
    "## Install `llama-index-embeddings-ipex-llm`\n",
    "\n",
    "This will also install `ipex-llm` and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-ipex-llm[xpu] --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> You can also use `https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/` as the `extra-indel-url`.\n",
    "\n",
    "## Runtime Configuration\n",
    "\n",
    "For optimal performance, it is recommended to set several environment variables based on your device:\n",
    "\n",
    "### For Windows Users with Intel Core Ultra integrated GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SYCL_CACHE_PERSISTENT\"] = \"1\"\n",
    "os.environ[\"BIGDL_LLM_XMX_DISABLED\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Linux Users with Intel Arc A-Series GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"USE_XETLA\"] = \"OFF\"\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "os.environ[\"SYCL_CACHE_PERSISTENT\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> For the first time that each model runs on Intel iGPU/Intel Arc A300-Series or Pro A60, it may take several minutes to compile.\n",
    ">\n",
    "> For other GPU type, please refer to [here](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Overview/install_gpu.html#runtime-configuration) for Windows users, and  [here](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Overview/install_gpu.html#id5) for Linux users.\n",
    "\n",
    "## `IpexLLMEmbedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ipex_llm import IpexLLMEmbedding\n",
    "\n",
    "embedding_model = IpexLLMEmbedding(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\", device=\"xpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please note that `IpexLLMEmbedding` currently only provides optimization for Hugging Face Bge models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency.\"\n",
    "query = \"What is IPEX-LLM?\"\n",
    "\n",
    "text_embedding = embedding_model.get_text_embedding(sentence)\n",
    "print(f\"embedding[:10]: {text_embedding[:10]}\")\n",
    "\n",
    "text_embeddings = embedding_model.get_text_embedding_batch([sentence, query])\n",
    "print(f\"text_embeddings[0][:10]: {text_embeddings[0][:10]}\")\n",
    "print(f\"text_embeddings[1][:10]: {text_embeddings[1][:10]}\")\n",
    "\n",
    "query_embedding = embedding_model.get_query_embedding(query)\n",
    "print(f\"query_embedding[:10]: {query_embedding[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
