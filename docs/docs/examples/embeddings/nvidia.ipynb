{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nvidia NIM embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to NVIDIA's NIM embedding service using the NVIDIA Embedding class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start an NVIDIA NIM embedding microservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model you want to deploy from NVIDIA NGC\n",
    "$ ngc registry model download-version \"ohlfw0olaadg/ea-participants/nv-embed-qa:4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull and tag the NIM embeddings container\n",
    "$ docker pull nvcr.io/ohlfw0olaadg/ea-participants/embedding-ms:cb2bba4eaf58b622acd3ac856dbf03d284cd770d\n",
    "$ docker tag nvcr.io/ohlfw0olaadg/ea-participants/embedding-ms:cb2bba4eaf58b622acd3ac856dbf03d284cd770d embedding-ms:0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the container\n",
    "$ docker run -it -p 12345:12345 --gpus='\"device=0\"' -v path/to/model.nemo:/path/to/model.nemo --name embedding-ms embedding-ms:0.0.1 /bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the model config at the following path with the path to your model\n",
    "# app/model_config_templates/NV-EMBED-QA_template.yaml\n",
    "# and build the triton model store\n",
    "$ model_repo_generator /app/model_config_templates/NV-Embed-QA_template.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the triton server and the API server\n",
    "$ /app/bin/web -m /model-store -p 12345 -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the embedding microservice with LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "batch_size = 16\n",
    "model_name = \"NV-Embed-QA\"\n",
    "api_endpoint_url = \"http://localhost:12345/v1/embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = NVIDIAEmbedding(\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    api_endpoint_url=api_endpoint_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding for a query\n",
    "embedding_model.get_query_embedding(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for multiple passages in batches\n",
    "embedding_model.get_text_embedding_batch([\"Hello\", \"World\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
