{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ae9bad-b8cc-43de-ba7d-387e0155674c",
   "metadata": {},
   "source": [
    "## Multimodal RAG Pipeline with Guardrails\n",
    "\n",
    "This guide introduces a robust **Multimodal Retrieval-Augmented Generation (RAG)** pipeline enhanced with integrated **guardrails** for secure, reliable, and contextually accurate responses. The pipeline processes multimodal inputs such as text, tables, images, and diagrams while employing guardrails to monitor and validate both input and output.\n",
    "\n",
    "### Note:\n",
    "This pipeline leverages the **Gemini 1.5 Flash** model through a free API for inference, making it accessible and cost-effective for development and experimentation.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview of the Pipeline\n",
    "\n",
    "The Multimodal RAG pipeline is designed to overcome the limitations of traditional text-based RAG systems by natively handling diverse document layouts and modalities. It leverages both text and image embeddings to retrieve and synthesize context-aware answers.\n",
    "\n",
    "### Key Features:\n",
    "1. **Multimodal Input Processing**:\n",
    "   - Handles text, images, and complex layouts directly.\n",
    "   - Converts document content into robust embeddings for retrieval.\n",
    "\n",
    "2. **Guardrails Integration**:\n",
    "   - Adds input/output scanners to enforce safety and quality.\n",
    "   - Dynamically validates queries and responses for risks such as toxicity or token overflow.\n",
    "\n",
    "3. **Custom Query Engine**:\n",
    "   - Designed to incorporate guardrails into query handling.\n",
    "   - Dynamically blocks, sanitizes, or validates inputs/outputs based on scanner results.\n",
    "\n",
    "4. **Cost-Effective Implementation**:\n",
    "   - Uses **Gemini 1.5 Flash** via a free API, minimizing costs while maintaining high performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Add Guardrails to Multimodal RAG?\n",
    "\n",
    "While Multimodal RAG pipelines are powerful, they are prone to risks such as inappropriate inputs, hallucinated outputs, or exceeding token limits. Guardrails act as safeguards, ensuring:\n",
    "- **Safety**: Prevents harmful or offensive queries and outputs.\n",
    "- **Reliability**: Validates the integrity of responses.\n",
    "- **Scalability**: Enables the pipeline to handle complex scenarios dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "### 1. Input Scanners\n",
    "Input scanners validate incoming queries before they are processed. For example:\n",
    "- **Toxicity Scanner**: Detects and blocks harmful language.\n",
    "- **Token Limit Scanner**: Ensures queries do not exceed processing limits.\n",
    "\n",
    "### 2. Custom Query Engine\n",
    "The query engine integrates retrieval and synthesis while applying guardrails at multiple stages:\n",
    "- **Pre-processing**: Validates input queries using scanners.\n",
    "- **Processing**: Retrieves relevant nodes using multimodal embeddings.\n",
    "- **Post-processing**: Sanitizes and validates outputs.\n",
    "\n",
    "### 3. Multimodal LLM\n",
    "The pipeline uses a **multimodal LLM (e.g., Gemini 1.5 Flash)** capable of understanding and generating context-aware text and image-based outputs. Its free API access makes it suitable for development without incurring significant costs.\n",
    "\n",
    "---\n",
    "\n",
    "## Guardrails Workflow\n",
    "\n",
    "### Input Validation\n",
    "1. Scans incoming queries using pre-defined scanners.\n",
    "2. Blocks or sanitizes queries based on scanner results.\n",
    "\n",
    "### Retrieval\n",
    "1. Fetches relevant text and image nodes.\n",
    "2. Converts content into embeddings for synthesis.\n",
    "\n",
    "### Output Validation\n",
    "1. Analyzes generated responses with output scanners.\n",
    "2. Blocks or sanitizes outputs based on thresholds (e.g., toxicity).\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of the Multimodal RAG with Guardrails\n",
    "1. **Improved Safety**: Queries and responses are validated to reduce risk.\n",
    "2. **Enhanced Robustness**: Multimodal inputs are processed without loss of context.\n",
    "3. **Dynamic Control**: Guardrails provide flexibility to handle diverse inputs and outputs.\n",
    "4. **Cost-Efficiency**: Selective application of input/output validation optimizes resources, while the free **Gemini 1.5 Flash** API reduces operational expenses.\n",
    "\n",
    "---\n",
    "\n",
    "This pipeline demonstrates how a **natively multimodal RAG system** can be augmented with **guardrails** to deliver secure, reliable, and high-quality results in complex document environments while remaining cost-effective through the use of free APIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8d9a7-5036-4d32-818f-00b2e888521f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ccdd53-e68a-4199-aacb-cfe71ad1ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c5556-a789-4386-a1ee-cce01dbeb6cf",
   "metadata": {},
   "source": [
    "### Setup Observability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb362db-b1b1-4eea-be1a-b1f78b0779d7",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Here we load the [Conoco Phillips 2023 investor meeting slide deck](https://static.conocophillips.com/files/2023-conocophillips-aim-presentation.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bce3407-a7d2-47e8-9eaf-ab297a94750c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "mkdir: data_images: File exists\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!mkdir data_images\n",
    "!wget \"https://static.conocophillips.com/files/2023-conocophillips-aim-presentation.pdf\" -O data/conocophillips.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d7063",
   "metadata": {},
   "source": [
    "### Install Dependency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b7b54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Using cached llama_index-0.12.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Using cached llama_index_agent_openai-0.4.0-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Using cached llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.2 (from llama-index)\n",
      "  Using cached llama_index_core-0.12.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Using cached llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_llms_openai-0.3.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
      "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Using cached llama_index_readers_file-0.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
      "  Using cached llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached openai-1.56.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached SQLAlchemy-2.0.36-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached aiohttp-3.11.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting httpx (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached httpx-0.28.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/macintosh/.local/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached numpy-2.1.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached pillow-11.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting pydantic<2.10.0,>=2.7.0 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting requests>=2.31.0 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached tiktoken-0.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/macintosh/.local/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (4.12.2)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached wrapt-1.17.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
      "  Using cached llama_cloud-0.1.6-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting pandas (from llama-index-legacy<0.10.0,>=0.9.48->llama-index)\n",
      "  Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Using cached llama_parse-0.5.15-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting click (from nltk>3.8.1->llama-index)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached frozenlist-1.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached propcache-0.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached yarl-1.18.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting anyio (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached jiter-0.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.2->llama-index)\n",
      "  Using cached marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/macintosh/.local/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/macintosh/.local/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.2->llama-index) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/macintosh/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Using cached llama_index-0.12.2-py3-none-any.whl (6.8 kB)\n",
      "Using cached llama_index_agent_openai-0.4.0-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
      "Using cached llama_index_core-0.12.2-py3-none-any.whl (1.6 MB)\n",
      "Using cached llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
      "Using cached llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
      "Using cached llama_index_llms_openai-0.3.2-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
      "Using cached llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Using cached llama_index_readers_file-0.4.0-py3-none-any.whl (38 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached aiohttp-3.11.9-cp311-cp311-macosx_11_0_arm64.whl (454 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached llama_cloud-0.1.6-py3-none-any.whl (195 kB)\n",
      "Using cached httpx-0.28.0-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached llama_parse-0.5.15-py3-none-any.whl (13 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached openai-1.56.0-py3-none-any.whl (389 kB)\n",
      "Using cached pillow-11.0.0-cp311-cp311-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached SQLAlchemy-2.0.36-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.8.0-cp311-cp311-macosx_11_0_arm64.whl (982 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.17.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached numpy-2.1.3-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.5.0-cp311-cp311-macosx_11_0_arm64.whl (52 kB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl (272 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jiter-0.8.0-cp311-cp311-macosx_11_0_arm64.whl (310 kB)\n",
      "Using cached marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached propcache-0.2.1-cp311-cp311-macosx_11_0_arm64.whl (45 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached yarl-1.18.3-cp311-cp311-macosx_11_0_arm64.whl (92 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: striprtf, pytz, filetype, dirtyjson, wrapt, urllib3, tzdata, tqdm, tenacity, SQLAlchemy, soupsieve, sniffio, regex, PyYAML, pypdf, pydantic-core, propcache, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, joblib, jiter, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, requests, pydantic, pandas, nltk, httpcore, deprecated, beautifulsoup4, anyio, aiosignal, tiktoken, httpx, dataclasses-json, aiohttp, openai, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-storage 2.18.2 requires google-api-core<3.0.0dev,>=2.15.0, which is not installed.\n",
      "google-cloud-storage 2.18.2 requires google-auth<3.0dev,>=2.26.1, which is not installed.\n",
      "google-cloud-bigquery 3.26.0 requires google-api-core[grpc]<3.0.0dev,>=2.11.1, which is not installed.\n",
      "google-cloud-bigquery 3.26.0 requires google-auth<3.0.0dev,>=2.14.1, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.2 SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.9 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.2.post1 attrs-24.2.0 beautifulsoup4-4.12.3 certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 dataclasses-json-0.6.7 deprecated-1.2.15 dirtyjson-1.0.8 distro-1.9.0 filetype-1.2.0 frozenlist-1.5.0 fsspec-2024.10.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.0 idna-3.10 jiter-0.8.0 joblib-1.4.2 llama-cloud-0.1.6 llama-index-0.12.2 llama-index-agent-openai-0.4.0 llama-index-cli-0.4.0 llama-index-core-0.12.2 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.3.2 llama-index-multi-modal-llms-openai-0.3.0 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.0 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.15 marshmallow-3.23.1 multidict-6.1.0 mypy-extensions-1.0.0 networkx-3.4.2 nltk-3.9.1 numpy-2.1.3 openai-1.56.0 pandas-2.2.3 pillow-11.0.0 propcache-0.2.1 pydantic-2.9.2 pydantic-core-2.23.4 pypdf-5.1.0 pytz-2024.2 regex-2024.11.6 requests-2.32.3 sniffio-1.3.1 soupsieve-2.6 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.8.0 tqdm-4.67.1 typing-inspect-0.9.0 tzdata-2024.2 urllib3-2.2.3 wrapt-1.17.0 yarl-1.18.3\n",
      "Requirement already satisfied: llama-parse in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (0.5.15)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-parse) (8.1.7)\n",
      "Requirement already satisfied: llama-index-core>=0.11.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-parse) (0.12.2)\n",
      "Requirement already satisfied: pydantic!=2.10 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-parse) (2.9.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama-parse) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (3.11.9)\n",
      "Requirement already satisfied: dataclasses-json in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (2024.10.0)\n",
      "Requirement already satisfied: httpx in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (0.28.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/macintosh/.local/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (2.1.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (11.0.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/macintosh/.local/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core>=0.11.0->llama-parse) (1.17.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic!=2.10->llama-parse) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic!=2.10->llama-parse) (2.23.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-parse) (1.18.3)\n",
      "Requirement already satisfied: joblib in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core>=0.11.0->llama-parse) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core>=0.11.0->llama-parse) (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-parse) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-parse) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-parse) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-parse) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama-parse) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core>=0.11.0->llama-parse) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from dataclasses-json->llama-index-core>=0.11.0->llama-parse) (3.23.1)\n",
      "Requirement already satisfied: anyio in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core>=0.11.0->llama-parse) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core>=0.11.0->llama-parse) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core>=0.11.0->llama-parse) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/macintosh/.local/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.11.0->llama-parse) (24.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from anyio->httpx->llama-index-core>=0.11.0->llama-parse) (1.3.1)\n",
      "Collecting llama-index-llms-langchain\n",
      "  Using cached llama_index_llms_langchain-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting langchain>=0.1.3 (from llama-index-llms-langchain)\n",
      "  Using cached langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-llms-langchain) (0.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (3.11.9)\n",
      "Collecting langchain-core<0.4.0,>=0.3.21 (from langchain>=0.1.3->llama-index-llms-langchain)\n",
      "  Using cached langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain>=0.1.3->llama-index-llms-langchain)\n",
      "  Using cached langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain>=0.1.3->llama-index-llms-langchain)\n",
      "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy<2,>=1.22.4 (from langchain>=0.1.3->llama-index-llms-langchain)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (2024.10.0)\n",
      "Requirement already satisfied: httpx in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (0.28.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/macintosh/.local/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (3.9.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (11.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/macintosh/.local/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (1.18.3)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.21->langchain>=0.1.3->llama-index-llms-langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/macintosh/.local/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain>=0.1.3->llama-index-llms-langchain) (24.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain>=0.1.3->llama-index-llms-langchain)\n",
      "  Using cached orjson-3.10.12-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain>=0.1.3->llama-index-llms-langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: anyio in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.3->llama-index-llms-langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.3->llama-index-llms-langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.1.3->llama-index-llms-langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.1.3->llama-index-llms-langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (3.23.1)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain>=0.1.3->llama-index-llms-langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-langchain) (1.3.1)\n",
      "Using cached llama_index_llms_langchain-0.5.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached langchain-0.3.9-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
      "Using cached langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Using cached langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached orjson-3.10.12-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (248 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: orjson, numpy, jsonpointer, requests-toolbelt, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain, llama-index-llms-langchain\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.9 langchain-core-0.3.21 langchain-text-splitters-0.3.2 langsmith-0.1.147 llama-index-llms-langchain-0.5.0 numpy-1.26.4 orjson-3.10.12 requests-toolbelt-1.0.0\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Using cached llama_index_embeddings_huggingface-0.4.0-py3-none-any.whl.metadata (767 bytes)\n",
      "Collecting huggingface-hub>=0.19.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Using cached huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-embeddings-huggingface) (0.12.2)\n",
      "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/macintosh/.local/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/macintosh/.local/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n",
      "Requirement already satisfied: aiohttp in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.11.9)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.0.36)\n",
      "Requirement already satisfied: dataclasses-json in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.2.0)\n",
      "Requirement already satisfied: httpx in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.28.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/macintosh/.local/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (11.0.0)\n",
      "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.17.0)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached torch-2.5.1-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.18.3)\n",
      "Requirement already satisfied: click in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.1.1)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached tokenizers-0.20.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached safetensors-0.4.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.23.1)\n",
      "Requirement already satisfied: anyio in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.14.0)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Using cached llama_index_embeddings_huggingface-0.4.0-py3-none-any.whl (8.6 kB)\n",
      "Using cached huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Using cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Using cached torch-2.5.1-cp311-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Using cached scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
      "Using cached safetensors-0.4.5-cp311-cp311-macosx_11_0_arm64.whl (381 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.20.3-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, MarkupSafe, filelock, scikit-learn, jinja2, huggingface-hub, torch, tokenizers, transformers, sentence-transformers, llama-index-embeddings-huggingface\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 huggingface-hub-0.26.3 jinja2-3.1.4 llama-index-embeddings-huggingface-0.4.0 mpmath-1.3.0 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.3.1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.20.3 torch-2.5.1 transformers-4.46.3\n",
      "Collecting llama-index-llms-gemini\n",
      "  Using cached llama_index_llms_gemini-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting google-generativeai<0.6.0,>=0.5.2 (from llama-index-llms-gemini)\n",
      "  Using cached google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-llms-gemini) (0.12.2)\n",
      "Collecting pillow<11.0.0,>=10.2.0 (from llama-index-llms-gemini)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.4 (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached google_ai_generativelanguage-0.6.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-api-core (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached google_api_core-2.23.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached google_api_python_client-2.154.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting protobuf (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached protobuf-5.29.0-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: pydantic in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (2.9.2)\n",
      "Requirement already satisfied: tqdm in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/macintosh/.local/lib/python3.11/site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached protobuf-4.25.5-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (3.11.9)\n",
      "Requirement already satisfied: dataclasses-json in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (2024.10.0)\n",
      "Requirement already satisfied: httpx in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (0.28.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/macintosh/.local/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.18.3)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: click in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (3.23.1)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: anyio in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (0.14.0)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached grpcio-1.68.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/macintosh/.local/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (24.1)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-gemini) (1.3.1)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached llama_index_llms_gemini-0.4.0-py3-none-any.whl (6.1 kB)\n",
      "Using cached google_generativeai-0.5.4-py3-none-any.whl (150 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.4-py3-none-any.whl (679 kB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached google_api_core-2.23.0-py3-none-any.whl (156 kB)\n",
      "Using cached google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "Using cached protobuf-4.25.5-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached google_api_python_client-2.154.0-py2.py3-none-any.whl (12.6 MB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Using cached grpcio-1.68.1-cp311-cp311-macosx_10_9_universal2.whl (11.2 MB)\n",
      "Using cached grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: uritemplate, pyparsing, pyasn1, protobuf, pillow, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai, llama-index-llms-gemini\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.0.0\n",
      "    Uninstalling pillow-11.0.0:\n",
      "      Successfully uninstalled pillow-11.0.0\n",
      "Successfully installed cachetools-5.5.0 google-ai-generativelanguage-0.6.4 google-api-core-2.23.0 google-api-python-client-2.154.0 google-auth-2.36.0 google-auth-httplib2-0.2.0 google-generativeai-0.5.4 googleapis-common-protos-1.66.0 grpcio-1.68.1 grpcio-status-1.62.3 httplib2-0.22.0 llama-index-llms-gemini-0.4.0 pillow-10.4.0 proto-plus-1.25.0 protobuf-4.25.5 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyparsing-3.2.0 rsa-4.9 uritemplate-4.1.1\n",
      "Collecting llama-index-multi-modal-llms-gemini\n",
      "  Using cached llama_index_multi_modal_llms_gemini-0.4.1-py3-none-any.whl.metadata (767 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-multi-modal-llms-gemini) (0.12.2)\n",
      "Requirement already satisfied: llama-index-llms-gemini<0.5.0,>=0.4.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-multi-modal-llms-gemini) (0.4.0)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-multi-modal-llms-gemini) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (3.11.9)\n",
      "Requirement already satisfied: dataclasses-json in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (2024.10.0)\n",
      "Requirement already satisfied: httpx in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (0.28.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/macintosh/.local/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.26.4)\n",
      "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/macintosh/.local/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.17.0)\n",
      "Requirement already satisfied: google-generativeai<0.6.0,>=0.5.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (0.5.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.18.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (0.6.4)\n",
      "Requirement already satisfied: google-api-core in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (2.23.0)\n",
      "Requirement already satisfied: google-api-python-client in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (2.154.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (2.36.0)\n",
      "Requirement already satisfied: protobuf in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (4.25.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (1.25.0)\n",
      "Requirement already satisfied: click in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (3.23.1)\n",
      "Requirement already satisfied: anyio in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (0.14.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (1.66.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (4.9)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/macintosh/.local/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (24.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-gemini) (1.3.1)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (4.1.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (1.68.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (1.62.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini<0.5.0,>=0.4.0->llama-index-multi-modal-llms-gemini) (0.6.1)\n",
      "Using cached llama_index_multi_modal_llms_gemini-0.4.1-py3-none-any.whl (4.6 kB)\n",
      "Installing collected packages: llama-index-multi-modal-llms-gemini\n",
      "Successfully installed llama-index-multi-modal-llms-gemini-0.4.1\n",
      "Collecting litellm\n",
      "  Using cached litellm-1.53.2-py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: aiohttp in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from litellm) (3.11.9)\n",
      "Requirement already satisfied: click in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from litellm) (8.1.7)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm)\n",
      "  Using cached importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from litellm) (3.1.4)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0 (from litellm)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: openai>=1.54.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from litellm) (1.56.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from litellm) (2.9.2)\n",
      "Collecting python-dotenv>=0.2.0 (from litellm)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from litellm) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from litellm) (0.8.0)\n",
      "Requirement already satisfied: tokenizers in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from litellm) (0.20.3)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm)\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (24.2.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Using cached rpds_py-0.22.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from openai>=1.54.0->litellm) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from openai>=1.54.0->litellm) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from openai>=1.54.0->litellm) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from openai>=1.54.0->litellm) (0.8.0)\n",
      "Requirement already satisfied: sniffio in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from openai>=1.54.0->litellm) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from openai>=1.54.0->litellm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/macintosh/.local/lib/python3.11/site-packages (from openai>=1.54.0->litellm) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (2024.8.30)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->litellm) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->litellm) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->litellm) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->litellm) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->litellm) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from aiohttp->litellm) (1.18.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from tokenizers->litellm) (0.26.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.54.0->litellm) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.54.0->litellm) (0.14.0)\n",
      "Requirement already satisfied: filelock in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/macintosh/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
      "Using cached litellm-1.53.2-py3-none-any.whl (6.4 MB)\n",
      "Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.22.0-cp311-cp311-macosx_11_0_arm64.whl (345 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: zipp, rpds-py, python-dotenv, referencing, importlib-metadata, jsonschema-specifications, jsonschema, litellm\n",
      "Successfully installed importlib-metadata-8.5.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 litellm-1.53.2 python-dotenv-1.0.1 referencing-0.35.1 rpds-py-0.22.0 zipp-3.21.0\n",
      "Collecting llm-guard\n",
      "  Using cached llm_guard-0.3.15-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting bc-detect-secrets==1.5.15 (from llm-guard)\n",
      "  Using cached bc_detect_secrets-1.5.15-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting faker<28,>=26.0.0 (from llm-guard)\n",
      "  Using cached Faker-27.4.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting fuzzysearch<0.9,>=0.7 (from llm-guard)\n",
      "  Using cached fuzzysearch-0.7.3-py3-none-any.whl\n",
      "Collecting json-repair<0.29,>=0.25.2 (from llm-guard)\n",
      "  Using cached json_repair-0.28.4-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: nltk<4,>=3.9.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llm-guard) (3.9.1)\n",
      "Collecting presidio-analyzer==2.2.354 (from llm-guard)\n",
      "  Using cached presidio_analyzer-2.2.354-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting presidio-anonymizer==2.2.354 (from llm-guard)\n",
      "  Using cached presidio_anonymizer-2.2.354-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting regex==2024.7.24 (from llm-guard)\n",
      "  Using cached regex-2024.7.24-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tiktoken<0.8,>=0.5 (from llm-guard)\n",
      "  Using cached tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: torch>=2.4.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llm-guard) (2.5.1)\n",
      "Requirement already satisfied: transformers>=4.43.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from llm-guard) (4.46.3)\n",
      "Collecting structlog>=24 (from llm-guard)\n",
      "  Using cached structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting oldest-supported-numpy (from llm-guard)\n",
      "  Using cached oldest_supported_numpy-2023.12.21-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: pyyaml in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from bc-detect-secrets==1.5.15->llm-guard) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from bc-detect-secrets==1.5.15->llm-guard) (2.32.3)\n",
      "Collecting unidiff (from bc-detect-secrets==1.5.15->llm-guard)\n",
      "  Using cached unidiff-0.7.5-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting spacy<4.0.0,>=3.4.4 (from presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached spacy-3.8.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting tldextract (from presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting phonenumbers<9.0.0,>=8.12 (from presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached phonenumbers-8.13.51-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pycryptodome>=3.10.1 (from presidio-anonymizer==2.2.354->llm-guard)\n",
      "  Using cached pycryptodome-3.21.0-cp36-abi3-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /Users/macintosh/.local/lib/python3.11/site-packages (from faker<28,>=26.0.0->llm-guard) (2.9.0.post0)\n",
      "Requirement already satisfied: attrs>=19.3 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from fuzzysearch<0.9,>=0.7->llm-guard) (24.2.0)\n",
      "Requirement already satisfied: click in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk<4,>=3.9.1->llm-guard) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk<4,>=3.9.1->llm-guard) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from nltk<4,>=3.9.1->llm-guard) (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from torch>=2.4.0->llm-guard) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/macintosh/.local/lib/python3.11/site-packages (from torch>=2.4.0->llm-guard) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from torch>=2.4.0->llm-guard) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from torch>=2.4.0->llm-guard) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from torch>=2.4.0->llm-guard) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from torch>=2.4.0->llm-guard) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.4.0->llm-guard) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from transformers>=4.43.4->llm-guard) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from transformers>=4.43.4->llm-guard) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/macintosh/.local/lib/python3.11/site-packages (from transformers>=4.43.4->llm-guard) (24.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from transformers>=4.43.4->llm-guard) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from transformers>=4.43.4->llm-guard) (0.4.5)\n",
      "Collecting numpy>=1.17 (from transformers>=4.43.4->llm-guard)\n",
      "  Using cached numpy-1.23.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/macintosh/.local/lib/python3.11/site-packages (from python-dateutil>=2.4->faker<28,>=26.0.0->llm-guard) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests->bc-detect-secrets==1.5.15->llm-guard) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests->bc-detect-secrets==1.5.15->llm-guard) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests->bc-detect-secrets==1.5.15->llm-guard) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from requests->bc-detect-secrets==1.5.15->llm-guard) (2024.8.30)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached murmurhash-1.0.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached cymem-2.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached thinc-8.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached typer-0.14.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (2.9.2)\n",
      "Requirement already satisfied: setuptools in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (75.1.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from jinja2->torch>=2.4.0->llm-guard) (3.0.2)\n",
      "Collecting requests-file>=1.4 (from tldextract->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (2.23.4)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached blis-1.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached thinc-8.3.1.tar.gz (193 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached thinc-8.3.0.tar.gz (193 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting spacy<4.0.0,>=3.4.4 (from presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached spacy-3.7.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached thinc-8.2.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached marisa_trie-1.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/macintosh/.local/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (1.17.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached llm_guard-0.3.15-py3-none-any.whl (138 kB)\n",
      "Using cached bc_detect_secrets-1.5.15-py3-none-any.whl (119 kB)\n",
      "Using cached presidio_analyzer-2.2.354-py3-none-any.whl (92 kB)\n",
      "Using cached presidio_anonymizer-2.2.354-py3-none-any.whl (31 kB)\n",
      "Using cached regex-2024.7.24-cp311-cp311-macosx_11_0_arm64.whl (278 kB)\n",
      "Using cached Faker-27.4.0-py3-none-any.whl (1.8 MB)\n",
      "Using cached json_repair-0.28.4-py3-none-any.whl (13 kB)\n",
      "Using cached structlog-24.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl (907 kB)\n",
      "Using cached oldest_supported_numpy-2023.12.21-py3-none-any.whl (4.9 kB)\n",
      "Using cached numpy-1.23.2-cp311-cp311-macosx_11_0_arm64.whl (13.3 MB)\n",
      "Using cached phonenumbers-8.13.51-py2.py3-none-any.whl (2.6 MB)\n",
      "Using cached pycryptodome-3.21.0-cp36-abi3-macosx_10_9_universal2.whl (2.5 MB)\n",
      "Using cached spacy-3.7.5-cp311-cp311-macosx_11_0_arm64.whl (6.5 MB)\n",
      "Using cached thinc-8.2.5-cp311-cp311-macosx_11_0_arm64.whl (773 kB)\n",
      "Using cached tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
      "Using cached unidiff-0.7.5-py2.py3-none-any.whl (14 kB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.10-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.11-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
      "Using cached preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl (128 kB)\n",
      "Using cached requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl (488 kB)\n",
      "Using cached typer-0.14.0-py3-none-any.whl (44 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
      "Using cached cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Using cached marisa_trie-1.2.1-cp311-cp311-macosx_11_0_arm64.whl (174 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: unidiff, phonenumbers, cymem, wasabi, structlog, spacy-loggers, spacy-legacy, smart-open, shellingham, regex, pycryptodome, numpy, murmurhash, mdurl, marisa-trie, json-repair, fuzzysearch, cloudpathlib, catalogue, tiktoken, srsly, requests-file, presidio-anonymizer, preshed, oldest-supported-numpy, markdown-it-py, language-data, faker, blis, bc-detect-secrets, tldextract, rich, langcodes, confection, typer, thinc, weasel, spacy, presidio-analyzer, llm-guard\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.11.6\n",
      "    Uninstalling regex-2024.11.6:\n",
      "      Successfully uninstalled regex-2024.11.6\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.8.0\n",
      "    Uninstalling tiktoken-0.8.0:\n",
      "      Successfully uninstalled tiktoken-0.8.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scipy 1.14.1 requires numpy<2.3,>=1.23.5, but you have numpy 1.23.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bc-detect-secrets-1.5.15 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.10 faker-27.4.0 fuzzysearch-0.7.3 json-repair-0.28.4 langcodes-3.5.0 language-data-1.3.0 llm-guard-0.3.15 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.11 numpy-1.23.2 oldest-supported-numpy-2023.12.21 phonenumbers-8.13.51 preshed-3.0.9 presidio-analyzer-2.2.354 presidio-anonymizer-2.2.354 pycryptodome-3.21.0 regex-2024.7.24 requests-file-2.1.0 rich-13.9.4 shellingham-1.5.4 smart-open-7.0.5 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 structlog-24.4.0 thinc-8.2.5 tiktoken-0.7.0 tldextract-5.1.3 typer-0.14.0 unidiff-0.7.5 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-parse\n",
    "!pip install llama-index-llms-langchain\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-llms-gemini\n",
    "!pip install llama-index-multi-modal-llms-gemini\n",
    "!pip install litellm\n",
    "!pip install llm-guard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ba6b0-51af-42f9-b1b2-8d3e721ef782",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "Setup models that will be used for downstream orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e2071d-bbc2-4707-8ae7-cb4e1fecafd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages/nltk/metrics/association.py:26: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.23.2)\n",
      "  from scipy.stats import fisher_exact\n",
      "/Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/.conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in GeminiMultiModal has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.multi_modal_llms.gemini import GeminiMultiModal\n",
    "import os\n",
    "LlamaCloud_API_KEY=\"\"\n",
    "MultiGeminiKey=\"\"\n",
    "GOOGLE_API_KEY=\"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"GEMINI_API_KEY\"] = GOOGLE_API_KEY\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "gemini_multimodal = GeminiMultiModal(model_name=\"models/gemini-1.5-flash\", api_key=MultiGeminiKey)\n",
    "api_key = GOOGLE_API_KEY\n",
    "llamaAPI_KEY = LlamaCloud_API_KEY\n",
    "llm = Gemini(model=\"models/gemini-1.5-flash\",api_key=api_key)\n",
    "Settings.llm = llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6416f-f580-4722-aaa9-7f3500408547",
   "metadata": {},
   "source": [
    "## Use LlamaParse to Parse Text and Images\n",
    "\n",
    "In this example, use LlamaParse to parse both the text and images from the document.\n",
    "\n",
    "We parse out the text in two ways: \n",
    "- in regular `text` mode using our default text layout algorithm\n",
    "- in `markdown` mode using GPT-4o (`gpt4o_mode=True`). This also allows us to capture page screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570089e5-238a-4dcc-af65-96e7393c2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "\n",
    "parser_text = LlamaParse(result_type=\"text\",api_key=llamaAPI_KEY)\n",
    "parser_gpt4o = LlamaParse(result_type=\"markdown\", gpt4o_mode=True,api_key=llamaAPI_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef82a985-4088-4bb7-9a21-0318e1b9207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing text...\n",
      "Started parsing the file under job_id e79a470b-e8d3-4f55-a048-d1b3d81b6d1e\n",
      "Parsing PDF file...\n",
      "Started parsing the file under job_id 84943607-b630-45bd-bf89-8470840e73b5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parsing text...\")\n",
    "docs_text = parser_text.load_data(\"data/conocophillips.pdf\")\n",
    "print(f\"Parsing PDF file...\")\n",
    "md_json_objs = parser_gpt4o.get_json_result(\"data/conocophillips.pdf\")\n",
    "md_json_list = md_json_objs[0][\"pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5318fb7b-fe6a-4a8a-b82e-4ed7b4512c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Commitment to Disciplined Reinvestment Rate\n",
      "\n",
      "| Period       | Description                        | Reinvestment Rate | WTI Average |\n",
      "|--------------|------------------------------------|-------------------|-------------|\n",
      "| 2012-2016    | Industry Growth Focus              | >100%             | ~$75/BBL    |\n",
      "| 2017-2022    | ConocoPhillips Strategy Reset      | <60%              | ~$63/BBL    |\n",
      "| 2023E        |                                    |                   | at $80/BBL  |\n",
      "| 2024-2028    | Disciplined Reinvestment Rate      | ~50%              | at $60/BBL  |\n",
      "| 2029-2032    |                                    | ~6% CFO CAGR      | at $60/BBL  |\n",
      "\n",
      "- **Historic Reinvestment Rate**: Shown in gray.\n",
      "- **Reinvestment Rate at $60/BBL WTI**: Shown in blue.\n",
      "- **Reinvestment Rate at $80/BBL WTI**: Shown with dashed lines.\n",
      "\n",
      "**Note**: Reinvestment rate and cash from operations (CFO) are non-GAAP measures. Definitions and reconciliations are included in the Appendix.\n"
     ]
    }
   ],
   "source": [
    "print(md_json_list[10][\"md\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeadb16c-97eb-4622-9551-b34d7f90d72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Image for page 1: [{'name': 'page_1.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 2: [{'name': 'page_2.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 3: [{'name': 'page_3.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 4: [{'name': 'page_4.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 5: [{'name': 'page_5.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 6: [{'name': 'page_6.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 7: [{'name': 'page_7.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 8: [{'name': 'page_8.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 9: [{'name': 'page_9.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 10: [{'name': 'page_10.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 11: [{'name': 'page_11.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 12: [{'name': 'page_12.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 13: [{'name': 'page_13.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 14: [{'name': 'page_14.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 15: [{'name': 'page_15.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 16: [{'name': 'page_16.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 17: [{'name': 'page_17.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 18: [{'name': 'page_18.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 19: [{'name': 'page_19.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 20: [{'name': 'page_20.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 21: [{'name': 'page_21.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 22: [{'name': 'page_22.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 23: [{'name': 'page_23.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 24: [{'name': 'page_24.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 25: [{'name': 'page_25.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 26: [{'name': 'page_26.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 27: [{'name': 'page_27.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 28: [{'name': 'page_28.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 29: [{'name': 'page_29.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 30: [{'name': 'page_30.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 31: [{'name': 'page_31.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 32: [{'name': 'page_32.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 33: [{'name': 'page_33.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 34: [{'name': 'page_34.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 35: [{'name': 'page_35.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 36: [{'name': 'page_36.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 37: [{'name': 'page_37.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 38: [{'name': 'page_38.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 39: [{'name': 'page_39.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 40: [{'name': 'page_40.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 41: [{'name': 'page_41.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 42: [{'name': 'page_42.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 43: [{'name': 'page_43.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 44: [{'name': 'page_44.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 45: [{'name': 'page_45.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 46: [{'name': 'page_46.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 47: [{'name': 'page_47.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 48: [{'name': 'page_48.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 49: [{'name': 'page_49.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 50: [{'name': 'page_50.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 51: [{'name': 'page_51.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 52: [{'name': 'page_52.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 53: [{'name': 'page_53.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 54: [{'name': 'page_54.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 55: [{'name': 'page_55.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 56: [{'name': 'page_56.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 57: [{'name': 'page_57.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 58: [{'name': 'page_58.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 59: [{'name': 'page_59.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 60: [{'name': 'page_60.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 61: [{'name': 'page_61.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n",
      "> Image for page 62: [{'name': 'page_62.jpg', 'height': 0, 'width': 0, 'x': 0, 'y': 0, 'type': 'full_page_screenshot'}]\n"
     ]
    }
   ],
   "source": [
    "image_dicts = parser_gpt4o.get_images(md_json_objs, download_path=\"data_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e098b-0606-4429-b48d-d4fe0140fc0e",
   "metadata": {},
   "source": [
    "## Build Multimodal Index\n",
    "\n",
    "In this section we build the multimodal index over the parsed deck. \n",
    "\n",
    "We do this by creating **text** nodes from the document that contain metadata referencing the original image path.\n",
    "\n",
    "In this example we're indexing the text node for retrieval. The text node has a reference to both the parsed text as well as the image screenshot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae2dee-9d85-4604-8a51-705d4db527f7",
   "metadata": {},
   "source": [
    "#### Get Text Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18c24174-05ce-417f-8dd2-79c3f375db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e331dfe-a627-4e23-8c57-70ab1d9342e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pages loaded through llamaparse\n",
    "import re\n",
    "def get_page_number(file_name):\n",
    "    match = re.search(r\"-page-(\\d+)\\.jpg$\", str(file_name))\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def _get_sorted_image_files(image_dir):\n",
    "    \"\"\"Get image files sorted by page.\"\"\"\n",
    "    raw_files = [f for f in list(Path(image_dir).iterdir()) if f.is_file()]\n",
    "    sorted_files = sorted(raw_files, key=get_page_number)\n",
    "    return sorted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "346fe5ef-171e-4a54-9084-7a7805103a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming TextNode class is defined somewhere else in your code\n",
    "# Attach image metadata to the text nodes\n",
    "def get_text_nodes(docs, image_dir=None, json_dicts=None):\n",
    "    \"\"\"Split docs into nodes, by separator.\"\"\"\n",
    "    nodes = []\n",
    "\n",
    "    # Get image files (if provided)\n",
    "    image_files = _get_sorted_image_files(image_dir) if image_dir is not None else None\n",
    "\n",
    "    # Get markdown texts (if provided)\n",
    "    md_texts = [d[\"md\"] for d in json_dicts] if json_dicts is not None else None\n",
    "\n",
    "    # Split docs into chunks by separator\n",
    "    doc_chunks = [c for d in docs for c in d.text.split(\"---\")]\n",
    "\n",
    "    # Handle both single-page and multi-page cases\n",
    "    for idx, doc_chunk in enumerate(doc_chunks):\n",
    "        chunk_metadata = {\"page_num\": idx + 1}\n",
    "        \n",
    "        # Check if there are image files and handle the single-page case\n",
    "        if image_files is not None:\n",
    "            # Use the first image file if there's only one\n",
    "            image_file = image_files[idx] if idx < len(image_files) else image_files[0]\n",
    "            chunk_metadata[\"image_path\"] = str(image_file)\n",
    "        \n",
    "        # Check if there are markdown texts and handle the single-page case\n",
    "        if md_texts is not None:\n",
    "            # Use the first markdown text if there's only one\n",
    "            parsed_text_md = md_texts[idx] if idx < len(md_texts) else md_texts[0]\n",
    "            chunk_metadata[\"parsed_text_markdown\"] = parsed_text_md\n",
    "\n",
    "        # Add the chunk text as metadata\n",
    "        chunk_metadata[\"parsed_text\"] = doc_chunk\n",
    "\n",
    "        # Create the TextNode with the parsed text and metadata\n",
    "        node = TextNode(\n",
    "            text=\"\",\n",
    "            metadata=chunk_metadata,\n",
    "        )\n",
    "        nodes.append(node)\n",
    "\n",
    "    return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f591669c-5a8e-491d-9cef-0b754abbf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# this will split into pages\n",
    "text_nodes = get_text_nodes(docs_text, image_dir=\"/Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/llama_index/docs/docs/examples/rag_guardrail/data_images\", json_dicts=md_json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32c13950-c1db-435f-b5b4-89d62b8b7744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_num: 1\n",
      "image_path: /Users/macintosh/TA-DOCUMENT/StudyZone/ComputerScience/Artificial Intelligence/Llama_index/llama_index/docs/docs/examples/rag_guardrail/data_images/84943607-b630-45bd-bf89-8470840e73b5-page_51.jpg\n",
      "parsed_text_markdown: NO_CONTENT_HERE\n",
      "parsed_text: ConocoPhillips\n",
      "                2023 Analyst & Investor Meeting\n"
     ]
    }
   ],
   "source": [
    "print(text_nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f404f56-db1e-4ed7-9ba1-ead763546348",
   "metadata": {},
   "source": [
    "#### Build Index\n",
    "\n",
    "Once the text nodes are ready, we feed into our vector store index abstraction, which will index these nodes into a simple in-memory vector store (of course, you should definitely check out our 40+ vector store integrations!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea53c31-0e38-421c-8d9b-0e3adaa1677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"storage_nodes\"):\n",
    "    index = VectorStoreIndex(text_nodes, embed_model=embed_model)\n",
    "    # save index to disk\n",
    "    index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(\"./storage_nodes\")\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage_nodes\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "\n",
    "retriever = index.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a536f",
   "metadata": {},
   "source": [
    "## Build Guardrail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a05efd",
   "metadata": {},
   "source": [
    "Define the global rail output formal for guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89dc0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def result_response(guardrail_type, activated, guard_output, is_valid,risk_score,threshold, response_text):\n",
    "    \"\"\"\n",
    "    Standardizes the result format for all guardrail checks.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"guardrail_type\": guardrail_type,\n",
    "        \"activated\": activated,\n",
    "        \"guardrail_detail\": {\n",
    "            \"guard_output\": guard_output,\n",
    "            \"is_valid\": is_valid,\n",
    "            \"risk_score/threshold\": f\"{risk_score}/{threshold}\",\n",
    "            \"response_text\": response_text\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ba30e",
   "metadata": {},
   "source": [
    "## Adding scanner for Guardrail. \n",
    "Here you can visit \"https://llm-guard.com\" for discover the nessessary scanner. Below is 2 example scanners you should follow this format function. \n",
    "Here we use Gemini 1.5 Flash for the response of Scanner. Hence, you can choose your own LLMs such as ChatGPT, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ff28f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from llm_guard.input_scanners import Toxicity\n",
    "from llm_guard.input_scanners.toxicity import MatchType\n",
    "from litellm import completion\n",
    "\n",
    "def guardrail_toxicLanguage(prompt):\n",
    "    # Interact with the LLM to generate a response\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    # Generate the response using the LLM (Gemini-1.5-flash)\n",
    "    response = completion(\n",
    "        model=\"gemini/gemini-1.5-flash\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    response_text = response.choices[0].message.content\n",
    "    \n",
    "    # Define the threshold and scan for toxicity\n",
    "    threshold = 0.5\n",
    "    toxic_scanner = Toxicity(threshold=threshold, match_type=MatchType.FULL)\n",
    "    sanitized_output, is_valid, risk_score = toxic_scanner.scan(prompt)\n",
    "    \n",
    "    return result_response(\n",
    "        guardrail_type=\"Toxicity\",\n",
    "        activated=not is_valid,\n",
    "        guard_output=sanitized_output,\n",
    "        is_valid=is_valid,\n",
    "        risk_score=risk_score,\n",
    "        threshold=threshold,\n",
    "        response_text=response_text\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdcea84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_guard.input_scanners import TokenLimit\n",
    "from llm_guard import scan_output\n",
    "from litellm import completion\n",
    "def guardrail_tokenlimit(prompt):\n",
    "    threshold = 400\n",
    "    response = completion(\n",
    "        model=\"gemini/gemini-1.5-flash\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    response_text = response.choices[0].message.content\n",
    "    \n",
    "    scanner = TokenLimit(limit=threshold, encoding_name=\"cl100k_base\")\n",
    "    sanitized_output, is_valid, risk_score = scanner.scan(prompt)\n",
    "    \n",
    "    # Use the global rail to format the result\n",
    "    result = result_response(\n",
    "        guardrail_type=\"Token limit\",\n",
    "        activated=not is_valid,\n",
    "        guard_output=sanitized_output,\n",
    "        is_valid=is_valid,\n",
    "        risk_score=risk_score,\n",
    "        threshold=threshold,\n",
    "        response_text=response_text\n",
    "    )\n",
    "    \n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e9430",
   "metadata": {},
   "source": [
    "### `InputScanner` - `OutputScanner` Function\n",
    "\n",
    "The `InputScanner` function runs a series of scanners on a given input query and evaluates whether any of them detect a threat. It returns a boolean value indicating whether a threat was detected and a list of results from scanners that returned a positive detection.\n",
    "\n",
    "#### Parameters:\n",
    "- `query` (*str*): The input to be scanned for potential threats.\n",
    "- `listOfScanners` (*list*): A list of scanner functions. Each scanner function should accept the query as input and return a dictionary with a key `\"activated\"` (boolean) to indicate whether a threat was detected.\n",
    "\n",
    "#### Returns:\n",
    "- `detected` (*bool*): `True` if any scanner detects a threat, otherwise `False`.\n",
    "- `triggered_scanners` (*list*): A list of dictionaries returned by the scanners that detected a threat.\n",
    "\n",
    "#### Key Steps:\n",
    "1. Initialize `detected` to `False` to track if any scanner finds a threat.\n",
    "2. Create an empty list `triggered_scanners` to store results from scanners that detect a threat.\n",
    "3. Iterate over each scanner in `listOfScanners`:\n",
    "   - Run the scanner on the `query`.\n",
    "   - Check if the scanner's result includes `\"activated\": True`.\n",
    "   - If a threat is detected:\n",
    "     - Set `detected` to `True`.\n",
    "     - Append the scanner's result to `triggered_scanners`.\n",
    "4. Return the `detected` status and the list of `triggered_scanners`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "890da9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputScanner(query, listOfScanners):\n",
    "    \"\"\"\n",
    "    Runs all scanners on the query and returns:\n",
    "    - True if any scanner detects a threat.\n",
    "    - A list of results from scanners that returned True.\n",
    "    \"\"\"\n",
    "    detected = False  # Track if any scanner detects a threat\n",
    "    triggered_scanners = []  # Store results from triggered scanners\n",
    "\n",
    "    # Run each scanner on the query\n",
    "    for scanner in listOfScanners:\n",
    "        result = scanner(query)  \n",
    "        \n",
    "        if result[\"activated\"]:  # Check if the scanner found a threat (activated=True)\n",
    "            detected = True  # Set detected to True if any scanner triggers\n",
    "            triggered_scanners.append(result)  # Track which scanner triggered\n",
    "\n",
    "    return detected, triggered_scanners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfde0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputScanner(response, query, context, listOfScanners):\n",
    "    \"\"\"\n",
    "    Runs all scanners on the response and returns:\n",
    "    - True if any scanner detects a threat.\n",
    "    - A list of results from scanners that returned True.\n",
    "    \"\"\"\n",
    "    detected = False  # Track if any scanner detects a threat\n",
    "    triggered_scanners = []  # Store results from triggered scanners\n",
    "\n",
    "    # Run each scanner on the response\n",
    "    for scanner in listOfScanners:\n",
    "        # Check if scanner is `evaluate_rag_response` (which needs query & context)\n",
    "        if scanner.__name__ == \"evaluate_rag_response\":\n",
    "            result = scanner(response, query, context)  # Execute with query & context\n",
    "        else:\n",
    "            result = scanner(response)  # Default scanner execution\n",
    "        \n",
    "        # print(f\"Debug Output Scanner Result: {result}\")\n",
    "\n",
    "        if result[\"activated\"]:  # Check if the scanner was triggered\n",
    "            detected = True\n",
    "            triggered_scanners.append(result)  # Track which scanner triggered\n",
    "\n",
    "    return detected, triggered_scanners\n",
    "\n",
    "# Example usage with a query engine response\n",
    "# scanners = [detect_and_anonymize_pii]\n",
    "# response = query_engine.query(\"Give me account name of Peter Kelly and Role and Credit Card Number\")\n",
    "# detected, triggered_scanners = OutputScanner(str(response), scanners)\n",
    "# print(triggered_scanners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e33a4-9422-498d-87ee-d917bdf74d80",
   "metadata": {},
   "source": [
    "## Custom Multimodal Query Engine\n",
    "\n",
    "This custom query engine extends standard retrieval-based architectures to handle both text and image data, enabling more comprehensive and context-aware responses. It integrates multimodal reasoning and incorporates advanced input and output validation mechanisms for robust query handling.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Multimodal Support**:\n",
    "   - Combines text and image data to generate more informed and accurate responses.\n",
    "\n",
    "2. **Input and Output Validation**:\n",
    "   - Scans input queries for sensitive or invalid content and blocks them if necessary.\n",
    "   - Validates and sanitizes generated responses to ensure compliance with predefined rules.\n",
    "\n",
    "3. **Context-Aware Prompting**:\n",
    "   - Retrieves relevant data and constructs a context string for the query.\n",
    "   - Uses this context to guide the response synthesis process.\n",
    "\n",
    "4. **Metadata and Logging**:\n",
    "   - Tracks the query process, including any validations or adjustments made, for transparency and debugging.\n",
    "\n",
    "### How It Works:\n",
    "1. Scans the input query to check for violations.\n",
    "2. Retrieves relevant text and image data for the query.\n",
    "3. Synthesizes a response using both textual and visual context.\n",
    "4. Validates the response for appropriateness before returning it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35a94be2-e289-41a6-92e4-d3cb428fb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine, SimpleMultiModalQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.core.schema import ImageNode, NodeWithScore, MetadataMode\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.base.response.schema import Response\n",
    "\n",
    "from typing import List, Callable, Optional\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "QA_PROMPT_TMPL = \"\"\"\\\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query if it is related to the context. \n",
    "If the query is not related to the context, respond with:\n",
    "\"I'm sorry, but I can't help with that.\"\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \"\"\"\n",
    "\n",
    "QA_PROMPT = PromptTemplate(QA_PROMPT_TMPL)\n",
    "\n",
    "\n",
    "\n",
    "class MultimodalQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"Custom multimodal Query Engine.\n",
    "\n",
    "    Takes in a retriever to retrieve a set of document nodes.\n",
    "    Also takes in a prompt template and multimodal model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    qa_prompt: PromptTemplate\n",
    "    retriever: BaseRetriever\n",
    "    multi_modal_llm: GeminiMultiModal\n",
    "    input_scanners: List[Callable[[str], dict]] = Field(default_factory=list)\n",
    "    output_scanners: List[Callable[[str], dict]] = Field(default_factory=list)\n",
    "\n",
    "    def __init__(self, qa_prompt: Optional[PromptTemplate] = None, **kwargs) -> None:\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__(qa_prompt=qa_prompt or QA_PROMPT, **kwargs)\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "\n",
    "        query_metadata = {\n",
    "                \"input_scanners\": [],\n",
    "                \"output_scanners\": [],\n",
    "                \"retrieved_nodes\": [],\n",
    "                \"response_status\": \"success\",\n",
    "            }\n",
    "\n",
    "        input_detected, input_triggered = InputScanner(query_str, self.input_scanners)\n",
    "        if input_triggered:\n",
    "            # print(\"Triggered Input Scanners:\", input_triggered)\n",
    "            # Log triggered input scanners in metadata\n",
    "            query_metadata[\"input_scanners\"] = input_triggered\n",
    "        # If input contains sensitive information, block the query\n",
    "            if input_detected:\n",
    "                return Response(\n",
    "                    response=\"I'm sorry, but I can't help with that.\",\n",
    "                    source_nodes=[],\n",
    "                    metadata={\n",
    "                        \"guardrail\": \"Input Scanner\",\n",
    "                        \"triggered_scanners\": input_triggered,\n",
    "                        \"response_status\": \"blocked\",\n",
    "                    },\n",
    "            )\n",
    "\n",
    "        # retrieve text nodes\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "        # create ImageNode items from text nodes\n",
    "        image_nodes = [\n",
    "            NodeWithScore(node=ImageNode(image_path=n.metadata[\"image_path\"]))\n",
    "            for n in nodes\n",
    "        ]\n",
    "\n",
    "        # create context string from text nodes, dump into the prompt\n",
    "        context_str = \"\\n\\n\".join(\n",
    "            [r.get_content(metadata_mode=MetadataMode.LLM) for r in nodes]\n",
    "        )\n",
    "        fmt_prompt = self.qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "\n",
    "        # synthesize an answer from formatted text and images\n",
    "        llm_response = self.multi_modal_llm.complete(\n",
    "            prompt=fmt_prompt,\n",
    "            image_documents=[image_node.node for image_node in image_nodes],\n",
    "        )\n",
    "\n",
    "        # Step 5: Run Output Scanners\n",
    "        output_detected, output_triggered = OutputScanner(str(llm_response), str(query_str), str(context_str), self.output_scanners)\n",
    "        if output_triggered:\n",
    "            # print(\"Triggered Output Scanners:\", output_triggered)\n",
    "            query_metadata[\"output_scanners\"] = output_triggered  # Store output scanner info\n",
    "\n",
    "        final_response = str(llm_response)\n",
    "        if output_detected:\n",
    "            final_response = \"I'm sorry, but I can't help with that.\"\n",
    "            query_metadata[\"response_status\"] = \"sanitized\"\n",
    "        # Return the response with detailed metadata\n",
    "        return Response(\n",
    "            response=final_response,\n",
    "                source_nodes=nodes,\n",
    "                metadata=query_metadata\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd3974",
   "metadata": {},
   "source": [
    "### Input and Output Scanners Configuration\n",
    "\n",
    "You can put the scanner which you need to guard your RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a44b28eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_scanners = [guardrail_toxicLanguage,guardrail_tokenlimit]\n",
    "output_scanners = [guardrail_toxicLanguage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0890be59-fb12-4bb5-959b-b2d9600f7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = MultimodalQueryEngine(\n",
    "    retriever=index.as_retriever(similarity_top_k=9),\n",
    "    multi_modal_llm=gemini_multimodal,\n",
    "    input_scanners=input_scanners,\n",
    "    output_scanners=output_scanners,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2336f98b-c0a1-413a-849d-8a89bacb90b5",
   "metadata": {},
   "source": [
    "## Try out Queries\n",
    "\n",
    "Let's try out queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d78e53cf-35cb-4ef8-b03e-1b47ba15ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Tell me about the diverse geographies where Conoco Phillips has a production base\n",
      "\u001b[2m2024-12-03 17:43:08\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mInitialized classification model\u001b[0m \u001b[36mdevice\u001b[0m=\u001b[35mdevice(type='mps')\u001b[0m \u001b[36mmodel\u001b[0m=\u001b[35mModel(path='unitary/unbiased-toxic-roberta', subfolder='', revision='36295dd80b422dc49f40052021430dae76241adc', onnx_path='ProtectAI/unbiased-toxic-roberta-onnx', onnx_revision='34480fa958f6657ad835c345808475755b6974a7', onnx_subfolder='', onnx_filename='model.onnx', kwargs={}, pipeline_kwargs={'batch_size': 1, 'device': device(type='mps'), 'padding': 'max_length', 'top_k': None, 'function_to_apply': 'sigmoid', 'return_token_type_ids': False, 'max_length': 512, 'truncation': True}, tokenizer_kwargs={})\u001b[0m\n",
      "\u001b[2m2024-12-03 17:43:09\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mNot toxicity found in the text\u001b[0m \u001b[36mresults\u001b[0m=\u001b[35m[[{'label': 'toxicity', 'score': 0.00041448281263001263}, {'label': 'male', 'score': 0.00018738119979389012}, {'label': 'insult', 'score': 0.00011956175148952752}, {'label': 'female', 'score': 0.00011725842341547832}, {'label': 'psychiatric_or_mental_illness', 'score': 8.512590284226462e-05}, {'label': 'white', 'score': 7.451862620655447e-05}, {'label': 'christian', 'score': 5.6545581173850223e-05}, {'label': 'muslim', 'score': 5.644273551297374e-05}, {'label': 'black', 'score': 3.8606172893196344e-05}, {'label': 'obscene', 'score': 3.222753730369732e-05}, {'label': 'identity_attack', 'score': 3.1757666874909773e-05}, {'label': 'threat', 'score': 2.8462023692554794e-05}, {'label': 'jewish', 'score': 2.7872381906490773e-05}, {'label': 'homosexual_gay_or_lesbian', 'score': 2.5694836949696764e-05}, {'label': 'sexual_explicit', 'score': 1.859129588410724e-05}, {'label': 'severe_toxicity', 'score': 1.0931341876130318e-06}]]\u001b[0m\n",
      "\u001b[2m2024-12-03 17:43:15\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mPrompt fits the maximum tokens\u001b[0m \u001b[36mnum_tokens\u001b[0m=\u001b[35m15\u001b[0m \u001b[36mthreshold\u001b[0m=\u001b[35m400\u001b[0m\n",
      "Prompt: ConocoPhillips has a diverse production base across several geographic locations.  These include:\n",
      "\n",
      "* **Alaska:**  The company has a significant presence in Alaska's conventional basins, including the Prudhoe Bay area, with a long history of production and existing infrastructure.  The Willow project is also located in Alaska.\n",
      "* **Lower 48 (United States):**  ConocoPhillips operates extensively in the Lower 48 states, focusing on unconventional plays in the Permian Basin (Delaware and Midland Basins), Eagle Ford, and Bakken.\n",
      "* **International:** The company has operations in other international locations, including Qatar (LNG), and previously had operations in the UK, Australia, Indonesia, and Canada (though some of these have been divested).  They also have a global marketing presence with offices in London, Singapore, Houston, Calgary, Beijing, and Tokyo.\n",
      "\u001b[2m2024-12-03 17:43:36\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mInitialized classification model\u001b[0m \u001b[36mdevice\u001b[0m=\u001b[35mdevice(type='mps')\u001b[0m \u001b[36mmodel\u001b[0m=\u001b[35mModel(path='unitary/unbiased-toxic-roberta', subfolder='', revision='36295dd80b422dc49f40052021430dae76241adc', onnx_path='ProtectAI/unbiased-toxic-roberta-onnx', onnx_revision='34480fa958f6657ad835c345808475755b6974a7', onnx_subfolder='', onnx_filename='model.onnx', kwargs={}, pipeline_kwargs={'batch_size': 1, 'device': device(type='mps'), 'padding': 'max_length', 'top_k': None, 'function_to_apply': 'sigmoid', 'return_token_type_ids': False, 'max_length': 512, 'truncation': True}, tokenizer_kwargs={})\u001b[0m\n",
      "\u001b[2m2024-12-03 17:43:37\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mNot toxicity found in the text\u001b[0m \u001b[36mresults\u001b[0m=\u001b[35m[[{'label': 'toxicity', 'score': 0.0003606641257647425}, {'label': 'male', 'score': 0.000291528704110533}, {'label': 'insult', 'score': 0.00011418585199862719}, {'label': 'psychiatric_or_mental_illness', 'score': 0.00011314846051391214}, {'label': 'female', 'score': 0.00010114537144545466}, {'label': 'white', 'score': 9.688278078101575e-05}, {'label': 'muslim', 'score': 6.954199488973245e-05}, {'label': 'christian', 'score': 5.551999493036419e-05}, {'label': 'black', 'score': 4.1746119677554816e-05}, {'label': 'identity_attack', 'score': 3.3705578971421346e-05}, {'label': 'homosexual_gay_or_lesbian', 'score': 3.157216633553617e-05}, {'label': 'obscene', 'score': 2.798157584038563e-05}, {'label': 'jewish', 'score': 2.618367398099508e-05}, {'label': 'threat', 'score': 2.1199964976403862e-05}, {'label': 'sexual_explicit', 'score': 1.9145050828228705e-05}, {'label': 'severe_toxicity', 'score': 1.1292050885458593e-06}]]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"Tell me about the diverse geographies where Conoco Phillips has a production base\"\n",
    ")\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b090dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConocoPhillips has a diverse production base across several geographic locations.  These include:\n",
      "\n",
      "* **Alaska:**  The company has a significant presence in Alaska's conventional basins, including the Prudhoe Bay area, with a long history of production and existing infrastructure.  The Willow project is also located in Alaska.\n",
      "* **Lower 48 (United States):**  ConocoPhillips operates extensively in the Lower 48 states, focusing on unconventional plays in the Permian Basin (Delaware and Midland Basins), Eagle Ford, and Bakken.\n",
      "* **International:** The company has operations in other international locations, including Qatar (LNG), and previously had operations in the UK, Australia, Indonesia, and Canada (though some of these have been divested).  They also have a global marketing presence with offices in London, Singapore, Houston, Calgary, Beijing, and Tokyo.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "528752bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_scanners': [], 'output_scanners': [], 'retrieved_nodes': [], 'response_status': 'success'}\n"
     ]
    }
   ],
   "source": [
    "print(str(response.metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0b2d82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "    If you're looking for random paragraphs, you've come to the right place. When a random word or a random sentence isn't quite enough, the next logical step is to find a random paragraph. We created the Random Paragraph Generator with you in mind. The process is quite simple. Choose the number of random paragraphs you'd like to see and click the button. Your chosen number of paragraphs will instantly appear.\n",
      "\n",
      "While it may not be obvious to everyone, there are a number of reasons creating random paragraphs can be useful. A few examples of how some people use this generator are listed in the following paragraphs.\n",
      "\n",
      "Creative Writing\n",
      "Generating random paragraphs can be an excellent way for writers to get their creative flow going at the beginning of the day. The writer has no idea what topic the random paragraph will be about when it appears. This forces the writer to use creativity to complete one of three common writing challenges. The writer can use the paragraph as the first one of a short story and build upon it. A second option is to use the random paragraph somewhere in a short story they create. The third option is to have the random paragraph be the ending paragraph in a short story. No matter which of these challenges is undertaken, the writer is forced to use creativity to incorporate the paragraph into their writing.\n",
      "\n",
      "Tackle Writers' Block\n",
      "A random paragraph can also be an excellent way for a writer to tackle writers' block. Writing block can often happen due to being stuck with a current project that the writer is trying to complete. By inserting a completely random paragraph from which to begin, it can take down some of the issues that may have been causing the writers' block in the first place.\n",
      "\n",
      "Beginning Writing Routine\n",
      "Another productive way to use this tool to begin a daily writing routine. One way is to generate a random paragraph with the intention to try to rewrite it while still keeping the original meaning. The purpose here is to just get the writing started so that when the writer goes onto their day's writing projects, words are already flowing from their fingers.\n",
      "\n",
      "Writing Challenge\n",
      "Another writing challenge can be to take the individual sentences in the random paragraph and incorporate a single sentence from that into a new paragraph to create a short story. Unlike the random sentence generator, the sentences from the random paragraph will have some connection to one another so it will be a bit different. You also won't know exactly how many sentences will appear in the random paragraph.\n",
      "\n",
      "Programmers\n",
      "It's not only writers who can benefit from this free online tool. If you're a programmer who's working on a project where blocks of text are needed, this tool can be a great way to get that. It's a good way to test your programming and that the tool being created is working well.\n",
      "\n",
      "Above are a few examples of how the random paragraph generator can be beneficial. The best way to see if this random paragraph picker will be useful for your intended purposes is to give it a try. Generate a number of paragraphs to see if they are beneficial to your current project.\n",
      "\n",
      "If you do find this paragraph tool useful, please do us a favor and let us know how you're using it. It's greatly beneficial for us to know the different ways this tool is being used so we can improve it with updates. This is especially true since there are times when the generators we create get used in completely unanticipated ways from when we initially created them. If you have the time, please send us a quick note on what you'd like to see changed or added to make it better in the future.\n",
      "\n",
      "Frequently Asked Questions\n",
      "\n",
      "Can I use these random paragraphs for my project?\n",
      "\n",
      "Yes! All of the random paragraphs in our generator are free to use for your projects.\n",
      "\n",
      "Does a computer generate these paragraphs?\n",
      "\n",
      "No! All of the paragraphs in the generator are written by humans, not computers. When first building this generator we thought about using computers to generate the paragraphs, but they weren't very good and many times didn't make any sense at all. We therefore took the time to create paragraphs specifically for this generator to make it the best that we could.\n",
      "\n",
      "Can I contribute random paragraphs?\n",
      "\n",
      "Yes. We're always interested in improving this generator and one of the best ways to do that is to add new and interesting paragraphs to the generator. If you'd like to contribute some random paragraphs, please contact us.\n",
      "\n",
      "How many words are there in a paragraph?\n",
      "\n",
      "There are usually about 200 words in a paragraph, but this can vary widely. Most paragraphs focus on a single idea that's expressed with an introductory sentence, then followed by two or more supporting sentences about the idea. A short paragraph may not reach even 50 words while long paragraphs can be over 400 words long, but generally speaking they tend to be approximately 200 words in length.\n",
      "    \n",
      "\u001b[2m2024-12-03 17:43:42\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mInitialized classification model\u001b[0m \u001b[36mdevice\u001b[0m=\u001b[35mdevice(type='mps')\u001b[0m \u001b[36mmodel\u001b[0m=\u001b[35mModel(path='unitary/unbiased-toxic-roberta', subfolder='', revision='36295dd80b422dc49f40052021430dae76241adc', onnx_path='ProtectAI/unbiased-toxic-roberta-onnx', onnx_revision='34480fa958f6657ad835c345808475755b6974a7', onnx_subfolder='', onnx_filename='model.onnx', kwargs={}, pipeline_kwargs={'batch_size': 1, 'device': device(type='mps'), 'padding': 'max_length', 'top_k': None, 'function_to_apply': 'sigmoid', 'return_token_type_ids': False, 'max_length': 512, 'truncation': True}, tokenizer_kwargs={})\u001b[0m\n",
      "\u001b[2m2024-12-03 17:43:42\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mNot toxicity found in the text\u001b[0m \u001b[36mresults\u001b[0m=\u001b[35m[[{'label': 'toxicity', 'score': 0.0011976422974839807}, {'label': 'insult', 'score': 0.00045695927110500634}, {'label': 'male', 'score': 0.00018701529188547283}, {'label': 'psychiatric_or_mental_illness', 'score': 0.00014795312017668039}, {'label': 'white', 'score': 9.39662495511584e-05}, {'label': 'female', 'score': 7.459904009010643e-05}, {'label': 'obscene', 'score': 6.114380084909499e-05}, {'label': 'threat', 'score': 5.259696990833618e-05}, {'label': 'muslim', 'score': 4.745226033264771e-05}, {'label': 'identity_attack', 'score': 3.541662226780318e-05}, {'label': 'black', 'score': 3.5083121474599466e-05}, {'label': 'christian', 'score': 3.272023604949936e-05}, {'label': 'sexual_explicit', 'score': 3.164245936204679e-05}, {'label': 'jewish', 'score': 1.5377421732409857e-05}, {'label': 'homosexual_gay_or_lesbian', 'score': 1.5361225450760685e-05}, {'label': 'severe_toxicity', 'score': 1.3027844261159771e-06}]]\u001b[0m\n",
      "\u001b[2m2024-12-03 17:43:46\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mPrompt is too big. Splitting into chunks\u001b[0m \u001b[36mchunks\u001b[0m=\u001b[35m[\"\\n    If you're looking for random paragraphs, you've come to the right place. When a random word or a random sentence isn't quite enough, the next logical step is to find a random paragraph. We created the Random Paragraph Generator with you in mind. The process is quite simple. Choose the number of random paragraphs you'd like to see and click the button. Your chosen number of paragraphs will instantly appear.\\n\\nWhile it may not be obvious to everyone, there are a number of reasons creating random paragraphs can be useful. A few examples of how some people use this generator are listed in the following paragraphs.\\n\\nCreative Writing\\nGenerating random paragraphs can be an excellent way for writers to get their creative flow going at the beginning of the day. The writer has no idea what topic the random paragraph will be about when it appears. This forces the writer to use creativity to complete one of three common writing challenges. The writer can use the paragraph as the first one of a short story and build upon it. A second option is to use the random paragraph somewhere in a short story they create. The third option is to have the random paragraph be the ending paragraph in a short story. No matter which of these challenges is undertaken, the writer is forced to use creativity to incorporate the paragraph into their writing.\\n\\nTackle Writers' Block\\nA random paragraph can also be an excellent way for a writer to tackle writers' block. Writing block can often happen due to being stuck with a current project that the writer is trying to complete. By inserting a completely random paragraph from which to begin, it can take down some of the issues that may have been causing the writers' block in the first place.\\n\\nBeginning Writing Routine\\nAnother productive way to use this tool to begin a daily writing routine. One way is to generate a random paragraph with the intention to try to rewrite it while still keeping the original meaning. The purpose here is to just get the writing started so that when the writer goes onto their day's writing\", \" projects, words are already flowing from their fingers.\\n\\nWriting Challenge\\nAnother writing challenge can be to take the individual sentences in the random paragraph and incorporate a single sentence from that into a new paragraph to create a short story. Unlike the random sentence generator, the sentences from the random paragraph will have some connection to one another so it will be a bit different. You also won't know exactly how many sentences will appear in the random paragraph.\\n\\nProgrammers\\nIt's not only writers who can benefit from this free online tool. If you're a programmer who's working on a project where blocks of text are needed, this tool can be a great way to get that. It's a good way to test your programming and that the tool being created is working well.\\n\\nAbove are a few examples of how the random paragraph generator can be beneficial. The best way to see if this random paragraph picker will be useful for your intended purposes is to give it a try. Generate a number of paragraphs to see if they are beneficial to your current project.\\n\\nIf you do find this paragraph tool useful, please do us a favor and let us know how you're using it. It's greatly beneficial for us to know the different ways this tool is being used so we can improve it with updates. This is especially true since there are times when the generators we create get used in completely unanticipated ways from when we initially created them. If you have the time, please send us a quick note on what you'd like to see changed or added to make it better in the future.\\n\\nFrequently Asked Questions\\n\\nCan I use these random paragraphs for my project?\\n\\nYes! All of the random paragraphs in our generator are free to use for your projects.\\n\\nDoes a computer generate these paragraphs?\\n\\nNo! All of the paragraphs in the generator are written by humans, not computers. When first building this generator we thought about using computers to generate the paragraphs, but they weren't very good and many times didn't make any sense at all\", \". We therefore took the time to create paragraphs specifically for this generator to make it the best that we could.\\n\\nCan I contribute random paragraphs?\\n\\nYes. We're always interested in improving this generator and one of the best ways to do that is to add new and interesting paragraphs to the generator. If you'd like to contribute some random paragraphs, please contact us.\\n\\nHow many words are there in a paragraph?\\n\\nThere are usually about 200 words in a paragraph, but this can vary widely. Most paragraphs focus on a single idea that's expressed with an introductory sentence, then followed by two or more supporting sentences about the idea. A short paragraph may not reach even 50 words while long paragraphs can be over 400 words long, but generally speaking they tend to be approximately 200 words in length.\\n    \"]\u001b[0m \u001b[36mnum_tokens\u001b[0m=\u001b[35m961\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = (\n",
    "    \"\"\"\n",
    "    If you're looking for random paragraphs, you've come to the right place. When a random word or a random sentence isn't quite enough, the next logical step is to find a random paragraph. We created the Random Paragraph Generator with you in mind. The process is quite simple. Choose the number of random paragraphs you'd like to see and click the button. Your chosen number of paragraphs will instantly appear.\n",
    "\n",
    "While it may not be obvious to everyone, there are a number of reasons creating random paragraphs can be useful. A few examples of how some people use this generator are listed in the following paragraphs.\n",
    "\n",
    "Creative Writing\n",
    "Generating random paragraphs can be an excellent way for writers to get their creative flow going at the beginning of the day. The writer has no idea what topic the random paragraph will be about when it appears. This forces the writer to use creativity to complete one of three common writing challenges. The writer can use the paragraph as the first one of a short story and build upon it. A second option is to use the random paragraph somewhere in a short story they create. The third option is to have the random paragraph be the ending paragraph in a short story. No matter which of these challenges is undertaken, the writer is forced to use creativity to incorporate the paragraph into their writing.\n",
    "\n",
    "Tackle Writers' Block\n",
    "A random paragraph can also be an excellent way for a writer to tackle writers' block. Writing block can often happen due to being stuck with a current project that the writer is trying to complete. By inserting a completely random paragraph from which to begin, it can take down some of the issues that may have been causing the writers' block in the first place.\n",
    "\n",
    "Beginning Writing Routine\n",
    "Another productive way to use this tool to begin a daily writing routine. One way is to generate a random paragraph with the intention to try to rewrite it while still keeping the original meaning. The purpose here is to just get the writing started so that when the writer goes onto their day's writing projects, words are already flowing from their fingers.\n",
    "\n",
    "Writing Challenge\n",
    "Another writing challenge can be to take the individual sentences in the random paragraph and incorporate a single sentence from that into a new paragraph to create a short story. Unlike the random sentence generator, the sentences from the random paragraph will have some connection to one another so it will be a bit different. You also won't know exactly how many sentences will appear in the random paragraph.\n",
    "\n",
    "Programmers\n",
    "It's not only writers who can benefit from this free online tool. If you're a programmer who's working on a project where blocks of text are needed, this tool can be a great way to get that. It's a good way to test your programming and that the tool being created is working well.\n",
    "\n",
    "Above are a few examples of how the random paragraph generator can be beneficial. The best way to see if this random paragraph picker will be useful for your intended purposes is to give it a try. Generate a number of paragraphs to see if they are beneficial to your current project.\n",
    "\n",
    "If you do find this paragraph tool useful, please do us a favor and let us know how you're using it. It's greatly beneficial for us to know the different ways this tool is being used so we can improve it with updates. This is especially true since there are times when the generators we create get used in completely unanticipated ways from when we initially created them. If you have the time, please send us a quick note on what you'd like to see changed or added to make it better in the future.\n",
    "\n",
    "Frequently Asked Questions\n",
    "\n",
    "Can I use these random paragraphs for my project?\n",
    "\n",
    "Yes! All of the random paragraphs in our generator are free to use for your projects.\n",
    "\n",
    "Does a computer generate these paragraphs?\n",
    "\n",
    "No! All of the paragraphs in the generator are written by humans, not computers. When first building this generator we thought about using computers to generate the paragraphs, but they weren't very good and many times didn't make any sense at all. We therefore took the time to create paragraphs specifically for this generator to make it the best that we could.\n",
    "\n",
    "Can I contribute random paragraphs?\n",
    "\n",
    "Yes. We're always interested in improving this generator and one of the best ways to do that is to add new and interesting paragraphs to the generator. If you'd like to contribute some random paragraphs, please contact us.\n",
    "\n",
    "How many words are there in a paragraph?\n",
    "\n",
    "There are usually about 200 words in a paragraph, but this can vary widely. Most paragraphs focus on a single idea that's expressed with an introductory sentence, then followed by two or more supporting sentences about the idea. A short paragraph may not reach even 50 words while long paragraphs can be over 400 words long, but generally speaking they tend to be approximately 200 words in length.\n",
    "    \"\"\"\n",
    ")\n",
    "response = query_engine.query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f526e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I can't help with that.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "355d2aa4-c26f-480e-b512-4446acbd9227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guardrail': 'Input Scanner', 'triggered_scanners': [{'guardrail_type': 'Token limit', 'activated': True, 'guardrail_detail': {'guard_output': \"\\n    If you're looking for random paragraphs, you've come to the right place. When a random word or a random sentence isn't quite enough, the next logical step is to find a random paragraph. We created the Random Paragraph Generator with you in mind. The process is quite simple. Choose the number of random paragraphs you'd like to see and click the button. Your chosen number of paragraphs will instantly appear.\\n\\nWhile it may not be obvious to everyone, there are a number of reasons creating random paragraphs can be useful. A few examples of how some people use this generator are listed in the following paragraphs.\\n\\nCreative Writing\\nGenerating random paragraphs can be an excellent way for writers to get their creative flow going at the beginning of the day. The writer has no idea what topic the random paragraph will be about when it appears. This forces the writer to use creativity to complete one of three common writing challenges. The writer can use the paragraph as the first one of a short story and build upon it. A second option is to use the random paragraph somewhere in a short story they create. The third option is to have the random paragraph be the ending paragraph in a short story. No matter which of these challenges is undertaken, the writer is forced to use creativity to incorporate the paragraph into their writing.\\n\\nTackle Writers' Block\\nA random paragraph can also be an excellent way for a writer to tackle writers' block. Writing block can often happen due to being stuck with a current project that the writer is trying to complete. By inserting a completely random paragraph from which to begin, it can take down some of the issues that may have been causing the writers' block in the first place.\\n\\nBeginning Writing Routine\\nAnother productive way to use this tool to begin a daily writing routine. One way is to generate a random paragraph with the intention to try to rewrite it while still keeping the original meaning. The purpose here is to just get the writing started so that when the writer goes onto their day's writing\", 'is_valid': False, 'risk_score/threshold': '1.0/400', 'response_text': \"This text describes a random paragraph generator and its various uses.  Here's a summary broken down by section:\\n\\n**Introduction:** The text introduces a random paragraph generator, highlighting its simplicity and ease of use.\\n\\n**Uses of the Generator:**  The core of the text details how the generator can be beneficial in several contexts:\\n\\n* **Creative Writing:**  It aids writers in overcoming writer's block, sparking creativity, and providing starting points or endings for short stories.  Three specific challenges are suggested: using the paragraph as the beginning, middle, or end of a story.\\n\\n* **Tackling Writer's Block:** The random paragraph acts as a disruption to overcome creative stagnation.\\n\\n* **Beginning a Writing Routine:** It helps initiate the writing process by providing a text to rewrite or use as inspiration.\\n\\n* **Writing Challenges:** The paragraph's sentences can be individually incorporated into new writing projects.\\n\\n* **Programmers:** The generator provides useful blocks of text for testing purposes in software development.\\n\\n\\n**Call to Action & Feedback:** The authors encourage users to try the generator, provide feedback, and contribute to its improvement by suggesting additions or changes.\\n\\n**Frequently Asked Questions (FAQ):**  The FAQ section addresses common questions about the generator, clarifying:\\n\\n* **Usage rights:**  The paragraphs are free to use.\\n* **Paragraph generation:**  Human-written paragraphs are used, not computer-generated ones.\\n* **Contribution:** Users can contribute their own paragraphs.\\n* **Paragraph length:** Paragraphs are roughly 200 words but can vary significantly.\\n\\n\\nIn essence, the text is a well-structured promotional piece for a random paragraph generator, emphasizing its versatility and usefulness for both writers and programmers, while encouraging user engagement and participation.\\n\"}}], 'response_status': 'blocked'}\n"
     ]
    }
   ],
   "source": [
    "print(str(response.metadata))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
