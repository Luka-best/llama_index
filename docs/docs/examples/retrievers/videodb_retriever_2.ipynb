{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/video-db/videodb-cookbook/blob/main/docs/integrations/llama-index/simple_video_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# VideoDB Retriever "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG: Multimodal Search on Videos and Stream Video Results üì∫\n",
    "\n",
    "Constructing a RAG pipeline for text is relatively straightforward, thanks to the tools developed for parsing, indexing, and retrieving text data. \n",
    "\n",
    "However, adapting RAG models for video content presents a greater challenge. Videos combine visual, auditory, and textual elements, requiring more processing power and sophisticated video pipelines.\n",
    "\n",
    "While Large Language Models (LLMs) excel with text, they fall short in helping you consume or create video clips. `VideoDB` provides a sophisticated database abstraction for your MP4 files, enabling the use of LLMs on your video data. With VideoDB, you can not only analyze but also `instantly watch video streams` of your search results.\n",
    "\n",
    "> [VideoDB](https://videodb.io) is a serverless database designed to streamline the storage, search, editing, and streaming of video content. VideoDB offers random access to sequential video data by building indexes and developing interfaces for querying and browsing video content. Learn more at [docs.videodb.io](https://docs.videodb.io).\n",
    "\n",
    "\n",
    "In this notebook, we introduce `VideoDBRetriever`, a tool specifically designed to simplify the creation of RAG pipelines for video content, without any hassle of dealing with complex video infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## üõ†Ô∏èÔ∏è Setup \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë Requirements\n",
    "\n",
    "To connect to VideoDB, simply get the API key and create a connection. This can be done by setting the `VIDEO_DB_API_KEY` environment variable. You can get it from üëâüèº [VideoDB Console](https://console.videodb.io). ( Free for first 50 uploads, **No credit card required!** )\n",
    "\n",
    "Get your `OPENAI_API_KEY` from OpenAI platform for `llama_index` response synthesizer.\n",
    "\n",
    "<!-- > Set the `OPENAI_API_KEY` & `VIDEO_DB_API_KEY` environment variable with your API keys. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"VIDEO_DB_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Installing Dependencies\n",
    "\n",
    "To get started, we'll need to install the following packages:\n",
    "\n",
    "- `llama-index`\n",
    "- `llama-index-retrievers-videodb`\n",
    "- `videodb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install videodb\n",
    "%pip install llama-index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-retrievers-videodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ† Building Multimodal RAG\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implementing this multimodal search query involves following steps with VideoDB:\n",
    "\n",
    "1. üé¨ **Upload and Index the Video**:\n",
    "     - Upload the video and get the video object.\n",
    "     - `index_scenes`  function to detect and recognize events, such as theft, within the video footage.\n",
    "     - `index_spoken_words` function to index spoken words of the news anchor to enable keyword search.\n",
    "2. üß© **Query Transformation**: Divide query into two parts that can be used with respective scene and spoken indexes.\n",
    "3. üîé **Finding Relevant nodes for each modality**: Using `VideoDBRetriever` find relevant nodes from Spoken Index and Scene Index \n",
    "4. ‚úèÔ∏è **Viewing the result : Text**: Use Relevant Nodes to sythesize a text reponse Integrating the results from both indexes for precise video segment identification. \n",
    "5. üé• **Viewing the result : Video Clip**: Integrating the results from both indexes for precise video segment identification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Step 1: Connect to VideoDB and Ingest Data\n",
    "\n",
    "Let's upload a our video file first.\n",
    "\n",
    "You can use any `public url`, `Youtube link` or `local file` on your system. \n",
    "\n",
    "> ‚ú® First 50 uploads are free!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Video\n",
      "Video uploaded with ID: m-0ccadfc8-bc8c-4183-b83a-543946460e2a\n"
     ]
    }
   ],
   "source": [
    "from videodb import connect\n",
    "\n",
    "# connect to VideoDB\n",
    "conn = connect()\n",
    "coll = conn.get_collection()\n",
    "\n",
    "# upload videos to default collection in VideoDB\n",
    "print(\"Uploading Video\")\n",
    "video = conn.upload(url=\"https://www.youtube.com/watch?v=libKVRa01L8\")\n",
    "print(f\"Video uploaded with ID: {video.id}\")\n",
    "\n",
    "\n",
    "# video = coll.get_video(\"m-56f55058-62b6-49c4-bbdc-43c0badf4c0b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `coll = conn.get_collection()` : Returns default collection object.\n",
    "> * `coll.get_videos()` : Returns list of all the videos in a collections.\n",
    "> * `coll.get_video(video_id)`: Returns Video object from given`video_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### üì∏üó£Ô∏è Step 2: Index the Video on different Modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó£Ô∏è Indexing Spoken Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing spoken content in Video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:23<00:00,  4.27it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Indexing spoken content in Video...\")\n",
    "video.index_spoken_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì∏Ô∏è Indexing Visual Content\n",
    "\n",
    "To learn more about Scene Index, explore the following guides:\n",
    "\n",
    "- [Quickstart Guide](https://github.com/video-db/videodb-cookbook/blob/main/quickstart/Scene%20Index%20QuickStart.ipynb) guide provides a step-by-step introduction to Scene Index. It's ideal for getting started quickly and understanding the primary functions.\n",
    "\n",
    "- [Scene Extraction Options Guide](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/playground_scene_extraction.ipynb) delves deeper into the various options available for scene extraction within Scene Index. It covers advanced settings, customization features, and tips for optimizing scene extraction based on different needs and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Visual content in Video...\n",
      "Scene Index successful with ID: f3eef7aee2a0ff58\n"
     ]
    }
   ],
   "source": [
    "from videodb import SceneExtractionType\n",
    "\n",
    "print(\"Indexing Visual content in Video...\")\n",
    "\n",
    "#Index scene content\n",
    "index_id = video.index_scenes(\n",
    "    extraction_type=SceneExtractionType.time_based,\n",
    "    extraction_config={\"time\": 2, \"select_frames\": [\"first\", \"last\"]},\n",
    "    prompt=\"Describe the scene in detail\",\n",
    ")\n",
    "video.get_scene_index(index_id)\n",
    "\n",
    "print(f\"Scene Index successful with ID: {index_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Step3: Simple Multimodal RAG \n",
    "\n",
    "To construct a multimodal RAG pipeline, follow these steps:\n",
    "\n",
    "- üß© **Segment the Query**: Split the query into two parts, each corresponding to the scene and spoken indexes.\n",
    "- üîé **Retrieve Nodes**: Use each segment of the query to fetch relevant nodes from the respective modalities.\n",
    "- üí¨ **Generate Text**: Synthesize a text-based answer using the retrieved nodes.\n",
    "- üé• **Extract Video Clips**: Identify and compile relevant video clips based on the retrieved nodes.\n",
    "\n",
    "Now that the videos are indexed, leverage `VideoDBRetriever` to access the pertinent nodes from the VideoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß© Query Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query for Spoken retriever :  Discuss the formation of the solar system\n",
      "Query for Scene retriever :  Visualize the milky way galaxy\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "def split_spoken_visual_query(query):\n",
    "    transformation_prompt = \"\"\"\n",
    "    Divide the following query into two distinct parts: one for spoken content and one for visual content. The spoken content should refer to any narration, dialogue, or verbal explanations and The visual content should refer to any images, videos, or graphical representations. Format the response strictly as:\\nSpoken: <spoken_query>\\nVisual: <visual_query>\\n\\nQuery: {query}\n",
    "    \"\"\"\n",
    "    prompt = transformation_prompt.format(query=query)\n",
    "    response = OpenAI(model=\"gpt-4\").complete(prompt)\n",
    "    divided_query = response.text.strip().split(\"\\n\")\n",
    "    spoken_query = divided_query[0].replace(\"Spoken:\", \"\").strip()\n",
    "    scene_query = divided_query[1].replace(\"Visual:\", \"\").strip()\n",
    "    return spoken_query, scene_query \n",
    "\n",
    "\n",
    "query = \"Show me where the narrator discusses the formation of the solar system and visualize the milky way galaxy\"\n",
    "spoken_query, scene_query = split_spoken_visual_query(query)\n",
    "print(\"Query for Spoken retriever : \", spoken_query)\n",
    "print(\"Query for Scene retriever : \", scene_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üîé Finding Relevant nodes for each modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.videodb import VideoDBRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "# VideoDBRetriever by default uses the default collection in the VideoDB\n",
    "spoken_retriever = VideoDBRetriever(video=video.id, search_type=\"semantic\", index_type=\"spoken_word\", score_threshold=0.1)\n",
    "scene_retriever = VideoDBRetriever(video=video.id, search_type=\"semantic\", index_type=\"scene\", scene_index_id=index_id, score_threshold=0.1)\n",
    "\n",
    "nodes_spoken_index = spoken_retriever.retrieve(spoken_query)\n",
    "nodes_scene_index = scene_retriever.retrieve(scene_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ô∏èüí¨Ô∏è Viewing the result : Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The narrator discusses the formation of the solar system when mentioning how it came into being about 4.5 billion years ago due to the collapse of a cloud of interstellar gas and dust, forming a solar nebula. To visualize the Milky Way Galaxy, the images depict a detailed illustration showing a spiral galaxy with multiple arms swirling from a bright central bulge, highlighting the position of our Solar System within one of the minor spiral arms known as the Orion Spur.\n"
     ]
    }
   ],
   "source": [
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "response = response_synthesizer.synthesize(\n",
    "    query, nodes=nodes_scene_index+nodes_spoken_index\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üé• Viewing the result : Video Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From each modality we have retrieved result that are relevant to the query in the relevant modality (semantic & scene/ visual, in this case).\n",
    "\n",
    "Each node has `start` and `end` fields in meatadata. which represent the time the node represents. \n",
    "\n",
    "There are two ways to combine these search results:\n",
    "\n",
    "- **Union**: This method takes all the timestamps from every node, creating a comprehensive list that includes every relevant time, even if some timestamps appear in only one modality.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/video-db/videodb-cookbook-assets/main/images/guides/multimodal_quickstart_union.png\" alt=\"Example Image\" width=\"500\"/>\n",
    "\n",
    "- **Intersection**: This method only includes timestamps from every node, resulting in a smaller list with times that are universally relevant across all modalities.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/video-db/videodb-cookbook-assets/main/images/guides/multimodal_quickstart_intersection.png\" alt=\"Example Image\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Depending on which method you prefer, you can pass the appropriate argument to the `combine_results()` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spoken results:  [[26.026, 30.03]]\n",
      "Scene results:  [[0.0, 37.63]]\n",
      "Combined results:  [[0.0, 37.63]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def merge_intervals(intervals):\n",
    "    if not intervals:\n",
    "        return []\n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "    merged = [intervals[0]]\n",
    "    for interval in intervals[1:]:\n",
    "        if interval[0] <= merged[-1][1]:\n",
    "            merged[-1][1] = max(merged[-1][1], interval[1])\n",
    "        else:\n",
    "            merged.append(interval)\n",
    "    return merged\n",
    "\n",
    "# Define a Function to Find Intersections\n",
    "def process_shots(l1, l2, operation):\n",
    "    def intersection(intervals1, intervals2):\n",
    "        i, j = 0, 0\n",
    "        result = []\n",
    "        while i < len(intervals1) and j < len(intervals2):\n",
    "            low = max(intervals1[i][0], intervals2[j][0])\n",
    "            high = min(intervals1[i][1], intervals2[j][1])\n",
    "            if low < high:\n",
    "                result.append([low, high])\n",
    "            if intervals1[i][1] < intervals2[j][1]:\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        return result\n",
    "\n",
    "    if operation.lower() == \"intersection\":\n",
    "        return intersection(merge_intervals(l1), merge_intervals(l2))\n",
    "    elif operation.lower() == \"union\":\n",
    "        return merge_intervals(l1 + l2)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operation. Please choose 'intersection' or 'union'.\")\n",
    "\n",
    "\n",
    "def combine_results(spoken_results, scene_results, operation):\n",
    "    spoken_timestamps = [[shot.node.metadata['start'], shot.metadata['end']] for shot in spoken_results]\n",
    "    scene_timestamps = [[shot.node.metadata['start'], shot.metadata['end']] for shot in scene_results]\n",
    "    result = process_shots(spoken_timestamps, scene_timestamps, operation)\n",
    "    print(\"Spoken results: \", spoken_timestamps)\n",
    "    print(\"Scene results: \", scene_timestamps)\n",
    "    print(\"Combined results: \", result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Combine results\n",
    "results = combine_results(nodes_scene_index, nodes_spoken_index, \"union\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal Query: Show me where the narrator discusses the formation of the solar system and visualize the milky way galaxy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/133575ed-6c9e-4368-800e-60fc94fa2b53.m3u8'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import play_stream\n",
    "print(f\"Multimodal Query: {query}\")\n",
    "stream_link = video.generate_stream(results)\n",
    "play_stream(stream_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## Configuring `VideoDBRetriever`\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Retriever for only one Video\n",
    "You can pass the `id` of the video object to search in only that video. \n",
    "```python\n",
    "VideoDBRetriever(video=\"my_video.id\")\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Retriever for a set of Video/ Collection\n",
    "You can pass the `id` of the Collection to search in only that Collection. \n",
    "```python\n",
    "VideoDBRetriever(collection=\"my_coll.id\")\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Retriever for different type of Indexes\n",
    "```python\n",
    "spoken_word = VideoDBRetriever(index_type=\"spoken_word\", search_type=\"semantic\")\n",
    "\n",
    "scene_retriever = VideoDBRetriever(index_type=\"scene\", scene_index_id=\"my_index_id\", search_type=\"semantic\")\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Configuring Search Type of Retriever \n",
    "`search_type` determines the search method used to retrieve nodes against given query \n",
    "```python\n",
    "keyword_spoken_search = VideoDBRetriever(search_type=\"keyword\", index_type=\"spoken_word\")\n",
    "\n",
    "semantic_scene_search = VideoDBRetriever(search_type=\"semantic\", index_type=\"spoken_word\")\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Configure threshold parameters  \n",
    "- `result_threshold`: is the threshold for number of results returned by retriever; the default value is `5`\n",
    "- `score_threshold`: only nodes with score higher than `score_threshold` will be returned by retriever; the default value is `0.2`  \n",
    "\n",
    "```python\n",
    "custom_retriever = VideoDBRetriever(result_threshold=2, score_threshold=0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ú® Incorporating VideoDB in your existing Llamaindex RAG Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use a Vector Index of your own choice, you can fetch all Transcript Nodes and Visual Nodes of a video, and then index or incorporate them into your existing LlamaIndex pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó£ Fetching Transcript Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can fetch transcript nodes using `Video.get_transcript()`\n",
    "\n",
    "To configure the segmenter, use the `segmenter` and `length` arguments.\n",
    "\n",
    "Possible values for segmenter are:\n",
    "- `Segmenter.time`: Segments the video based on the specified `length` in seconds.\n",
    "- `Segmenter.word`: Segments the video based on the word count specified by `length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import Segmenter\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "\n",
    "# Fetch all Transcript Nodes\n",
    "nodes_transcript_raw = video.get_transcript(segmenter=Segmenter.time, length=60)\n",
    "\n",
    "# Convert the raw transcript nodes to TextNode objects\n",
    "nodes_transcript = [\n",
    "    TextNode(\n",
    "        text=node[\"text\"],\n",
    "        metadata={key: value for key, value in node.items() if key != \"text\"},\n",
    "    )\n",
    "    for node in nodes_transcript_raw\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì∏ Fetching Scene Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all Scenes\n",
    "scenes = video.get_scene_index(index_id)\n",
    "\n",
    "# Convert the scenes to TextNode objects\n",
    "nodes_scenes = [\n",
    "    TextNode(\n",
    "        text=node[\"description\"],\n",
    "        metadata={key: value for key, value in node.items() if key != \"description\"},\n",
    "    )\n",
    "    for node in scenes\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Simple RAG Pipeline with Transcript + Scene Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The narrator discusses the location of our Solar System within the Milky Way galaxy, emphasizing its position in one of the minor spiral arms known as the Orion Spur. The images provided offer visual representations of the Milky Way's structure, with labels indicating the specific location of the Solar System within the galaxy.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Index both Transcript and Scene Nodes\n",
    "index = VectorStoreIndex(nodes_scenes + nodes_transcript)\n",
    "q = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ô∏èüí¨Ô∏è Viewing the result : Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = q.query(\n",
    "    \"Show me where the narrator discusses the formation of the solar system and visualize the milky way galaxy\"\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üé• Viewing the result : Video Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import play_stream\n",
    "\n",
    "relevant_timestamps = [\n",
    "    [node.metadata[\"start\"], node.metadata[\"end\"]] for node in res.source_nodes\n",
    "]\n",
    "\n",
    "stream_url = video.generate_stream(merge_intervals(relevant_timestamps))\n",
    "play_stream(stream_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Next Steps\n",
    "---\n",
    "\n",
    "In this guide, we built a Simple Multimodal RAG for Videos Using VideoDB, Llamaindex, and OpenAI\n",
    "\n",
    "You can optimize the pipeline by incorporating more advanced techniques like\n",
    "- Build a Search on Video Collection\n",
    "- Optimize Query Transformation\n",
    "- More methods to combine retrieved nodes from different modalities\n",
    "- Experiment with Different RAG pipelines like Knowledge Graph\n",
    "\n",
    "\n",
    "To learn more about Scene Index, explore the following guides:\n",
    "\n",
    "- [Quickstart Guide](https://github.com/video-db/videodb-cookbook/blob/main/quickstart/Scene%20Index%20QuickStart.ipynb) \n",
    "- [Scene Extraction Options](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/playground_scene_extraction.ipynb)\n",
    "- [Advanced Visual Search](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/advanced_visual_search.ipynb)\n",
    "- [Custom Annotation Pipelines](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/custom_annotations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Support & Community\n",
    "---\n",
    "\n",
    "If you have any questions or feedback. Feel free to reach out to us üôåüèº\n",
    "\n",
    "* [Discord](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fdiscord.gg%2Fpy9P639jGz)\n",
    "* [GitHub](https://github.com/video-db)\n",
    "* [Email](mailto:ashu@videodb.io)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
