{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/ai21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI21\n",
    "\n",
    "This notebook shows how to use AI21's newest model - Jamba-Instruct. Along with support for the previous model family - Jurassic (J2-Mid and J2-Ultra).\n",
    "\n",
    "By default - jamba-instruct model is used. If you want to use the Jurassic models, you can specify the model name as \"j2-mid\" or \"j2-ultra\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install llama-index-llms-ai21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install llama-index"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting API Key"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When creating the AI21 instance, you can pass the API key as a parameter or by default, it will be consumed from the environment variable `AI21_API_KEY`."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T06:20:21.737344Z",
     "start_time": "2024-06-09T06:20:16.985234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from llama_index.llms.ai21 import AI21\n",
    "\n",
    "api_key = \"Your-api-key\"\n",
    "os.environ[\"AI21_API_KEY\"] = api_key\n",
    "\n",
    "llm = AI21()\n",
    "\n",
    "# Or\n",
    "llm = AI21(api_key=api_key)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a computer scientist, entrepreneur, and author. He is best known as the co-founder of Y Combinator, a venture capital firm that provides seed funding to early-stage startups. He is also a well-known essayist and writer on various topics such as programming, startups, and technology. He has written several influential essays on these topics, which have been widely read and discussed in the tech community.\n",
      "Paul Graham is a computer scientist, entrepreneur, and essayist known for his work in the field of computer programming and his contributions to the startup ecosystem. He is the co-founder of Viaweb, which was later sold to Yahoo! and became Yahoo! Store. Graham is also recognized for his essays on various topics related to technology, startups, and the nature of innovation, which are widely read and discussed in the tech community. He co-founded Y Combinator, one of the most prominent startup accelerators in the world, which has helped launch over a thousand startups, including notable companies like Airbnb, Dropbox, and Reddit. His insights and writings on entrepreneurship and technology have made him a respected and influential voice in the tech world.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Call `chat` with a list of messages"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.ai21 import AI21\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"hello there\"),\n",
    "    ChatMessage(\n",
    "        role=\"assistant\", content=\"Arrrr, matey! How can I help ye today?\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "\n",
    "resp = AI21(api_key=api_key).chat(\n",
    "    messages, preamble_override=\"You are a pirate with a colorful personality\"\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(resp)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call `complete` with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ai21 import AI21\n",
    "\n",
    "api_key = \"Your api key\"\n",
    "resp = AI21(api_key=api_key).complete(\"Paul Graham is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "an American computer scientist, essayist, and venture capitalist. He is best known for his work on Lisp, programming language design, and entrepreneurship. Graham has written several books on these topics, including \" ANSI Common Lisp\" and \" Hackers and Painters.\" He is also the co-founder of Y Combinator, a venture capital firm that invests in early-stage technology companies.\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Configure the parameter passed to the model like - `max_tokens`, `temperature`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ai21 import AI21\n",
    "\n",
    "llm = AI21(model=\"jamba-instruct\", api_key=api_key, max_tokens=100, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.complete(\"Paul Graham is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "an American computer scientist, essayist, and venture capitalist. He is best known for his work on Lisp, programming language design, and entrepreneurship. Graham has written several books on these topics, including \" ANSI Common Lisp\" and \" Hackers and Painters.\" He is also the co-founder of Y Combinator, a venture capital firm that invests in early-stage technology companies.\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set API Key at a per-instance level\n",
    "If desired, you can have separate LLM instances use separate API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "an American computer scientist, essayist, and venture capitalist. He is best known for his work on Lisp, programming language design, and entrepreneurship. Graham has written several books on these topics, including \"Hackers and Painters\" and \"On Lisp.\" He is also the co-founder of Y Combinator, a venture capital firm that invests in early-stage technology companies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling POST https://api.ai21.com/studio/v1/j2-mid/complete failed with a non-200 response code: 401\n"
     ]
    },
    {
     "ename": "Unauthorized",
     "evalue": "Failed with http status code: 401 (Unauthorized). Details: {\"detail\":\"Forbidden: Bad or missing API token.\"}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnauthorized\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m/home/amit/Desktop/projects/lindex/llama_index/docs/examples/llm/ai21.ipynb Cell 14\u001B[0m line \u001B[0;36m9\n\u001B[1;32m      <a href='vscode-notebook-cell:/home/amit/Desktop/projects/lindex/llama_index/docs/examples/llm/ai21.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001B[0m resp \u001B[39m=\u001B[39m llm_good\u001B[39m.\u001B[39mcomplete(\u001B[39m\"\u001B[39m\u001B[39mPaul Graham is \u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m      <a href='vscode-notebook-cell:/home/amit/Desktop/projects/lindex/llama_index/docs/examples/llm/ai21.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001B[0m \u001B[39mprint\u001B[39m(resp)\n\u001B[0;32m----> <a href='vscode-notebook-cell:/home/amit/Desktop/projects/lindex/llama_index/docs/examples/llm/ai21.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001B[0m resp \u001B[39m=\u001B[39m llm_bad\u001B[39m.\u001B[39;49mcomplete(\u001B[39m\"\u001B[39;49m\u001B[39mPaul Graham is \u001B[39;49m\u001B[39m\"\u001B[39;49m)\n\u001B[1;32m     <a href='vscode-notebook-cell:/home/amit/Desktop/projects/lindex/llama_index/docs/examples/llm/ai21.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001B[0m \u001B[39mprint\u001B[39m(resp)\n",
      "File \u001B[0;32m~/Desktop/projects/lindex/llama_index/llama_index/llms/base.py:312\u001B[0m, in \u001B[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001B[0;34m(_self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[39mwith\u001B[39;00m wrapper_logic(_self) \u001B[39mas\u001B[39;00m callback_manager:\n\u001B[1;32m    303\u001B[0m     event_id \u001B[39m=\u001B[39m callback_manager\u001B[39m.\u001B[39mon_event_start(\n\u001B[1;32m    304\u001B[0m         CBEventType\u001B[39m.\u001B[39mLLM,\n\u001B[1;32m    305\u001B[0m         payload\u001B[39m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    309\u001B[0m         },\n\u001B[1;32m    310\u001B[0m     )\n\u001B[0;32m--> 312\u001B[0m     f_return_val \u001B[39m=\u001B[39m f(_self, \u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m    313\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(f_return_val, Generator):\n\u001B[1;32m    314\u001B[0m         \u001B[39m# intercept the generator and add a callback to the end\u001B[39;00m\n\u001B[1;32m    315\u001B[0m         \u001B[39mdef\u001B[39;00m \u001B[39mwrapped_gen\u001B[39m() \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m CompletionResponseGen:\n",
      "File \u001B[0;32m~/Desktop/projects/lindex/llama_index/llama_index/llms/ai21.py:104\u001B[0m, in \u001B[0;36mAI21.complete\u001B[0;34m(self, prompt, **kwargs)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mai21\u001B[39;00m\n\u001B[1;32m    102\u001B[0m ai21\u001B[39m.\u001B[39mapi_key \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_api_key\n\u001B[0;32m--> 104\u001B[0m response \u001B[39m=\u001B[39m ai21\u001B[39m.\u001B[39;49mCompletion\u001B[39m.\u001B[39;49mexecute(\u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mall_kwargs, prompt\u001B[39m=\u001B[39;49mprompt)\n\u001B[1;32m    106\u001B[0m \u001B[39mreturn\u001B[39;00m CompletionResponse(\n\u001B[1;32m    107\u001B[0m     text\u001B[39m=\u001B[39mresponse[\u001B[39m\"\u001B[39m\u001B[39mcompletions\u001B[39m\u001B[39m\"\u001B[39m][\u001B[39m0\u001B[39m][\u001B[39m\"\u001B[39m\u001B[39mdata\u001B[39m\u001B[39m\"\u001B[39m][\u001B[39m\"\u001B[39m\u001B[39mtext\u001B[39m\u001B[39m\"\u001B[39m], raw\u001B[39m=\u001B[39mresponse\u001B[39m.\u001B[39m\u001B[39m__dict__\u001B[39m\n\u001B[1;32m    108\u001B[0m )\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/llama-index-2x1vjWb5-py3.10/lib/python3.10/site-packages/ai21/modules/resources/nlp_task.py:22\u001B[0m, in \u001B[0;36mNLPTask.execute\u001B[0;34m(cls, **params)\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mcls\u001B[39m\u001B[39m.\u001B[39m_execute_sm(destination\u001B[39m=\u001B[39mdestination, params\u001B[39m=\u001B[39mparams)\n\u001B[1;32m     21\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(destination, AI21Destination):\n\u001B[0;32m---> 22\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mcls\u001B[39;49m\u001B[39m.\u001B[39;49m_execute_studio_api(params)\n\u001B[1;32m     24\u001B[0m \u001B[39mraise\u001B[39;00m WrongInputTypeException(key\u001B[39m=\u001B[39mDESTINATION_KEY, expected_type\u001B[39m=\u001B[39mDestination, given_type\u001B[39m=\u001B[39m\u001B[39mtype\u001B[39m(destination))\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/llama-index-2x1vjWb5-py3.10/lib/python3.10/site-packages/ai21/modules/completion.py:69\u001B[0m, in \u001B[0;36mCompletion._execute_studio_api\u001B[0;34m(cls, params)\u001B[0m\n\u001B[1;32m     65\u001B[0m     url \u001B[39m=\u001B[39m \u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39m{\u001B[39;00murl\u001B[39m}\u001B[39;00m\u001B[39m/\u001B[39m\u001B[39m{\u001B[39;00mcustom_model\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m\n\u001B[1;32m     67\u001B[0m url \u001B[39m=\u001B[39m \u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39m{\u001B[39;00murl\u001B[39m}\u001B[39;00m\u001B[39m/\u001B[39m\u001B[39m{\u001B[39;00m\u001B[39mcls\u001B[39m\u001B[39m.\u001B[39mMODULE_NAME\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m\n\u001B[0;32m---> 69\u001B[0m \u001B[39mreturn\u001B[39;00m execute_studio_request(task_url\u001B[39m=\u001B[39;49murl, params\u001B[39m=\u001B[39;49mparams)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/llama-index-2x1vjWb5-py3.10/lib/python3.10/site-packages/ai21/modules/resources/execution_utils.py:11\u001B[0m, in \u001B[0;36mexecute_studio_request\u001B[0;34m(task_url, params, method)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mexecute_studio_request\u001B[39m(task_url: \u001B[39mstr\u001B[39m, params, method: \u001B[39mstr\u001B[39m \u001B[39m=\u001B[39m \u001B[39m'\u001B[39m\u001B[39mPOST\u001B[39m\u001B[39m'\u001B[39m):\n\u001B[1;32m     10\u001B[0m     client \u001B[39m=\u001B[39m AI21StudioClient(\u001B[39m*\u001B[39m\u001B[39m*\u001B[39mparams)\n\u001B[0;32m---> 11\u001B[0m     \u001B[39mreturn\u001B[39;00m client\u001B[39m.\u001B[39;49mexecute_http_request(method\u001B[39m=\u001B[39;49mmethod, url\u001B[39m=\u001B[39;49mtask_url, params\u001B[39m=\u001B[39;49mparams)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/llama-index-2x1vjWb5-py3.10/lib/python3.10/site-packages/ai21/ai21_studio_client.py:52\u001B[0m, in \u001B[0;36mAI21StudioClient.execute_http_request\u001B[0;34m(self, method, url, params, files)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mexecute_http_request\u001B[39m(\u001B[39mself\u001B[39m, method: \u001B[39mstr\u001B[39m, url: \u001B[39mstr\u001B[39m, params: Optional[Dict] \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m, files\u001B[39m=\u001B[39m\u001B[39mNone\u001B[39;00m):\n\u001B[0;32m---> 52\u001B[0m     response \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mhttp_client\u001B[39m.\u001B[39;49mexecute_http_request(method\u001B[39m=\u001B[39;49mmethod, url\u001B[39m=\u001B[39;49murl, params\u001B[39m=\u001B[39;49mparams, files\u001B[39m=\u001B[39;49mfiles)\n\u001B[1;32m     53\u001B[0m     \u001B[39mreturn\u001B[39;00m convert_to_ai21_object(response)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/llama-index-2x1vjWb5-py3.10/lib/python3.10/site-packages/ai21/http_client.py:84\u001B[0m, in \u001B[0;36mHttpClient.execute_http_request\u001B[0;34m(self, method, url, params, files, auth)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[39mif\u001B[39;00m response\u001B[39m.\u001B[39mstatus_code \u001B[39m!=\u001B[39m \u001B[39m200\u001B[39m:\n\u001B[1;32m     83\u001B[0m     log_error(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mCalling \u001B[39m\u001B[39m{\u001B[39;00mmethod\u001B[39m}\u001B[39;00m\u001B[39m \u001B[39m\u001B[39m{\u001B[39;00murl\u001B[39m}\u001B[39;00m\u001B[39m failed with a non-200 response code: \u001B[39m\u001B[39m{\u001B[39;00mresponse\u001B[39m.\u001B[39mstatus_code\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m)\n\u001B[0;32m---> 84\u001B[0m     handle_non_success_response(response\u001B[39m.\u001B[39;49mstatus_code, response\u001B[39m.\u001B[39;49mtext)\n\u001B[1;32m     86\u001B[0m \u001B[39mreturn\u001B[39;00m response\u001B[39m.\u001B[39mjson()\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/llama-index-2x1vjWb5-py3.10/lib/python3.10/site-packages/ai21/http_client.py:23\u001B[0m, in \u001B[0;36mhandle_non_success_response\u001B[0;34m(status_code, response_text)\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[39mraise\u001B[39;00m BadRequest(details\u001B[39m=\u001B[39mresponse_text)\n\u001B[1;32m     22\u001B[0m \u001B[39mif\u001B[39;00m status_code \u001B[39m==\u001B[39m \u001B[39m401\u001B[39m:\n\u001B[0;32m---> 23\u001B[0m     \u001B[39mraise\u001B[39;00m Unauthorized(details\u001B[39m=\u001B[39mresponse_text)\n\u001B[1;32m     24\u001B[0m \u001B[39mif\u001B[39;00m status_code \u001B[39m==\u001B[39m \u001B[39m422\u001B[39m:\n\u001B[1;32m     25\u001B[0m     \u001B[39mraise\u001B[39;00m UnprocessableEntity(details\u001B[39m=\u001B[39mresponse_text)\n",
      "\u001B[0;31mUnauthorized\u001B[0m: Failed with http status code: 401 (Unauthorized). Details: {\"detail\":\"Forbidden: Bad or missing API token.\"}"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ai21 import AI21\n",
    "\n",
    "llm_good = AI21(api_key=api_key)\n",
    "llm_bad = AI21(model=\"j2-mid\", api_key=\"BAD_KEY\")\n",
    "\n",
    "resp = llm_good.complete(\"Paul Graham is \")\n",
    "print(resp)\n",
    "\n",
    "resp = llm_bad.complete(\"Paul Graham is \")\n",
    "print(resp)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Streaming"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "using `stream_chat` method"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T06:14:22.431244Z",
     "start_time": "2024-06-09T06:14:21.937653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.llms.ai21 import AI21\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "llm = AI21(api_key=api_key, model=\"jamba-instruct\")\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a story\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T06:14:28.299044Z",
     "start_time": "2024-06-09T06:14:24.456705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None Once upon a time, there was a pirate named Captain Jack. He was a very colorful pirate with a love for adventure and treasure. He sailed the seven seas, always searching for his next big score.\n",
      "\n",
      "One day, he stumbled upon a map that led to a hidden island filled with gold and riches beyond his wildest dreams. He gathered his loyal crew and set sail towards the island.\n",
      "\n",
      "As they approached the island, they were met with treacherous waters and dangerous obstacles. But Captain Jack was determined to find the treasure, and he wouldn't let anything stand in his way.\n",
      "\n",
      "Finally, after days of searching, they found the hidden island. They docked their ship and made their way through the dense jungle, following the map to the treasure.\n",
      "\n",
      "As they approached the treasure, they were met with a group of fierce warriors who guarded the treasure. But Captain Jack and his crew were not afraid. They fought bravely and defeated the warriors, finally reaching the treasure.\n",
      "\n",
      "They opened the treasure chest to find it filled with gold, jewels, and other valuable treasures. They were overjoyed and celebrated their victory.\n",
      "\n",
      "But Captain Jack knew that they couldn't stay on the island forever. They loaded their ship with the treasure and set sail back home.\n",
      "\n",
      "As they sailed away, they were met with a fierce storm that threatened to sink their ship. But Captain Jack was determined to make it back home with the treasure. He fought against the storm with all his might, and eventually, they made it through.\n",
      "\n",
      "Captain Jack and his crew finally made it back to their home port, where they were greeted as heroes. They had returned with a treasure that would make them rich beyond their wildest dreams.\n",
      "\n",
      "And so, Captain Jack and his crew lived happily ever after, sailing the seas and searching for their next big adventure."
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The type of the tokenizer is determined by the name of the model "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from llama_index.llms.ai21 import AI21\n",
    "\n",
    "llm = AI21(api_key=api_key, model=\"jamba-instruct\")\n",
    "\n",
    "tokenizer = llm.tokenizer\n",
    "\n",
    "tokens = tokenizer.encode(\"Hello llama-index!\")\n",
    "\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-2x1vjWb5-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
