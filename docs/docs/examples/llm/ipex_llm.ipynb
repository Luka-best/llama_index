{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPEX-LLM \n",
    "\n",
    "> [IPEX-LLM](https://github.com/intel-analytics/ipex-llm/) is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latenc. This PR adds ipex-llm integrations to LlamaIndex as a new integration package.\n",
    "\n",
    "This example goes over how to use LlamaIndex to interact with IPEX-LLM for text generation on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-ipex-llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `ipex-llm` for CPU. For more details, refer to [`ipex-llm` Install Guide](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Overview/install_cpu.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --pre --upgrade ipex-llm[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exmaple we use [HuggingFaceH4/zephyr-7b-alpha](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha) model for demostration. Updating transformers and tokenizers is required to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U transformers==4.37.0 tokenizers==0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/shane-llamaindex/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:00<00:00, 18.55it/s]\n",
      "2024-03-28 06:09:17,424 - INFO - Converting the current model to sym_int4 format......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tokenizer: /mnt/disk1/models/zephyr-7b-alpha\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ipex_llm import IpexLLM\n",
    "\n",
    "llm = IpexLLM(\n",
    "    model_name=\"/mnt/disk1/models/zephyr-7b-alpha\",\n",
    "    tokenizer_name=\"/mnt/disk1/models/zephyr-7b-alpha\",\n",
    "    # model_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    # tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    context_window=512,\n",
    "    max_new_tokens=128,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/shane-llamaindex/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In a land far, far away, \n",
      "\n",
      "Lived a young girl named Lily, \n",
      "\n",
      "Who had a dream to chase every day. \n",
      "\n",
      "Lily loved to read, write, and draw, \n",
      "\n",
      "She'd spend hours in her room, \n",
      "\n",
      "Creating stories, characters, and worlds, \n",
      "\n",
      "That only she knew. \n",
      "\n",
      "One day, Lily's teacher said, \n",
      "\n",
      "\"Lily, you're a talented writer, \n",
      "\n",
      "But you need to work on your grammar, \n",
      "\n",
      "And your spelling, and your\n"
     ]
    }
   ],
   "source": [
    "completion_response = llm.complete(\"Once upon a time, \")\n",
    "print(completion_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Text Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named Lily. She loves to play with her toys, especially her teddy bear named Ted. One day, Lily's mommy told her that they're going to the park. Lily was so excited because she loves to play in the park. When they arrived at the park, Lily saw a big slide. She ran to it and climbed up. She was so scared to slide down, but she wanted to try it. She closed her eyes and slid down. She felt so fast and so high. She opened her eyes and saw her mommy waving at her. She smiled and waved back. She slid down again"
     ]
    }
   ],
   "source": [
    "response_iter = llm.stream_complete(\"Once upon a time, there's a little girl\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: <|assistant|>\n",
      "The Big Bang Theory is a popular American sitcom that aired from 2007 to 2019. The show follows the lives of two brilliant but socially awkward physicists, Leonard Hofstadter and Sheldon Cooper, and their friends and colleagues, Penny, Raj, and Amy. The show is set in Pasadena, California, and revolves around the characters' work at Caltech and their personal lives. The show's humor is often based on science and pop culture references, and it explores themes such as friendship, love, and the\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "message = ChatMessage(role=\"user\", content=\"Explain Big Bang Theory briefly\")\n",
    "resp = llm.chat([message])\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|assistant|>\n",
      "AI, or artificial intelligence, is a branch of computer science that focuses on creating intelligent machines that can learn, reason, and make decisions like humans do. It involves the use of algorithms and statistical models to enable computers to perform tasks that would normally require human intelligence, such as recognizing speech, understanding natural language, and making predictions based on data. AI is used in a variety of fields, including healthcare, finance, and transportation, to improve efficiency, accuracy, and safety."
     ]
    }
   ],
   "source": [
    "message = ChatMessage(role=\"user\", content=\"What is AI?\")\n",
    "resp = llm.stream_chat([message], max_tokens=1000)\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shane-llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
