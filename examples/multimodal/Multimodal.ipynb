{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493dd86c",
   "metadata": {},
   "source": [
    "# Multimodal Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b2e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4073749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTSimpleVectorIndex\n",
    "from llama_index.readers.file.base import (\n",
    "    DEFAULT_FILE_EXTRACTOR, \n",
    "    ImageParser,\n",
    ")\n",
    "from gpt_index.response.notebook_utils import (\n",
    "    display_response, \n",
    "    display_image,\n",
    ")\n",
    "from gpt_index.indices.query.query_transform.base import (\n",
    "    ImageOutputQueryTransform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b74e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: By default, image parser converts image into text and discard the original image.  \n",
    "#       Here, we explicitly keep both the original image and parsed text in an image document\n",
    "image_parser = ImageParser(keep_image=True, parse_text=True)\n",
    "file_extractor = DEFAULT_FILE_EXTRACTOR\n",
    "file_extractor.update(\n",
    "{\n",
    "    \".jpg\": image_parser,\n",
    "    \".png\": image_parser,\n",
    "    \".jpeg\": image_parser,\n",
    "})\n",
    "\n",
    "# NOTE: we add filename as metadata for all documents\n",
    "filename_fn = lambda filename: {'file_name': filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca801c8c",
   "metadata": {},
   "source": [
    "## Q&A over Receipt Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cce8e4",
   "metadata": {},
   "source": [
    "We first ingest our receipt images with the *custom* `image parser` and `metadata function` defined above.   \n",
    "This gives us `image documents` instead of only text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc28f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "receipt_reader = SimpleDirectoryReader(\n",
    "    input_dir='data/receipts', \n",
    "    file_extractor=file_extractor, \n",
    "    file_metadata=filename_fn,\n",
    ")\n",
    "receipt_documents = receipt_reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd6f45",
   "metadata": {},
   "source": [
    "We build a simple vector index as usual, but unlike before, our index holds images in addition to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629cab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 2180 tokens\n"
     ]
    }
   ],
   "source": [
    "receipts_index = GPTSimpleVectorIndex.from_documents(receipt_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef454f",
   "metadata": {},
   "source": [
    "We can now ask a question that prompts for response with both text and image.  \n",
    "We use a custom query transform `ImageOutputQueryTransform` to add instruction on how to display the image nicely in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c078dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1004 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 30 tokens\n"
     ]
    }
   ],
   "source": [
    "receipts_response = receipts_index.query(\n",
    "    'When was the last time I went to McDonald\\'s and how much did I spend. \\\n",
    "    Also show me the receipt from my visit.',\n",
    "    query_transform=ImageOutputQueryTransform(width=400)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c180ae",
   "metadata": {},
   "source": [
    "We now have rich multimodal response with inline text and image!  \n",
    "\n",
    "The source nodes section gives additional details on retrieved data used for synthesizing the final response.  \n",
    "In this case, we can verify that the receipt for McDonald's is correctly retrieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "810ad2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The last time you went to McDonald's was on 03/10/2018 at 07:39:12 PM and you spent $26.15. Here is the receipt from your visit: <img src=\"data/receipts/1100-receipt.jpg\" width=\"400\" />"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/1`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Document ID:** 5b8ee185-d9ac-44f4-9abe-1d08b7277db8<br>**Similarity:** 0.7981665332785771<br>**Text:** file_name: data/receipts/1100-receipt.jpg\n",
       "\n",
       "<s_menu><s_nm> Story</s_nm><s_num> 16725 Stony Platin ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(receipts_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa834925",
   "metadata": {},
   "source": [
    "## Q & A over LlamaIndex Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a82a54",
   "metadata": {},
   "source": [
    "We now demo the same for Q&A over LlamaIndex documentations.   \n",
    "This demo higlights the ability to synthesize multimodal output with a mixture of text and image documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f04295",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_reader = SimpleDirectoryReader(\n",
    "    input_dir='data/llama',\n",
    "    file_extractor=file_extractor, \n",
    "    file_metadata=filename_fn,\n",
    ")\n",
    "llama_documents = llama_reader.load_data(concatenate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46db4191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 965 tokens\n"
     ]
    }
   ],
   "source": [
    "llama_index = GPTSimpleVectorIndex.from_documents(llama_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a4cc090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1475 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 13 tokens\n"
     ]
    }
   ],
   "source": [
    "llama_response = llama_index.query(\n",
    "    'Show an image to illustrate how tree index works and explain briefly.', \n",
    "    query_transform=ImageOutputQueryTransform(width=400),\n",
    "    similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559624a6",
   "metadata": {},
   "source": [
    "By inspecting the 2 source nodes, we see relevant text and image describing the tree index are retrieved for synthesizing the final multimodal response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c5721d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** This image illustrates how a tree index works. A tree index is a type of data structure that stores data in a hierarchical structure. It is composed of nodes, which can have multiple children and a single parent. Each node contains data, such as a key, value, or other information. The tree index is used to quickly search for data by traversing the tree from the root node to the desired node. During query time, we traverse from root nodes down to leaf nodes. By default, (`child_branch_factor=1`), a query chooses one child node given a parent node. If `child_branch_factor=2`, a query chooses two child nodes per parent. LlamaIndex also offers different methods of synthesizing a response, such as Create and Refine and Tree Summarize. Create and Refine is an iterative way of generating a response, while Tree Summarize builds a tree index over the set of candidate nodes with a summary prompt seeded with the query."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Document ID:** c1d13929-ecb4-4d9e-8d76-ead7df960ae1<br>**Similarity:** 0.815091548290817<br>**Text:** file_name: data/llama/tree_index.png\n",
       "\n",
       "<s_menu><s_nm> Root Node</s_nm><s_unitprice> Parent</s_nm><...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Document ID:** c75338b8-cee3-46ca-8b73-beefa50e387a<br>**Similarity:** 0.8134417398079422<br>**Text:** How Each Index Works\n",
       "\n",
       "This guide describes how each index works with diagrams. We also visually h...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(llama_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7376e",
   "metadata": {},
   "source": [
    "We show another example asking about vector store index instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92569825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1404 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 14 tokens\n"
     ]
    }
   ],
   "source": [
    "llama_response = llama_index.query(\n",
    "    'Show an image to illustrate how vector store index works and explain briefly.', \n",
    "    query_transform=ImageOutputQueryTransform(width=400),\n",
    "    similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cfdd68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** <img src=\"data/llama/vector_store_index.png\" width=\"400\" />\n",
       "Vector store index is a way of storing data in a vector format. It is used to store data in a way that is easy to access and manipulate. The data is stored in a vector format, which is a collection of numbers that represent the data. This makes it easier to access and manipulate the data, as well as to store it in a more efficient way. Vector store index stores each Node and a corresponding embedding in a Vector Store. During query time, we extract relevant keywords from the query, and match those with pre-extracted Node keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our Response Synthesis module, which can be configured to use different methods of synthesizing a response, such as Create and Refine or Tree Summarize."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Document ID:** 957904ba-d811-41b1-b156-1d9e1cb7ceff<br>**Similarity:** 0.816241967681269<br>**Text:** file_name: data/llama/vector_store_index.png\n",
       "\n",
       "<s_menu><s_nm> Nodel</s_nm><s_unitprice> Node2</s_u...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Document ID:** c75338b8-cee3-46ca-8b73-beefa50e387a<br>**Similarity:** 0.7878806797032482<br>**Text:** How Each Index Works\n",
       "\n",
       "This guide describes how each index works with diagrams. We also visually h...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(llama_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
