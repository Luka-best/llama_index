{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d43caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4921c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import GPTSimpleVectorIndex, SimpleDirectoryReader, LLMPredictor, PromptHelper\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "261d923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23b5169",
   "metadata": {},
   "source": [
    "# davinci-003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c635cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8ad1a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_documents] Total embedding token usage: 17598 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58aac80a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index._index_struct.nodes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f843a73",
   "metadata": {},
   "source": [
    "# gpt-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0849d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb9eff4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "llm must be an instance of langchain.llms.base.LLM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gpt4_index \u001b[38;5;241m=\u001b[39m \u001b[43mGPTSimpleVectorIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_predictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_predictor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/vector_store/vector_indices.py:84\u001b[0m, in \u001b[0;36mGPTSimpleVectorIndex.__init__\u001b[0;34m(self, documents, index_struct, text_qa_template, llm_predictor, embed_model, simple_vector_store_data_dict, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Init params.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m SimpleVectorStore(\n\u001b[1;32m     81\u001b[0m     simple_vector_store_data_dict\u001b[38;5;241m=\u001b[39msimple_vector_store_data_dict\n\u001b[1;32m     82\u001b[0m )\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_predictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# TODO: Temporary hack to also store embeddings in index_struct\u001b[39;00m\n\u001b[1;32m     95\u001b[0m embedding_dict \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39membedding_dict\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/vector_store/base.py:63\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex.__init__\u001b[0;34m(self, documents, index_struct, text_qa_template, llm_predictor, embed_model, vector_store, text_splitter, use_async, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_qa_template \u001b[38;5;241m=\u001b[39m text_qa_template \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TEXT_QA_PROMPT\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async \u001b[38;5;241m=\u001b[39m use_async\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_predictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_splitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/base.py:89\u001b[0m, in \u001b[0;36mBaseGPTIndex.__init__\u001b[0;34m(self, documents, index_struct, llm_predictor, embed_model, docstore, index_registry, prompt_helper, text_splitter, chunk_size_limit, include_extra_info)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_include_extra_info \u001b[38;5;241m=\u001b[39m include_extra_info\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# TODO: move out of base if we need custom params per index\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_helper \u001b[38;5;241m=\u001b[39m prompt_helper \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mPromptHelper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm_predictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm_predictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size_limit\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_splitter \u001b[38;5;241m=\u001b[39m text_splitter \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_fallback_text_splitter()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# build index struct in the init function\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/prompt_helper.py:69\u001b[0m, in \u001b[0;36mPromptHelper.from_llm_predictor\u001b[0;34m(self, llm_predictor, max_chunk_overlap, embedding_limit, chunk_size_limit, tokenizer)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_llm_predictor\u001b[39m(\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     tokenizer: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], List]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPromptHelper\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create from llm predictor.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    This will autofill values like max_input_size and num_output.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     llm_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mllm_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_llm_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     max_chunk_overlap \u001b[38;5;241m=\u001b[39m max_chunk_overlap \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m     71\u001b[0m         MAX_CHUNK_OVERLAP,\n\u001b[1;32m     72\u001b[0m         llm_metadata\u001b[38;5;241m.\u001b[39mmax_input_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk_size_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/langchain_helpers/chain_wrapper.py:91\u001b[0m, in \u001b[0;36mLLMPredictor.get_llm_metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# TODO: refactor mocks in unit tests, this is a stopgap solution\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_llm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_llm_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMMetadata()\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/langchain_helpers/chain_wrapper.py:38\u001b[0m, in \u001b[0;36m_get_llm_metadata\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get LLM metadata from llm.\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, BaseLLM):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm must be an instance of langchain.llms.base.LLM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, OpenAI):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMMetadata(\n\u001b[1;32m     41\u001b[0m         max_input_size\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodelname_to_contextsize(llm\u001b[38;5;241m.\u001b[39mmodel_name),\n\u001b[1;32m     42\u001b[0m         num_output\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39mmax_tokens,\n\u001b[1;32m     43\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: llm must be an instance of langchain.llms.base.LLM"
     ]
    }
   ],
   "source": [
    "gpt4_index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c04b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
