{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9f19f3",
   "metadata": {},
   "source": [
    "# LlamaIndex / Predibase Integration\n",
    "\n",
    "This notebook shows how you can use [Predibase](https://predibase.com) to manage the prompts you use within LlamaIndex. By registering LlamaIndex prompts with Vellum via the `VellumPredictor` class, you're able to use Vellum to:\n",
    "1. View and manage all your prompts in a UI\n",
    "2. Compare and constrast prompt templates, and even different LLM providers/models side-by-side\n",
    "3. Update your prompt, or the provider/model backing it, without updating or re-deploying code\n",
    "4. Set up unit tests to test your prompt and quantitatively evaluate the quality of its outputs\n",
    "5. Observe and monitor all traffic to it, including the inputs sent to the model, and the outputs sent back\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. Sign up for a free Predibase account [here](https://predibase.com/free-trial)\n",
    "2. Create an Account\n",
    "3. Go to Settings > My profile and Generate a new API Token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a726c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-index --quiet\n",
    "!pip install predibase --quiet\n",
    "!pip install sentence-transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\n",
    "    \"PREDIBASE_API_TOKEN\"\n",
    "] = \"{PREDIBASE_API_TOKEN}\"\n",
    "from llama_index.llms import PredibaseLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a602a2a",
   "metadata": {},
   "source": [
    "## Flow 1: Query Predibase LLM directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baffaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = PredibaseLLM(model_name=\"llama-2-13b\", temperature=0.3, max_new_tokens=512)\n",
    "# You can query any HuggingFace or fine-tuned LLM that's hosted on Predibase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7039a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.complete(\"Can you recommend me a nice dry white wine?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112e828",
   "metadata": {},
   "source": [
    "## Flow 2: Retrieval Augmented Generation (RAG) with Predibase LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacff36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd41d1",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5941151",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"../paul_graham_essay/data\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df4407f",
   "metadata": {},
   "source": [
    "### Configure Predibase LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec73a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = PredibaseLLM(\n",
    "    model_name=\"llama-2-13b\", temperature=0.3, max_new_tokens=400, context_window=1024\n",
    ")\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024, llm=llm, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a131a8e",
   "metadata": {},
   "source": [
    "### Setup and Query Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b10269",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
