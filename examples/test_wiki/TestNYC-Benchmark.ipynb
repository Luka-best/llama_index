{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "9080b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Uncomment if you want to temporarily disable logger\n",
    "# logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "7de92ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a9eb90-335c-4214-8bb6-fd1edbe3ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My OpenAI Key\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "8d0b2364-4806-4656-81e7-3f6e4b910b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import GPTTreeIndex, SimpleDirectoryReader, LLMPredictor, GPTSimpleVectorIndex, GPTListIndex, Prompt\n",
    "from gpt_index.indices.base import BaseGPTIndex\n",
    "from gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from gpt_index.response.schema import Response\n",
    "import pandas as pd\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707662e5",
   "metadata": {},
   "source": [
    "# Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b4387b-413e-4016-ba1e-88b3d9410a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch \"New York City\" page from Wikipedia\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "response = requests.get(\n",
    "    'https://en.wikipedia.org/w/api.php',\n",
    "    params={\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'New York City',\n",
    "        'prop': 'extracts',\n",
    "        # 'exintro': True,\n",
    "        'explaintext': True,\n",
    "    }\n",
    ").json()\n",
    "page = next(iter(response['query']['pages'].values()))\n",
    "nyc_text = page['extract']\n",
    "\n",
    "data_path = Path('data')\n",
    "if not data_path.exists():\n",
    "    Path.mkdir(data_path)\n",
    "\n",
    "with open('data/nyc_text.txt', 'w') as fp:\n",
    "    fp.write(nyc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "523fbebe-6e79-4d7b-b400-188b711a0e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a269bd",
   "metadata": {},
   "source": [
    "# Setup benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62f01ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ff13cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestCase:\n",
    "    query: str \n",
    "    must_contain: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9c653b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestOutcome:\n",
    "    test: TestCase\n",
    "    response: Response\n",
    "    \n",
    "    @property\n",
    "    def is_correct_response(self) -> bool:\n",
    "        \n",
    "        is_correct = True\n",
    "        for answer in self.test.must_contain:\n",
    "            if answer not in self.response.response:\n",
    "                is_correct = False\n",
    "        return is_correct\n",
    "    \n",
    "    @property\n",
    "    def is_correct_source(self) -> bool:\n",
    "        is_correct = True\n",
    "        for answer in self.test.must_contain:\n",
    "            if all(answer not in node.source_text for node in self.response.source_nodes):\n",
    "                is_correct = False\n",
    "        return is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "b9cd18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Benchmark:\n",
    "    def __init__(self, tests: List[TestCase]) -> None:\n",
    "        self._tests = tests\n",
    "    \n",
    "    def test(self, index: BaseGPTIndex, llm_predictor: LLMPredictor, **kwargs) -> List[TestOutcome]:\n",
    "        outcomes: List[TestOutcome] = []\n",
    "        for test in self._tests:\n",
    "            response = index.query(\n",
    "                test.query,\n",
    "                llm_predictor=llm_predictor,\n",
    "                **kwargs\n",
    "            )\n",
    "            outcome = TestOutcome(test=test, response=response)\n",
    "            outcomes.append(outcome)\n",
    "        return outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8edad985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_outcome(outcomes: List[TestOutcome]) -> None:\n",
    "    rows = []\n",
    "    for outcome in outcomes:\n",
    "        row = [outcome.test.query, outcome.is_correct_response, outcome.is_correct_source]\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows, columns=['Test Query', 'Correct Response', 'Correct Source'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "4bc38077",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_battle = TestCase(\n",
    "    query=\"What battles took place in New York City in the American Revolution?\",\n",
    "    must_contain=[\"Battle of Long Island\"]\n",
    ")\n",
    "\n",
    "test_mayor = TestCase(\n",
    "    query='Who was elected as the mayor after the Great Depression?',\n",
    "    must_contain=[\"Fiorello La Guardia\"]\n",
    ")\n",
    "\n",
    "test_tourists = TestCase(\n",
    "    query='How many tourists visited New York City in 2019?',\n",
    "    must_contain=['66.6 million']\n",
    ")\n",
    "test_airport = TestCase(\n",
    "    query='What are the airports in New York City?',\n",
    "    must_contain=['LaGuardia Airport']\n",
    ")\n",
    "test_visit = TestCase(\n",
    "    query='When was the first documented visit into New York Harbor?',\n",
    "    must_contain=['1524']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "f159dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = Benchmark([\n",
    "    test_battle,\n",
    "    test_mayor,\n",
    "    test_tourists,\n",
    "    test_airport,\n",
    "    test_visit,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ddbd56",
   "metadata": {},
   "source": [
    "# LLM based evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "ed175de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index.prompts.prompt_type import PromptType\n",
    "\n",
    "EVAL_PROMPT_TMPL = (\n",
    "    \"Given the question below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{query_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Decide if the following retreived context is relevant. \\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Then decide if the answer is correct. \\n\"\n",
    "    \".\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"{answer_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Answer in the following format:\\n\"\n",
    "    \"'Context is relevant: <True>\\nAnswer is correct: <True>' \"\n",
    "    \"and explain why.\"\n",
    ")\n",
    "\n",
    "class EvalPrompt(Prompt):\n",
    "    prompt_type: PromptType = PromptType.CUSTOM\n",
    "    input_variables: List[str] = [\"query_str\", 'context_str', 'answer_str']\n",
    "\n",
    "DEFAULT_EVAL_PROMPT = EvalPrompt(EVAL_PROMPT_TMPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "93c498b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_eval_result(result_str: str):\n",
    "    boolean_pattern = r\"(True|False)\"\n",
    "    matches = re.findall(boolean_pattern, result_str)\n",
    "    return [match == \"True\" for match in matches]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "4c8109c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_outcome_llm_single(outcome: TestOutcome, llm_predictor: LLMPredictor) -> Tuple[bool, bool]:\n",
    "    try:\n",
    "        source_text = outcome.response.source_nodes[0].source_text\n",
    "    except:\n",
    "        source_text = \"Failed to retrieve any context\"\n",
    "    result_str, _ = llm_predictor.predict(\n",
    "        DEFAULT_EVAL_PROMPT,\n",
    "        query_str=outcome.test.query,\n",
    "        context_str=source_text,\n",
    "        answer_str=outcome.response.response\n",
    "    )\n",
    "    is_context_relevant, is_answer_correct = extract_eval_result(result_str)\n",
    "    return is_answer_correct, is_context_relevant, result_str\n",
    "\n",
    "def analyze_outcome_llm(outcomes: List[TestOutcome], llm_predictor: LLMPredictor) -> None:\n",
    "    rows = []\n",
    "    for outcome in outcomes:\n",
    "        is_correct_response, is_correct_source, result_str = analyze_outcome_llm_single(outcome, llm_predictor)\n",
    "        row = [outcome.test.query, is_correct_response, is_correct_source, result_str]\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows, columns=['Test Query', 'Correct Response (LLM)', 'Correct Source (LLM)', 'Eval (LLM)'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f43a6",
   "metadata": {},
   "source": [
    "# Build Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "790bad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = GPTSimpleVectorIndex(\n",
    "    documents, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "64c970e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_index = GPTListIndex(\n",
    "    documents, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "bacc4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_index = GPTTreeIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "a600d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save indices\n",
    "vector_index.save_to_disk('vector_index.json')\n",
    "tree_index.save_to_disk('tree_index.json')\n",
    "list_index.save_to_disk('list_index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "5eec265d-211b-4f26-b05b-5b4e7072bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load indices\n",
    "tree_index = GPTTreeIndex.load_from_disk('tree_index.json')\n",
    "list_index = GPTListIndex.load_from_disk('list_index.json')\n",
    "vector_index = GPTSimpleVectorIndex.load_from_disk('vector_index.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e7fdd",
   "metadata": {},
   "source": [
    "# Create LLMPredictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4766ac56-ac8d-4f33-b994-6901964241ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gpt-4\n",
    "llm_predictor_gpt4 = LLMPredictor(\n",
    "    llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c8692cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-3 (text-davinci-003)\n",
    "llm_predictor_gpt3 = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "fb74ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatgpt\n",
    "llm_predictor_chatgpt = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354f668",
   "metadata": {},
   "source": [
    "# Benchmarking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01124a3f",
   "metadata": {},
   "source": [
    "### Tree Index + GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "6f418554",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_tree_gpt4 = bm.test(tree_index, llm_predictor_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "de98ceba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Query</th>\n",
       "      <th>Correct Response</th>\n",
       "      <th>Correct Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What battles took place in New York City in th...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was elected as the mayor after the Great D...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many tourists visited New York City in 2019?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the airports in New York City?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was the first documented visit into New Y...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Test Query  Correct Response  \\\n",
       "0  What battles took place in New York City in th...              True   \n",
       "1  Who was elected as the mayor after the Great D...             False   \n",
       "2   How many tourists visited New York City in 2019?             False   \n",
       "3            What are the airports in New York City?             False   \n",
       "4  When was the first documented visit into New Y...             False   \n",
       "\n",
       "   Correct Source  \n",
       "0            True  \n",
       "1           False  \n",
       "2           False  \n",
       "3           False  \n",
       "4           False  "
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_outcome(outcomes_tree_gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f391b",
   "metadata": {},
   "source": [
    "Use GPT4 to evaluate its own answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef33a0",
   "metadata": {},
   "source": [
    "### Tree Index + GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "ba871d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_tree_gpt3 = bm.test(tree_index, llm_predictor_gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "7d4c6930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Query</th>\n",
       "      <th>Correct Response</th>\n",
       "      <th>Correct Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What battles took place in New York City in th...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was elected as the mayor after the Great D...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many tourists visited New York City in 2019?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the airports in New York City?</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was the first documented visit into New Y...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Test Query  Correct Response  \\\n",
       "0  What battles took place in New York City in th...              True   \n",
       "1  Who was elected as the mayor after the Great D...             False   \n",
       "2   How many tourists visited New York City in 2019?             False   \n",
       "3            What are the airports in New York City?              True   \n",
       "4  When was the first documented visit into New Y...              True   \n",
       "\n",
       "   Correct Source  \n",
       "0           False  \n",
       "1           False  \n",
       "2           False  \n",
       "3           False  \n",
       "4           False  "
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_outcome(outcomes_tree_gpt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a9ba34",
   "metadata": {},
   "source": [
    "### List Index + GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "bc0f05d1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[510], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outcomes_list_gpt4 \u001b[38;5;241m=\u001b[39m \u001b[43mbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_predictor_gpt4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtree_summarize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[495], line 8\u001b[0m, in \u001b[0;36mBenchmark.test\u001b[0;34m(self, index, llm_predictor, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m outcomes: List[TestOutcome] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tests:\n\u001b[0;32m----> 8\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_predictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     outcome \u001b[38;5;241m=\u001b[39m TestOutcome(test\u001b[38;5;241m=\u001b[39mtest, response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m     14\u001b[0m     outcomes\u001b[38;5;241m.\u001b[39mappend(outcome)\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/base.py:421\u001b[0m, in \u001b[0;36mBaseGPTIndex.query\u001b[0;34m(self, query_str, mode, query_transform, use_async, **query_kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m query_config \u001b[38;5;241m=\u001b[39m QueryConfig(\n\u001b[1;32m    406\u001b[0m     index_struct_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct\u001b[38;5;241m.\u001b[39mget_type(),\n\u001b[1;32m    407\u001b[0m     query_mode\u001b[38;5;241m=\u001b[39mmode_enum,\n\u001b[1;32m    408\u001b[0m     query_kwargs\u001b[38;5;241m=\u001b[39mquery_kwargs,\n\u001b[1;32m    409\u001b[0m )\n\u001b[1;32m    410\u001b[0m query_runner \u001b[38;5;241m=\u001b[39m QueryRunner(\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm_predictor,\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_helper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    419\u001b[0m     use_async\u001b[38;5;241m=\u001b[39muse_async,\n\u001b[1;32m    420\u001b[0m )\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_struct\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/query/query_runner.py:149\u001b[0m, in \u001b[0;36mQueryRunner.query\u001b[0;34m(self, query_str_or_bundle, index_struct)\u001b[0m\n\u001b[1;32m    144\u001b[0m query_bundle \u001b[38;5;241m=\u001b[39m query_transform(\n\u001b[1;32m    145\u001b[0m     query_str_or_bundle, extra_info\u001b[38;5;241m=\u001b[39mtransform_extra_info\n\u001b[1;32m    146\u001b[0m )\n\u001b[1;32m    147\u001b[0m query_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_query_obj(index_struct)\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/token_counter/token_counter.py:86\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_llm_predict\u001b[39m(_self: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self):\n\u001b[0;32m---> 86\u001b[0m         f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_return_val\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/query/base.py:387\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery.query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;129m@llm_token_counter\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# if include_summary is True, then include summary text in answer\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# summary text is set through `set_text` on the underlying index.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;66;03m# TODO: refactor response builder to be in the __init__\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_mode \u001b[38;5;241m!=\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mNO_TEXT \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_include_summary:\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/query/base.py:363\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_response_builder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_builder, query_bundle, tuples)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_mode \u001b[38;5;241m!=\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mNO_TEXT:\n\u001b[0;32m--> 363\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_give_response_for_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/query/base.py:255\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery._give_response_for_nodes\u001b[0;34m(self, query_str)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_give_response_for_nodes\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_str: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TEXT_TYPE:\n\u001b[1;32m    254\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Give response for nodes.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/response/builder.py:324\u001b[0m, in \u001b[0;36mResponseBuilder.get_response\u001b[0;34m(self, query_str, prev_response, mode, **response_kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response_compact(query_str, prev_response)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mTREE_SUMMARIZE:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response_tree_summarize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/response/builder.py:288\u001b[0m, in \u001b[0;36mResponseBuilder._get_response_tree_summarize\u001b[0;34m(self, query_str, prev_response, num_children)\u001b[0m\n\u001b[1;32m    283\u001b[0m summary_template \u001b[38;5;241m=\u001b[39m SummaryPrompt\u001b[38;5;241m.\u001b[39mfrom_prompt(text_qa_template)\n\u001b[1;32m    285\u001b[0m index_builder, all_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tree_index_builder_and_nodes(\n\u001b[1;32m    286\u001b[0m     summary_template, query_str, num_children\n\u001b[1;32m    287\u001b[0m )\n\u001b[0;32m--> 288\u001b[0m root_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mindex_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tree_response_over_root_nodes(\n\u001b[1;32m    290\u001b[0m     query_str, prev_response, root_nodes, text_qa_template\n\u001b[1;32m    291\u001b[0m )\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/indices/common/tree/base.py:154\u001b[0m, in \u001b[0;36mGPTTreeIndexBuilder.build_index_from_nodes\u001b[0;34m(self, cur_nodes, all_nodes)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async:\n\u001b[1;32m    148\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm_predictor\u001b[38;5;241m.\u001b[39mapredict(\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary_prompt, context_str\u001b[38;5;241m=\u001b[39mtext_chunk\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks\n\u001b[1;32m    153\u001b[0m     ]\n\u001b[0;32m--> 154\u001b[0m     outputs: List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mrun_async_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m [output[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/jerry_gpt_index/gpt_index/async_utils.py:12\u001b[0m, in \u001b[0;36mrun_async_tasks\u001b[0;34m(tasks)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gather\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[0;32m---> 12\u001b[0m outputs: List[Any] \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/jerry-gpt-index/lib/python3.10/site-packages/nest_asyncio.py:35\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     33\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/miniconda3/envs/jerry-gpt-index/lib/python3.10/site-packages/nest_asyncio.py:84\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     82\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jerry-gpt-index/lib/python3.10/site-packages/nest_asyncio.py:107\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     heappop(scheduled)\n\u001b[1;32m    102\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    105\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 107\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    110\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m~/miniconda3/envs/jerry-gpt-index/lib/python3.10/selectors.py:562\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "outcomes_list_gpt4 = bm.test(list_index, llm_predictor_gpt4, response_mode=\"tree_summarize\", use_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_outcome(outcomes_list_gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba793c",
   "metadata": {},
   "source": [
    "### List Index + GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "66cfa3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_list_gpt3 = bm.test(list_index, llm_predictor_gpt3, response_mode=\"tree_summarize\", use_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "06bc98d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Query</th>\n",
       "      <th>Correct Response</th>\n",
       "      <th>Correct Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What battles took place in New York City in th...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was elected as the mayor during the Great ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many tourists visited New York City in 2019?</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the airports in New York City?</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was the first documented visit into New Y...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Test Query  Correct Response  \\\n",
       "0  What battles took place in New York City in th...              True   \n",
       "1  Who was elected as the mayor during the Great ...              True   \n",
       "2   How many tourists visited New York City in 2019?             False   \n",
       "3            What are the airports in New York City?              True   \n",
       "4  When was the first documented visit into New Y...              True   \n",
       "\n",
       "   Correct Source  \n",
       "0            True  \n",
       "1            True  \n",
       "2            True  \n",
       "3            True  \n",
       "4            True  "
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_outcome(outcomes_list_gpt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0b3eb",
   "metadata": {},
   "source": [
    "### List Index + ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_list_chatgpt = bm.test(list_index, llm_predictor_chatgpt, response_mode=\"tree_summarize\", use_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb9d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_outcome(outcomes_list_chatgpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc1438",
   "metadata": {},
   "source": [
    "### Vector Store Index + GPT4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "5349d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_vector_gpt4 = bm.test(vector_index, llm_predictor_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "7fc53e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Query</th>\n",
       "      <th>Correct Response</th>\n",
       "      <th>Correct Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What battles took place in New York City in th...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was elected as the mayor during the Great ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many tourists visited New York City in 2019?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the airports in New York City?</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was the first documented visit into New Y...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Test Query  Correct Response  \\\n",
       "0  What battles took place in New York City in th...              True   \n",
       "1  Who was elected as the mayor during the Great ...              True   \n",
       "2   How many tourists visited New York City in 2019?             False   \n",
       "3            What are the airports in New York City?              True   \n",
       "4  When was the first documented visit into New Y...              True   \n",
       "\n",
       "   Correct Source  \n",
       "0            True  \n",
       "1            True  \n",
       "2           False  \n",
       "3            True  \n",
       "4            True  "
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_outcome(outcomes_vector_gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb711f",
   "metadata": {},
   "source": [
    "### Vector Store Index + GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "e35ebdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_vector_gpt3 = bm.test(vector_index, llm_predictor_gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "95c49697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Query</th>\n",
       "      <th>Correct Response</th>\n",
       "      <th>Correct Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What battles took place in New York City in th...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was elected as the mayor after the Great D...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many tourists visited New York City in 2019?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the airports in New York City?</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was the first documented visit into New Y...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Test Query  Correct Response  \\\n",
       "0  What battles took place in New York City in th...              True   \n",
       "1  Who was elected as the mayor after the Great D...              True   \n",
       "2   How many tourists visited New York City in 2019?             False   \n",
       "3            What are the airports in New York City?              True   \n",
       "4  When was the first documented visit into New Y...              True   \n",
       "\n",
       "   Correct Source  \n",
       "0            True  \n",
       "1           False  \n",
       "2           False  \n",
       "3           False  \n",
       "4           False  "
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_outcome(outcomes_vector_gpt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ba2ee",
   "metadata": {},
   "source": [
    "# LLM based Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "59ff561c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Query</th>\n",
       "      <th>Correct Response</th>\n",
       "      <th>Correct Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What battles took place in New York City in th...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was elected as the mayor after the Great D...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many tourists visited New York City in 2019?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the airports in New York City?</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was the first documented visit into New Y...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Test Query  Correct Response  \\\n",
       "0  What battles took place in New York City in th...              True   \n",
       "1  Who was elected as the mayor after the Great D...              True   \n",
       "2   How many tourists visited New York City in 2019?             False   \n",
       "3            What are the airports in New York City?              True   \n",
       "4  When was the first documented visit into New Y...              True   \n",
       "\n",
       "   Correct Source  \n",
       "0            True  \n",
       "1           False  \n",
       "2           False  \n",
       "3           False  \n",
       "4           False  "
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_outcome(outcomes_vector_gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "e4ffaca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_gpt4 = analyze_outcome_llm(outcomes_vector_gpt3, llm_predictor_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "85c4e415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Query</th>\n",
       "      <th>Correct Response (LLM)</th>\n",
       "      <th>Correct Source (LLM)</th>\n",
       "      <th>Eval (LLM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What battles took place in New York City in th...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Context is relevant: True\\nAnswer is correct: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was elected as the mayor after the Great D...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Context is relevant: False\\nAnswer is correct:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many tourists visited New York City in 2019?</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Context is relevant: False\\nAnswer is correct:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the airports in New York City?</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Context is relevant: False\\nAnswer is correct:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was the first documented visit into New Y...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Context is relevant: False\\nAnswer is correct:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Test Query  Correct Response (LLM)  \\\n",
       "0  What battles took place in New York City in th...                    True   \n",
       "1  Who was elected as the mayor after the Great D...                    True   \n",
       "2   How many tourists visited New York City in 2019?                    True   \n",
       "3            What are the airports in New York City?                    True   \n",
       "4  When was the first documented visit into New Y...                    True   \n",
       "\n",
       "   Correct Source (LLM)                                         Eval (LLM)  \n",
       "0                  True  Context is relevant: True\\nAnswer is correct: ...  \n",
       "1                 False  Context is relevant: False\\nAnswer is correct:...  \n",
       "2                 False  Context is relevant: False\\nAnswer is correct:...  \n",
       "3                 False  Context is relevant: False\\nAnswer is correct:...  \n",
       "4                 False  Context is relevant: False\\nAnswer is correct:...  "
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "3efb66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chatgpt = analyze_outcome_llm(outcomes_vector_gpt3, llm_predictor_chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "4c452767",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Query</th>\n",
       "      <th>Correct Response (LLM)</th>\n",
       "      <th>Correct Source (LLM)</th>\n",
       "      <th>Eval (LLM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What battles took place in New York City in th...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n\\nContext is relevant: True\\nAnswer is corre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was elected as the mayor after the Great D...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n\\nContext is relevant: True\\nAnswer is corre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many tourists visited New York City in 2019?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n\\nContext is relevant: False\\nAnswer is corr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the airports in New York City?</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n\\nContext is relevant: False\\nAnswer is corr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was the first documented visit into New Y...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n\\nContext is relevant: True\\nAnswer is corre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Test Query  Correct Response (LLM)  \\\n",
       "0  What battles took place in New York City in th...                    True   \n",
       "1  Who was elected as the mayor after the Great D...                    True   \n",
       "2   How many tourists visited New York City in 2019?                   False   \n",
       "3            What are the airports in New York City?                    True   \n",
       "4  When was the first documented visit into New Y...                   False   \n",
       "\n",
       "   Correct Source (LLM)                                         Eval (LLM)  \n",
       "0                  True  \\n\\nContext is relevant: True\\nAnswer is corre...  \n",
       "1                  True  \\n\\nContext is relevant: True\\nAnswer is corre...  \n",
       "2                 False  \\n\\nContext is relevant: False\\nAnswer is corr...  \n",
       "3                 False  \\n\\nContext is relevant: False\\nAnswer is corr...  \n",
       "4                  True  \\n\\nContext is relevant: True\\nAnswer is corre...  "
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "df83616d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nContext is relevant: True\\nAnswer is correct: True. The context provided is relevant to the question as it provides information about the history of New York City and the establishment of the Five Boroughs. The answer is correct as it accurately states that the first documented visit into New York Harbor was in 1524 by Italian explorer Giovanni da Verrazzano.'"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_gpt3.loc[4]['Eval (LLM)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "61e8dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_gpt3 = analyze_outcome_llm(outcomes_vector_gpt3, llm_predictor_gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "170400c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Query</th>\n",
       "      <th>Correct Response (LLM)</th>\n",
       "      <th>Correct Source (LLM)</th>\n",
       "      <th>Eval (LLM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What battles took place in New York City in th...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n\\nContext is relevant: True\\nAnswer is corre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was elected as the mayor after the Great D...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n\\nContext is relevant: True\\nAnswer is corre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many tourists visited New York City in 2019?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n\\nContext is relevant: False\\nAnswer is corr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the airports in New York City?</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n\\nContext is relevant: True\\nAnswer is corre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was the first documented visit into New Y...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n\\nContext is relevant: True\\nAnswer is corre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Test Query  Correct Response (LLM)  \\\n",
       "0  What battles took place in New York City in th...                    True   \n",
       "1  Who was elected as the mayor after the Great D...                    True   \n",
       "2   How many tourists visited New York City in 2019?                   False   \n",
       "3            What are the airports in New York City?                    True   \n",
       "4  When was the first documented visit into New Y...                    True   \n",
       "\n",
       "   Correct Source (LLM)                                         Eval (LLM)  \n",
       "0                  True  \\n\\nContext is relevant: True\\nAnswer is corre...  \n",
       "1                  True  \\n\\nContext is relevant: True\\nAnswer is corre...  \n",
       "2                 False  \\n\\nContext is relevant: False\\nAnswer is corr...  \n",
       "3                  True  \\n\\nContext is relevant: True\\nAnswer is corre...  \n",
       "4                  True  \\n\\nContext is relevant: True\\nAnswer is corre...  "
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_gpt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "7c318acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context is relevant: False\\nAnswer is correct: True\\n\\nThe context provided does not discuss airports in New York City, but rather focuses on sports and other aspects of the city. However, the answer is correct as it lists the three major airports in New York City: John F. Kennedy International Airport (JFK), LaGuardia Airport (LGA), and Newark Liberty International Airport (EWR).'"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_gpt4.loc[3]['Eval (LLM)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7dd22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
