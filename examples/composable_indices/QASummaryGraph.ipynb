{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a54d1c43-4b7f-4917-939f-a964f6f3dafc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa67fa07-1395-4aab-a356-72bdb302f6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d12d766-3ca8-4012-9da2-248be80bb6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerryliu/Programming/gpt_index/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gpt_index.composability.joint_qa_summary import QASummaryGraphBuilder\n",
    "from gpt_index import SimpleDirectoryReader, ServiceContext, LLMPredictor\n",
    "from gpt_index.composability import ComposableGraph\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7cdaf9d-cfbd-4ced-8d4e-6eef8508224d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader('../paul_graham_essay/data')\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bba68f3-2743-437e-93b6-ce9ba92e40c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
      "Unknown max input size for gpt-3.5-turbo, using defaults.\n"
     ]
    }
   ],
   "source": [
    "llm_predictor_gpt4 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm_predictor=llm_predictor_gpt4, chunk_size_limit=1024)\n",
    "\n",
    "llm_predictor_chatgpt = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "service_context_chatgpt = ServiceContext.from_defaults(llm_predictor=llm_predictor_chatgpt, chunk_size_limit=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16216dfb-35ea-49ac-b651-2e8a9e423512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "graph = QASummaryGraphBuilder().build_graph_from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec542141-a0ed-4fb7-8fe7-3ba0027eb6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph.save_to_disk('test_qa_summary_graph.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19834e20-5b11-4b08-920f-d05107359980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = ComposableGraph.load_from_disk('test_qa_summary_graph.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7716977-7f09-4e82-93d7-fa815f327a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set query config\n",
    "query_configs = [\n",
    "    {\n",
    "        \"index_struct_type\": \"simple_dict\",\n",
    "        \"query_mode\": \"default\",\n",
    "        \"query_kwargs\": {\n",
    "            \"similarity_top_k\": 1\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"index_struct_type\": \"list\",\n",
    "        \"query_mode\": \"default\",\n",
    "        \"query_kwargs\": {\n",
    "            \"response_mode\": \"tree_summarize\",\n",
    "            \"use_async\": True,\n",
    "            \"verbose\": True\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"index_struct_type\": \"tree\",\n",
    "        \"query_mode\": \"default\",\n",
    "        \"query_kwargs\": {\n",
    "            \"verbose\": True\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b740281-0644-4a82-b627-a1502c8c9298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph._docstore.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae60000b-403c-4350-af32-71e26cc68a75",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">[Level 0] Current response: ANSWER: 2\n",
      "The chosen summary is \"Use this index for summarization queries,\" as the question explicitly asks for a summary of the author's life. A summarization query requires the generation of a brief summary that contains the most important information about the author's life, and this option is specifically designed to facilitate such queries. On the other hand, option 1 is more suitable for queries that require the retrieval of specific context from documents, which is not applicable to the given question.\n",
      "INFO:gpt_index.indices.query.tree.leaf_query:>[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Node [2] Summary text: Use this index for summarization queries\n",
      "\u001b[36;1m\u001b[1;3m> Got node text: \t\t\n",
      "\n",
      "What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supp...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn't much left for these supposed ultimat...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: mean the sort of AI in which a program that's told \"the dog is sitting on the chair\" translates this into some formal representation and adds it to the list of things it knows.\n",
      "\n",
      "What these programs...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: make enough to survive. And as an artist you could be truly independent. You wouldn't have a boss, or even need to get research funding.\n",
      "\n",
      "I had always liked looking at paintings. Could I make them?...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: fall. This was now only weeks away. My nice landlady let me leave my stuff in her attic. I had some money saved from consulting work I'd done in grad school; there was probably enough to last a yea...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: information-theoretic sense. [4]\n",
      "\n",
      "I liked painting still lives because I was curious about what I was seeing. In everyday life, we aren't consciously aware of much we're seeing. Most visual percept...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: people than sales people (though sales is a real skill and people who are good at it are really good at it), that it leads to bugs when code is edited by too many people, that cheap office space is...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: at RISD, but otherwise I was basically teaching myself to paint, and I could do that for free. So in 1993 I dropped out. I hung around Providence for a bit, and then my college friend Nancy Parmet ...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: and Robert wrote some to resize images and set up an http server to serve the pages. Then we tried to sign up galleries. To call this a difficult sale would be an understatement. It was difficult t...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: in September, but we got more ambitious about the software as we worked on it. Eventually we managed to build a WYSIWYG site builder, in the sense that as you were creating pages, they looked exact...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: of some clever insight that we set the price low. We had no idea what businesses paid for things. $300 a month seemed like a lot of money to us.\n",
      "\n",
      "We did a lot of things right by accident like that....\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: bought us it felt like going from rags to riches. Since we were going to California, I bought a car, a yellow 1998 VW GTI. I remember thinking that its leather seats alone were by far the most luxu...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: old patterns, except now there were doors where there hadn't been. Now when I was tired of walking, all I had to do was raise my hand, and (unless it was raining) a taxi would stop to pick me up. N...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: I doing this? If this vision had to be realized as a company, then screw the vision. I'd build a subset that could be done as an open source project.\n",
      "\n",
      "Much to my surprise, the time I spent working ...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: of the most conspicuous patterns I've noticed in my life is how well it has worked, for me at least, to work on things that weren't prestigious. Still life has always been the least prestigious for...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: start a startup. Maybe they'd be able to avoid the worst of the mistakes we'd made.\n",
      "\n",
      "So I gave this talk, in the course of which I told them that the best sources of seed funding were successful st...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: due to our ignorance about investing. We needed to get experience as investors. What better way, we thought, than to fund a whole bunch of startups at once? We knew undergrads got temporary jobs at...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: in. We also noticed that the startups were becoming one another's customers. We used to refer jokingly to the \"YC GDP,\" but as YC grows this becomes less and less of a joke. Now lots of startups ge...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: day in 2010, when he was visiting California for interviews, Robert Morris did something astonishing: he offered me unsolicited advice. I can only remember him doing that once before. One day at Vi...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: took a while to get back into shape, but it was at least completely engaging. [18]\n",
      "\n",
      "I spent most of the rest of 2014 painting. I'd never been able to work so uninterruptedly before, and I got to be...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: 2019. It was fortunate that I had a precisely defined goal, or it would have been hard to keep at it for so long.\n",
      "\n",
      "I wrote this new Lisp, called Bel, in itself in Arc. That may sound like a contrad...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: Italian words for abstract concepts can nearly always be predicted from their English cognates (except for occasional traps like polluzione). It's the everyday words that differ. So if you string t...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: that our experience with Y Combinator also teaches: Customs continue to constrain you long after the restrictions that caused them have disappeared. Customary VC practice had once, like the customs...\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m> Got node text: up a deeply rooted tree.\n",
      "\n",
      "[19] One way to get more precise about the concept of invented vs discovered is to talk about space aliens. Any sufficiently advanced alien civilization would certainly kn...\n",
      "\u001b[0mINFO:gpt_index.indices.common_tree.base:> Building index from nodes: 6 chunks\n",
      "> Building index from nodes: 6 chunks\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2074 request_id=33b12abc2b843111115446c8b9bdfabe response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2074 request_id=33b12abc2b843111115446c8b9bdfabe response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3920 request_id=a1a536c252b5aaeea5d8c395c4bf8434 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3920 request_id=a1a536c252b5aaeea5d8c395c4bf8434 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4251 request_id=6fb9cfb1f04bc396436f7b620510f687 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4251 request_id=6fb9cfb1f04bc396436f7b620510f687 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4541 request_id=8a1d421bbfd85cc6fd841c3cc643246b response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4541 request_id=8a1d421bbfd85cc6fd841c3cc643246b response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4729 request_id=43dad23730da4d31cf3fe45dbd83e214 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4729 request_id=43dad23730da4d31cf3fe45dbd83e214 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5131 request_id=50007fd87e5986a23ad36a4696647644 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5131 request_id=50007fd87e5986a23ad36a4696647644 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5788 request_id=9e8de06b862600dc35f7e322d43941d5 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5788 request_id=9e8de06b862600dc35f7e322d43941d5 response_code=200\n",
      "\u001b[36;1m\u001b[1;3m> Got node text: The context information provides a partial summary of the author's life, primarily focusing on their experiences with programming and co-founding startup accelerators, particularly Y Combinator. Th...\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = graph.query(\n",
    "    \"Can you give me a summary of the author's life?\", \n",
    "    query_configs=query_configs, \n",
    "    service_context=service_context_gpt4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca12b3ac-fcc2-4998-917a-16f568b59623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author has experience in programming and co-founding startup accelerators, particularly Y Combinator. They have also pursued interests in painting and writing essays, and have lived in multiple countries including England and Italy. The author reflects on their past experiences to inform their future decisions and has discussed the concept of invention versus discovery in programming languages.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4488669d-0f67-48c9-994c-bd7a42498ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">[Level 0] Current response: ANSWER: 1\n",
      "\n",
      "This summary was selected because the question asks for specific context from the author's life (what they did growing up), and the first choice mentions retrieval of specific context from documents. The second option relates to summarization and is not as relevant to the question.\n",
      "INFO:gpt_index.indices.query.tree.leaf_query:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Node [1] Summary text: Use this index for queries that require retrieval of specific context from documents.\n",
      "\u001b[36;1m\u001b[1;3m> Got node text: Growing up, the author mainly worked on writing short stories and programming. They started programming on an IBM 1401 at school, using an early version of Fortran. Later on, they got a TRS-80 micr...\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = graph.query(\n",
    "    \"What did the author do growing up?\", \n",
    "    query_configs=query_configs,\n",
    "    service_context=service_context_gpt4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1004bedc-f3c9-4643-9e65-f869a08dec32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growing up, the author mainly worked on writing short stories and programming. They started programming on an IBM 1401 at school, using an early version of Fortran, and later wrote simple games, a program to predict model rocket flights, and a word processor on a TRS-80 microcomputer. The author also had an interest in philosophy.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff95db5f-7cbe-4ed7-83ff-27e00b94e7da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">[Level 0] Current response: ANSWER: 1\n",
      "\n",
      "The first choice is more relevant to the question because it mentions retrieval of specific context from documents. The question asks about a specific event or experience (the author's time in art school), making choice 1 better suited to provide the necessary information. Choice 2, on the other hand, is related to summarization which may not give detailed information about the author's time in art school.\n",
      "INFO:gpt_index.indices.query.tree.leaf_query:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Node [1] Summary text: Use this index for queries that require retrieval of specific context from documents.\n",
      "\u001b[36;1m\u001b[1;3m> Got node text: The context information does not provide specific details about what the author did during his time in art school. However, it does mention that he took the foundation classes in fundamental subjec...\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = graph.query(\n",
    "    \"What did the author do during his time in art school?\", \n",
    "    query_configs=query_configs,\n",
    "    service_context=service_context_gpt4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75d4f291-88fe-4a51-8650-af77ed35766e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During his time in art school, the author took foundation classes in fundamental subjects like drawing, color, and design.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9bf34-d242-4fbd-b67a-1dc99b387a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
