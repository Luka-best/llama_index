{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df19606e-d67e-44d2-bed0-4b804e6fc6c3",
   "metadata": {},
   "source": [
    "# Using Token Predictor\n",
    "\n",
    "Using the token predictor, we can predict the token usage of an operation before actually performing it.\n",
    "We do this with the MockLLMPredictor class, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9eb90-335c-4214-8bb6-fd1edbe3ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My OpenAI Key\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b2364-4806-4656-81e7-3f6e4b910b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import GPTTreeIndex, SimpleDirectoryReader, LLMPredictor, GPTListIndex, GPTKeywordTableIndex\n",
    "from gpt_index.token_predictor.mock_chain_wrapper import MockLLMPredictor\n",
    "from gpt_index.prompts.base import Prompt\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f7baa-1c0a-430b-981b-83ddca9e71f2",
   "metadata": {},
   "source": [
    "### Predicting Usage of GPT Tree Index\n",
    "\n",
    "Here we predict usage of GPTTreeIndex during index construction and querying, without making any LLM calls.\n",
    "\n",
    "NOTE: Predicting query usage before tree is built is only possible with GPTTreeIndex due to the nature of tree traversal. Results will be more accurate if GPTTreeIndex is actually built beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ecdadc-1403-4bd4-a876-f80e4da911ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11056808-fd7f-4bc6-9348-0605fb4ee668",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_predictor = MockLLMPredictor(max_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d122b410-ba44-4def-93da-33a24fecaf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start token ct: 0\n",
      "> Building index from nodes: 5 chunks\n",
      "0/57\n",
      "10/57\n",
      "20/57\n",
      "30/57\n",
      "40/57\n",
      "50/57\n",
      "end token ct: 19495\n",
      "> [build_index_from_documents] Total token usage: 19495 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTTreeIndex(documents, llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d046c025-d257-4d1f-bce0-5221632cc72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start token ct: 19495\n",
      "> Starting query: What did the author do growing up?\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 1] Selected node: [1]/[1]\n",
      "end token ct: 24988\n",
      "> [query] Total token usage: 5493 tokens\n",
      "5493\n"
     ]
    }
   ],
   "source": [
    "# default query\n",
    "response = index.query(\"What did the author do growing up?\")\n",
    "print(llm_predictor.last_token_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4324d85b-ae80-48ab-baf0-7dc160dfae46",
   "metadata": {},
   "source": [
    "### Predicting Usage of GPT Keyword Table Index Query\n",
    "\n",
    "Here we build a real keyword table index over the data, but then predict query usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca76e72-5f43-47c1-a9a4-c5c5db4f0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n",
    "index = GPTKeywordTableIndex.load_from_disk('../paul_graham_essay/index_table.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f48870-65d2-4b23-b57e-79082ecb4ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start token ct: 0\n",
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "query keywords: ['the', 'at', 'y', 'after', 'time', 'combinator', 'author', 'do', 'his', 'did', 'what']\n",
      "Extracted keywords: ['combinator']\n",
      "> Querying with idx: 3483810247393006047: of 2016 we moved to England. We wanted our kids...\n",
      "> Querying with idx: 7597483754542696814: people edit code on our server through the brow...\n",
      "> Querying with idx: 7572417251450701751: invited about 20 of the 225 groups to interview...\n",
      "end token ct: 11313\n",
      "> [query] Total token usage: 11313 tokens\n",
      "11313\n"
     ]
    }
   ],
   "source": [
    "llm_predictor = MockLLMPredictor(max_tokens=256)\n",
    "response = index.query(\"What did the author do after his time at Y Combinator?\", llm_predictor=llm_predictor)\n",
    "print(llm_predictor.last_token_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee4405-05e0-46c2-87bb-64ec63a4c6c1",
   "metadata": {},
   "source": [
    "### Predicting Usage of GPT List Index Query\n",
    "\n",
    "Here we build a real list index over the data, but then predict query usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d553a8b1-7045-4756-9729-df84bd305279",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n",
    "index = GPTListIndex.load_from_disk('../paul_graham_essay/index_list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69c99c68-6a23-48ed-aa41-e7af50fef2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start token ct: 0\n",
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "end token ct: 23941\n",
      "> [query] Total token usage: 23941 tokens\n"
     ]
    }
   ],
   "source": [
    "llm_predictor = MockLLMPredictor(max_tokens=256)\n",
    "response = index.query(\"What did the author do after his time at Y Combinator?\", llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8422c5c-af68-4138-a8dd-f6e8d7208c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23941\n"
     ]
    }
   ],
   "source": [
    "print(llm_predictor.last_token_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9e039-8612-4fc9-a631-82d778afdb73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_retrieve_venv",
   "language": "python",
   "name": "gpt_retrieve_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
