{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbd03b9-99b8-40b6-b2e9-878a4419bfac",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Retrieval Networks\n",
    "\n",
    "A differentially private data exchange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff64ef-a540-496b-ad3b-7c5045f792de",
   "metadata": {},
   "source": [
    "### Steps to prove this out\n",
    "\n",
    "1. Implement DP ICL generator that generates dp-protected synthetic examples\n",
    "2. Index the dp-protected synthetic examples\n",
    "3. Build retriever over that index\n",
    "4. Expose retriever with a ContributorService\n",
    "5. Connect NetworkQueryEngine to these RetrieverContributorServices\n",
    "\n",
    "#### References\n",
    "[paper](https://openreview.net/pdf?id=oZtt0pRnOl) â—¦ [github](https://github.com/microsoft/dp-few-shot-generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c33c3-8ece-4f0d-83f3-175d8786115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a928d2c-cb4f-4d71-8c90-cdc11c5fe4d2",
   "metadata": {},
   "source": [
    "### Implementing DP ICL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee0856-33ae-4928-bfc5-41755e89eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "one_shot_template = (\n",
    "    \"{label_heading}: {example_label}\\n\"\n",
    "    \"{text_heading}: {example_text}\"\n",
    "    \"\\n\\n\"\n",
    "    \"{label_heading}: {label}\\n\"\n",
    "    \"{text_heading}:\"\n",
    ")\n",
    "\n",
    "zero_shot_template = \"{label_heading}: {label}\\n\" \"{text_heading}:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86455203-f2c7-448d-b7ba-b48376150b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_templates = [\n",
    "    ChatMessage(content=\"{instruction}\", role=MessageRole.SYSTEM),\n",
    "    ChatMessage(\n",
    "        content=one_shot_template,\n",
    "        role=MessageRole.USER,\n",
    "    ),\n",
    "]\n",
    "chat_template = ChatPromptTemplate(message_templates=message_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c43cf-02e5-4907-a77e-89502021cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = chat_template.format_messages(\n",
    "    instruction=\"Given a label of answer type, generate a question based on the given answer type accordingly.\",\n",
    "    label_heading=\"Answer Type\",\n",
    "    text_heading=\"Text\",\n",
    "    example_label=\"Number\",\n",
    "    example_text=\"How many people in the world speak French?\",\n",
    "    label=\"Number\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125333f6-61a0-4ab3-877d-7b806aca0fef",
   "metadata": {},
   "source": [
    "### Create A New LlamaDataset: `LabelledClassificationDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1a7d3-aeba-4699-af4c-000b5f442195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.base import (\n",
    "    BaseLlamaDataExample,\n",
    "    BaseLlamaDataset,\n",
    "    CreatedBy,\n",
    ")\n",
    "from llama_index.core.base.base_query_engine import BaseQueryEngine\n",
    "from llama_index.core.bridge.pydantic import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd1d7e-2743-4988-9acc-ac08df52b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelledSimpleDataExample(BaseLlamaDataExample):\n",
    "    label: str = Field(default_factory=str, description=\"Class label\")\n",
    "    text: str = Field(default_factory=str, description=\"Text body of example\")\n",
    "    is_synthetic: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Whether or not the example was synthetically generated.\",\n",
    "    )\n",
    "    text_by: Optional[CreatedBy] = Field(\n",
    "        default=None, description=\"What generated the query.\"\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def class_name(self) -> str:\n",
    "        \"\"\"Data example class name.\"\"\"\n",
    "        return \"LabelledSimpleDataExample\"\n",
    "\n",
    "\n",
    "class LabelledSimpleDataset(BaseLlamaDataset[BaseQueryEngine]):\n",
    "    _example_type = LabelledSimpleDataExample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc252e-e17b-445d-b3fb-a2893d751f59",
   "metadata": {},
   "source": [
    "### Using `OpenAI` and `ChatResponse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03108c8-f73e-44d7-866f-e06ede286580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfcdf76-1bcd-44d1-8beb-0fa0425f0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=1,\n",
    "    logprobs=True,\n",
    "    top_logprobs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54057bbe-7774-4a67-97b8-50b8a48d2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00552f-94cf-4fba-b130-3c4e8cef5d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[LogProb(token='How', logprob=-0.11545969, bytes=[72, 111, 119]),\n",
       "  LogProb(token='What', logprob=-2.372576, bytes=[87, 104, 97, 116]),\n",
       "  LogProb(token='Approx', logprob=-4.2451596, bytes=[65, 112, 112, 114, 111, 120]),\n",
       "  LogProb(token='Can', logprob=-8.133478, bytes=[67, 97, 110]),\n",
       "  LogProb(token='Est', logprob=-8.5663395, bytes=[69, 115, 116]),\n",
       "  LogProb(token='\"', logprob=-8.78406, bytes=[34]),\n",
       "  LogProb(token='According', logprob=-9.151157, bytes=[65, 99, 99, 111, 114, 100, 105, 110, 103]),\n",
       "  LogProb(token='In', logprob=-9.599209, bytes=[73, 110]),\n",
       "  LogProb(token=' How', logprob=-9.692478, bytes=[32, 72, 111, 119]),\n",
       "  LogProb(token='Do', logprob=-9.830822, bytes=[68, 111]),\n",
       "  LogProb(token='\"How', logprob=-9.891071, bytes=[34, 72, 111, 119]),\n",
       "  LogProb(token='R', logprob=-9.958864, bytes=[82]),\n",
       "  LogProb(token='About', logprob=-10.373207, bytes=[65, 98, 111, 117, 116]),\n",
       "  LogProb(token='To', logprob=-10.376238, bytes=[84, 111]),\n",
       "  LogProb(token='Question', logprob=-10.397839, bytes=[81, 117, 101, 115, 116, 105, 111, 110]),\n",
       "  LogProb(token='Based', logprob=-10.482714, bytes=[66, 97, 115, 101, 100]),\n",
       "  LogProb(token='I', logprob=-10.565065, bytes=[73]),\n",
       "  LogProb(token='The', logprob=-10.771877, bytes=[84, 104, 101]),\n",
       "  LogProb(token='A', logprob=-10.863096, bytes=[65]),\n",
       "  LogProb(token='**', logprob=-10.940279, bytes=[42, 42])]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08bfb9-506d-4584-b860-b3bfc4f8ad47",
   "metadata": {},
   "source": [
    "### Implement DP ICL Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e334f6c-0be8-4d43-ab75-145561815fb6",
   "metadata": {},
   "source": [
    "Algorithm Pseudocode\n",
    "\n",
    "```python\n",
    "t_max = 1\n",
    "sigma = 0.5\n",
    "num_splits = 5\n",
    "num_samples_per_split = 1\n",
    "examples: List[LabelledClassificationDataExample] = ...\n",
    "dataset =  LabelledClassificationDataset(examples=examples)\n",
    "\n",
    "# privacy params\n",
    "delta = 1 / len(dataset.examples)\n",
    "\n",
    "synthetic_example = \"\"\n",
    "for _ in range(t_max):\n",
    "    \n",
    "    # reduced token universe\n",
    "    token_universe_messages = get_messages_for_reduced_token_universe(synthetic_example)\n",
    "    response = llm.chat(messages)\n",
    "    token_universe_probas = {el.token: np.exp(el.logprob) for el in response.logprobs}\n",
    "\n",
    "    # split the private dataset\n",
    "    disjoint_splits = split_dataset(num_splits, num_samples_per_split)\n",
    "\n",
    "    # generate next token probability distributions per split\n",
    "    splits = []\n",
    "    for split in disjoint_splits:\n",
    "\n",
    "        split_probs = {token: 0 for token in token_universe_probas.keys()}\n",
    "        messages = get_messages_for_synthetic_generation(split, synthetic_example)\n",
    "        response = llm.chat(messages)\n",
    "\n",
    "        # updating and (rescaling) split probs\n",
    "        for el in response.logprobs:\n",
    "            if el.token in split_probs:\n",
    "                split_probs[el.token] = np.exp(el.logprob)\n",
    "        split_probs = normalize(split_probs)  # to make into a valid prob distribution\n",
    "\n",
    "        splits.append(split_probs)\n",
    "    \n",
    "    # noisy aggrergation\n",
    "    sigma_calib = math.sqrt(2) / num_splits * sigma\n",
    "    noise = generate_noise(sigma=sigma, size=len(token_universer_probas))\n",
    "    agg_probs = merge_probas(splits) + noise\n",
    "\n",
    "    # next token\n",
    "    synthetic_example += mode_of_distribution(agg_probs) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce2337-6a50-4d48-8301-97946e99ee73",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f80f2f-3223-4b0a-8a22-3e05113253af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_to_sigma(eps: float, delta: float, mode: str = \"gaussian\") -> float:\n",
    "    \"\"\"Return the scale parameter with a given epsilon value.\n",
    "\n",
    "    Source: https://programming-dp.com/ch6.html#the-gaussian-mechanism\n",
    "    \"\"\"\n",
    "    sensitivity_upper_bound = np.sqrt(2)\n",
    "    return (sensitivity_upper_bound * np.sqrt(np.log(1.25 / delta))) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da23ee-150c-4353-8811-655c1c560b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.589574318378231"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = eps_to_sigma(eps=1.0, delta=1 / 30000)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79fc75-f68c-4896-ae36-ecb94721d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages_for_reduced_token_universe(\n",
    "    synthetic_example: str,\n",
    "    instruction: str,\n",
    "    label_heading: str,\n",
    "    text_heading: str,\n",
    "    label: str,\n",
    ") -> List[ChatMessage]:\n",
    "    \"\"\"Get chat messages with instructions to produce the reduced next token universe.\"\"\"\n",
    "\n",
    "    message_templates = [\n",
    "        ChatMessage(content=\"{instruction}\", role=MessageRole.SYSTEM),\n",
    "        ChatMessage(\n",
    "            content=zero_shot_template,\n",
    "            role=MessageRole.USER,\n",
    "        ),\n",
    "    ]\n",
    "    chat_template = ChatPromptTemplate(message_templates=message_templates)\n",
    "    return chat_template.format_messages(\n",
    "        instruction=instruction,\n",
    "        label_heading=label_heading,\n",
    "        text_heading=text_heading,\n",
    "        label=label,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf98d1c-ae7c-4854-829b-d7c73617a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(\n",
    "    dataset: LabelledClassificationDataset, num_splits: int, num_samples_per_split: int\n",
    ") -> List[LabelledClassificationDataset]:\n",
    "    \"\"\"Splits a dataset into a set of disjoint datasets with equal number of examples.\"\"\"\n",
    "\n",
    "    indexes = list(range(len(dataset.examples)))\n",
    "    random.shuffle(indexes)\n",
    "    partitions = [indexes[i::num_splits] for i in range(num_splits)]\n",
    "    splits = []\n",
    "    for p in partitions:\n",
    "        sample = random.sample(p, num_samples_per_split)\n",
    "        examples = [dataset.examples[ix] for ix in sample]\n",
    "        splits.append(LabelledClassificationDataset(examples=examples))\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f7c10-9b05-4f0e-b22f-ebeabfa8a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages_for_synthetic_generation(\n",
    "    split: LabelledClassificationDataset,\n",
    "    synthetic_example: str,\n",
    "    instruction: str,\n",
    "    label_heading: str,\n",
    "    text_heading: str,\n",
    "    label: str,\n",
    ") -> List[ChatMessage]:\n",
    "    \"\"\"Get chat messages to produce the next token probabilities for a given split.\"\"\"\n",
    "\n",
    "    message_templates = [\n",
    "        ChatMessage(content=\"{instruction}\", role=MessageRole.SYSTEM),\n",
    "        ChatMessage(\n",
    "            content=one_shot_template,\n",
    "            role=MessageRole.USER,\n",
    "        ),\n",
    "    ]\n",
    "    chat_template = ChatPromptTemplate(message_templates=message_templates)\n",
    "    return chat_template.format_messages(\n",
    "        instruction=instruction,\n",
    "        label_heading=label_heading,\n",
    "        text_heading=text_heading,\n",
    "        example_label=split.examples[0].label,\n",
    "        example_text=split.examples[0].text,\n",
    "        label=label,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a72ca-083a-4e8d-acf6-a77d5f5c1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(split_probs: Dict[str, float]) -> Dict[str, float]:\n",
    "    \"\"\"Normalize a probability distribution over tokens to become a valid probability distribution.\"\"\"\n",
    "    scale = sum(proba for proba in split_probs.values())\n",
    "    return {token: proba / scale for token, proba in split_probs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e0f14-4090-49ad-a220-7e48e9231a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(sigma: float, size: int) -> float:\n",
    "    \"\"\"Generates noise that satisfies eps-delta differential privacy.\"\"\"\n",
    "    noise_rng: np.random.RandomState\n",
    "    return noise_rng.normal(0, sigma, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b27de9-ef31-49dd-afcc-14386722f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_probas(list_of_probas: List[Dict[str, float]]) -> Dict[str, float]:\n",
    "    \"\"\"Merges a set of probabillity distributions over a common token universe.\"\"\"\n",
    "    scale = len(list_of_probas)\n",
    "    tokens = list_of_probas[0].keys()\n",
    "    merged_distribution = {}\n",
    "    for token in tokens:\n",
    "        merged_distribution[token] = sum(pr[token] / scale for pr in list_of_probas)\n",
    "    return merged_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9cd0b-b29b-492e-8aa9-b80615368b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_of_distribution(proba: Dict[str, float]) -> str:\n",
    "    \"\"\"Returns the mode of a given probability distribution.\"\"\"\n",
    "    return max(proba, key=proba.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0006c8-e2cb-4985-99ef-f3fe28079b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff-private-examples-gen-pack",
   "language": "python",
   "name": "diff-private-examples-gen-pack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
