{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Hierarchy Node Parser\n",
    "\n",
    "The `CodeHierarchyNodeParser` is useful to split long code files into more reasonable chunks. What this will do is create a \"Hierarchy\" of sorts, where sections of the code are made more reasonable by replacing the scope body with short comments telling the LLM to search for a referenced node if it wants to read that context body. This is called skeletonization, and is toggled by setting `skeleton` to `True` which it is by default. Nodes in this hierarchy will be split based on scope, like function, class, or method scope, and will have links to their children and parents so the LLM can traverse the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Import\n",
    "\n",
    "First be sure to install the necessary [tree-sitter](https://tree-sitter.github.io/tree-sitter/) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tree-sitter in /home/ryanpeach/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages (0.20.4)\n",
      "Requirement already satisfied: tree-sitter-languages in /home/ryanpeach/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages (1.10.2)\n",
      "Requirement already satisfied: python-dotenv in /home/ryanpeach/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tree-sitter tree-sitter-languages python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.text_splitter import CodeSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.packs.code_hierarchy import CodeHierarchyNodeParser\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def print_python(python_text):\n",
    "    \"\"\"This function prints python text in ipynb nicely formatted.\"\"\"\n",
    "    display(Markdown(\"```python\\n\" + python_text + \"```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a directory you want to scan, and glob for all the code files you want to import.\n",
    "\n",
    "In this case I'm going to glob all \"*.py\" files in the `llama_index/node_parser` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[Path(\"../llama_index/packs/code_hierarchy/code_hierarchy.py\")],\n",
    "    file_metadata=lambda x: {\"filepath\": x},\n",
    ")\n",
    "nodes = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be the code hierarchy node parser itself. Lets have it parse itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 33427\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from collections import defaultdict\n",
       "from enum import Enum\n",
       "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
       "\n",
       "from llama_index.core.extractors.metadata_extractors import BaseExtractor\n",
       "from llama_index.core.node_parser.interface import NodeParser\n",
       "\n",
       "try:\n",
       "    from pydantic.v1 import BaseModel, Field\n",
       "except ImportError:\n",
       "    from pydantic import BaseModel, Field\n",
       "\n",
       "from tree_sitter import Node\n",
       "\n",
       "from llama_index.core.callbacks.base import CallbackManager\n",
       "from llama_index.core.schema import BaseNode, NodeRelationship, TextNode\n",
       "from llama_index.core.text_splitter import CodeSplitter\n",
       "from llama_index.core.utils import get_tqdm_iterable\n",
       "\n",
       "\n",
       "class _SignatureCaptureType(BaseModel):\n",
       "    \"\"\"\n",
       "    Unfortunately some languages need special options for how to make a signature.\n",
       "\n",
       "    For example, html element signatures should include their closing >, there is no\n",
       "    easy way to include this using an always-exclusive system.\n",
       "\n",
       "    However, using an always-inclusive system, python decorators don't work,\n",
       "    as there isn't an easy to define terminator for decorators that is inclusive\n",
       "    to their signature.\n",
       "    \"\"\"\n",
       "\n",
       "    type: str = Field(description=\"The type string to match on.\")\n",
       "    inclusive: bool = Field(\n",
       "        description=(\n",
       "            \"Whether to include the text of the node matched by this type or not.\"\n",
       "        ),\n",
       "    )\n",
       "\n",
       "\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "    \"\"\"\n",
       "    Options for capturing the signature of a node.\n",
       "    \"\"\"\n",
       "\n",
       "    start_signature_types: Optional[List[_SignatureCa\n",
       "\n",
       "# ...```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Length of text: {len(nodes[0].text)}\")\n",
    "print_python(nodes[0].text[:1500] + \"\\n\\n# ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is way too long to fit into the context of our LLM. So what are we to do? Well we will split it. We are going to use the `CodeHierarchyNodeParser` to split the nodes into more reasonable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes after splitting: 90\n"
     ]
    }
   ],
   "source": [
    "split_nodes = CodeHierarchyNodeParser(\n",
    "    language=\"python\",\n",
    "    # You can further parameterize the CodeSplitter to split the code\n",
    "    # into \"chunks\" that match your context window size using\n",
    "    # chunck_lines and max_chars parameters, here we just use the defaults\n",
    "    code_splitter=CodeSplitter(language=\"python\", max_chars=1000, chunk_lines=10),\n",
    ").get_nodes_from_documents(nodes)\n",
    "print(\"Number of nodes after splitting:\", len(split_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So that split up our data from 1 node into quite a few nodes! Whats the max length of any of these nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest text in nodes: 1152\n"
     ]
    }
   ],
   "source": [
    "print(f\"Longest text in nodes: {max(len(n.text) for n in split_nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much shorter than before! Let's look at a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from collections import defaultdict\n",
       "from enum import Enum\n",
       "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
       "\n",
       "from llama_index.core.extractors.metadata_extractors import BaseExtractor\n",
       "from llama_index.core.node_parser.interface import NodeParser\n",
       "\n",
       "try:\n",
       "    from pydantic.v1 import BaseModel, Field\n",
       "except ImportError:\n",
       "    from pydantic import BaseModel, Field\n",
       "\n",
       "from tree_sitter import Node\n",
       "\n",
       "from llama_index.core.callbacks.base import CallbackManager\n",
       "from llama_index.core.schema import BaseNode, NodeRelationship, TextNode\n",
       "from llama_index.core.text_splitter import CodeSplitter\n",
       "from llama_index.core.utils import get_tqdm_iterable\n",
       "\n",
       "\n",
       "class _SignatureCaptureType(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 76793a53-5410-4589-901f-df08e44de8c5\n",
       "\n",
       "\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 50d35182-2106-4328-ae9e-c076b096b5c9\n",
       "# Code replaced for brevity. See node_id 7d5b7f71-a7fd-4383-9617-e17a62d9b82a```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(split_nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without even needing a long printout we can see everything this module imported in the first document (which is at the module level) and some classes it defines.\n",
    "\n",
    "We also see that it has put comments in place of code that was removed to make the text size more reasonable.\n",
    "These can appear at the beginning or end of a chunk, or at a new scope level, like a class or function declaration.\n",
    "\n",
    "`# Code replaced for brevity. See node_id {node_id}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Hierarchy\n",
    "\n",
    "These scopes can be listed by the `CodeHierarchyNodeParser`, giving a \"repo map\" of sorts.\n",
    "The namesake of this node parser, it creates a tree of scope names to use to search the code.\n",
    "Put this in your context to give the LLM a default search hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruct an LLM using the KeywordQueryEngine (shown later) as a tool to:\n",
    "\n",
    "```\n",
    "\"Search the tool by any element in this list, or any uuid found in the resulting code, to get more information about that element.\"\n",
    "```\n",
    "\n",
    "Then append this to your context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(defaultdict(<class 'dict'>, {'..': defaultdict(<class 'dict'>, {'llama_index': defaultdict(<class 'dict'>, {'packs': defaultdict(<class 'dict'>, {'code_hierarchy': defaultdict(<class 'dict'>, {'code_hierarchy': defaultdict(<class 'dict'>, {'_SignatureCaptureType': defaultdict(<class 'dict'>, {}), '_SignatureCaptureOptions': defaultdict(<class 'dict'>, {}), '_ScopeMethod': defaultdict(<class 'dict'>, {}), '_CommentOptions': defaultdict(<class 'dict'>, {}), '_ScopeItem': defaultdict(<class 'dict'>, {}), '_ChunkNodeOutput': defaultdict(<class 'dict'>, {}), 'CodeHierarchyNodeParser': defaultdict(<class 'dict'>, {'class_name': defaultdict(<class 'dict'>, {}), '__init__': defaultdict(<class 'dict'>, {}), '_get_node_name': defaultdict(<class 'dict'>, {'recur': defaultdict(<class 'dict'>, {})}), '_get_node_signature': defaultdict(<class 'dict'>, {'find_start': defaultdict(<class 'dict'>, {}), 'find_end': defaultdict(<class 'dict'>, {})}), '_chunk_node': defaultdict(<class 'dict'>, {}), 'get_code_hierarchy_from_nodes': defaultdict(<class 'dict'>, {'get_subdict': defaultdict(<class 'dict'>, {}), 'recur_inclusive_scope': defaultdict(<class 'dict'>, {}), 'dict_to_markdown': defaultdict(<class 'dict'>, {})}), '_parse_nodes': defaultdict(<class 'dict'>, {}), '_get_indentation': defaultdict(<class 'dict'>, {}), '_get_comment_text': defaultdict(<class 'dict'>, {}), '_create_comment_line': defaultdict(<class 'dict'>, {}), '_get_replacement_text': defaultdict(<class 'dict'>, {}), '_skeletonize': defaultdict(<class 'dict'>, {}), '_skeletonize_list': defaultdict(<class 'dict'>, {'recur': defaultdict(<class 'dict'>, {})})})})})})})})}), '- ..\\n  - llama_index\\n    - packs\\n      - code_hierarchy\\n        - code_hierarchy\\n          - _SignatureCaptureType\\n          - _SignatureCaptureOptions\\n          - _ScopeMethod\\n          - _CommentOptions\\n          - _ScopeItem\\n          - _ChunkNodeOutput\\n          - CodeHierarchyNodeParser\\n            - class_name\\n            - __init__\\n            - _get_node_name\\n              - recur\\n            - _get_node_signature\\n              - find_start\\n              - find_end\\n            - _chunk_node\\n            - get_code_hierarchy_from_nodes\\n              - get_subdict\\n              - recur_inclusive_scope\\n              - dict_to_markdown\\n            - _parse_nodes\\n            - _get_indentation\\n            - _get_comment_text\\n            - _create_comment_line\\n            - _get_replacement_text\\n            - _skeletonize\\n            - _skeletonize_list\\n              - recur\\n')\n"
     ]
    }
   ],
   "source": [
    "print(CodeHierarchyNodeParser.get_code_hierarchy_from_nodes(split_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration by the Programmer\n",
    "\n",
    "So that we understand what is going on under the hood, what if we go to that node_id we found above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to print the node with UUID: 5b9b71eb-ec2f-4b39-a4e1-cecda086896e\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class CodeHierarchyNodeParser(NodeParser):\n",
       "# Code replaced for brevity. See node_id 37cab2a7-40d6-47d2-8043-a0a9a9afb697```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_nodes_by_id = {n.node_id: n for n in split_nodes}\n",
    "uuid_from_text = split_nodes[9].text.splitlines()[-1].split(\" \")[-1]\n",
    "print(\"Going to print the node with UUID:\", uuid_from_text)\n",
    "print_python(split_nodes_by_id[uuid_from_text].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the next split in the file. It is prepended with the node before it and appended with the node after it as a comment.\n",
    "\n",
    "We can also see the relationships on this node programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5b9b71eb-ec2f-4b39-a4e1-cecda086896e', node_type=<ObjectType.TEXT: '1'>, metadata={'language': 'python', 'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}], 'start_byte': 6293, 'end_byte': 33426, 'filepath': '../llama_index/packs/code_hierarchy/code_hierarchy.py'}, hash='0e5e84e11e403b09f986b97b1ffcd9ba7994c8bd3bf880037a0e48d513fabaf7'),\n",
       " <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='37cab2a7-40d6-47d2-8043-a0a9a9afb697', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='33db2c2bc76ae543d50770ecfbbe61e7f5de9a64ab84e72788e4a4e711f1cfe5'),\n",
       " <NodeRelationship.CHILD: '5'>: [RelatedNodeInfo(node_id='1f23f5f9-26c6-487f-ac48-5c0914ec243c', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': 'class_name', 'type': 'function_definition', 'signature': 'def class_name(cls) -> str:'}], 'start_byte': 6495, 'end_byte': 6597}, hash='0a1d51ecc201c8edf92d96677720b4468e3190917fb5b76c2dd2db424fa74fa6'),\n",
       "  RelatedNodeInfo(node_id='1daaa620-fa4c-4464-81b0-98e3f753c98e', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '__init__', 'type': 'function_definition', 'signature': 'def __init__(\\n        self,\\n        language: str,\\n        skeleton: bool = True,\\n        signature_identifiers: Optional[Dict[str, _SignatureCaptureOptions]] = None,\\n        code_splitter: Optional[CodeSplitter] = None,\\n        callback_manager: Optional[CallbackManager] = None,\\n        metadata_extractor: Optional[BaseExtractor] = None,\\n        chunk_min_characters: int = 80,\\n    ):'}], 'start_byte': 7896, 'end_byte': 9044}, hash='1b7f7dee190936d4c5cc35ec77d8010f37cd3ca401820536f988d8d7ee6532cf'),\n",
       "  RelatedNodeInfo(node_id='e5b7bc2d-3150-4342-9d2d-82a724580059', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_get_node_name', 'type': 'function_definition', 'signature': 'def _get_node_name(self, node: Node) -> str:'}], 'start_byte': 9046, 'end_byte': 9593}, hash='9eb4165c55e7b012711f21581f18bbba834506f26b0d5c0ac3f59089b857a7a4'),\n",
       "  RelatedNodeInfo(node_id='3bf28b8f-d77a-4b28-950d-c5bab36d91c0', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_get_node_signature', 'type': 'function_definition', 'signature': 'def _get_node_signature(self, text: str, node: Node) -> str:'}], 'start_byte': 9595, 'end_byte': 11187}, hash='3101ac703ad37065437112b7296c13bf06641358c284593ab63a24cd9335d8eb'),\n",
       "  RelatedNodeInfo(node_id='d72d6f8f-83e7-422d-b47c-b3323a5096bf', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_chunk_node', 'type': 'function_definition', 'signature': 'def _chunk_node(\\n        self,\\n        parent: Node,\\n        text: str,\\n        _context_list: Optional[List[_ScopeItem]] = None,\\n        _root: bool = True,\\n    ) -> _ChunkNodeOutput:'}], 'start_byte': 11189, 'end_byte': 17268}, hash='8e6a38dcda30508ba0eece50be23a1e9b475111411effd87161fd564e6256e8c'),\n",
       "  RelatedNodeInfo(node_id='7c08232d-4e63-4cea-af1d-19eba67ada5d', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': 'get_code_hierarchy_from_nodes', 'type': 'function_definition', 'signature': 'def get_code_hierarchy_from_nodes(\\n        nodes: Sequence[BaseNode],\\n        max_depth: int = -1,\\n    ) -> Tuple[Dict[str, Any], str]:'}], 'start_byte': 17288, 'end_byte': 19551}, hash='6f3b8b552708b766e125747382ad31f75c9adbc70be10f1ecb3a4928e4e63d31'),\n",
       "  RelatedNodeInfo(node_id='172e752e-00d4-415b-8f27-ffd0ff422707', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_parse_nodes', 'type': 'function_definition', 'signature': 'def _parse_nodes(\\n        self,\\n        nodes: Sequence[BaseNode],\\n        show_progress: bool = False,\\n        **kwargs: Any,\\n    ) -> List[BaseNode]:'}], 'start_byte': 19553, 'end_byte': 25524}, hash='fe32bd7a46f7ae24f0ee2822085fdfd428c4e1b87dc22604e5229f3c0ee591b2'),\n",
       "  RelatedNodeInfo(node_id='074e7fa0-db7d-441e-835f-00520f70c979', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_get_indentation', 'type': 'function_definition', 'signature': 'def _get_indentation(text: str) -> Tuple[str, int, int]:'}], 'start_byte': 25544, 'end_byte': 27869}, hash='a09be1f84e3c559b66c23aaf9fe19ba88ad45b56cebd0035f6acb93591003282'),\n",
       "  RelatedNodeInfo(node_id='15f25619-a6ac-498d-ab36-5e5d5a47ef5b', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_get_comment_text', 'type': 'function_definition', 'signature': 'def _get_comment_text(node: TextNode) -> str:'}], 'start_byte': 27889, 'end_byte': 28087}, hash='cca2e443418b8b1bb296f153ee64ffddfc02b98a4587212b6d630eedd8e2e245'),\n",
       "  RelatedNodeInfo(node_id='55567422-0cf1-4a68-91dd-5e7b0d390e13', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_create_comment_line', 'type': 'function_definition', 'signature': 'def _create_comment_line(cls, node: TextNode, indention_lvl: int = -1) -> str:'}], 'start_byte': 28106, 'end_byte': 29227}, hash='666fffc609791f229ebd4f5181809719bd27e26ad94c2d2d814c62a5921ca742'),\n",
       "  RelatedNodeInfo(node_id='bc39d10f-b227-465d-8653-9b6d5f4d9ca9', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_get_replacement_text', 'type': 'function_definition', 'signature': 'def _get_replacement_text(cls, child_node: TextNode) -> str:'}], 'start_byte': 29246, 'end_byte': 32069}, hash='95ef8acbdc76ca6519710d02d6cf1aa8e14a72206ca3d25ff29ef13ffaadb4b1'),\n",
       "  RelatedNodeInfo(node_id='dce2acf0-bca5-4a41-8448-0bb7c43bdbfb', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_skeletonize', 'type': 'function_definition', 'signature': 'def _skeletonize(cls, parent_node: TextNode, child_node: TextNode) -> None:'}], 'start_byte': 32088, 'end_byte': 32754}, hash='8619c7bf2f376f685da5dc0800940d4194c806c54846f1fd3823709079277222'),\n",
       "  RelatedNodeInfo(node_id='975455dd-76ae-477b-b02b-16f5e096be15', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_skeletonize_list', 'type': 'function_definition', 'signature': \"def _skeletonize_list(cls, nodes: List[TextNode]) -> None:\\n        # Create a convenient map for mapping node id's to nodes\"}], 'start_byte': 32773, 'end_byte': 33426}, hash='fe87b8733ff691c1cae32ab312ed84d9786c5fddd416c4f50947eb567576ca56')],\n",
       " <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='0a877c65-33a0-4b90-a1ca-89e87e0ffd4e', node_type=<ObjectType.TEXT: '1'>, metadata={'language': 'python', 'inclusive_scopes': [], 'start_byte': 0, 'end_byte': 33427, 'filepath': '../llama_index/packs/code_hierarchy/code_hierarchy.py'}, hash='0aaf2ec693e6b4e9167747f4428f54eae174cdb0e93c9bb0791c6a14e8c1f646')}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_nodes_by_id[uuid_from_text].relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NEXT` `PREV` relationships come from the `CodeSplitter` which is a component of the `CodeHierarchyNodeParser`. It is responsible for cutting up the nodes into chunks that are a certain character length. For more information about the `CodeSplitter` read this:\n",
    "\n",
    "[Code Splitter](https://docs.llamaindex.ai/en/latest/api/llama_index.node_parser.CodeSplitter.html)\n",
    "\n",
    "The `PARENT` and `CHILD` relationships come from the `CodeHierarchyNodeParser` which is responsible for creating the hierarchy of nodes. Things like classes, functions, and methods are nodes in this hierarchy.\n",
    "\n",
    "The `SOURCE` is the original file that this node came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "# Code replaced for brevity. See node_id 5b9b71eb-ec2f-4b39-a4e1-cecda086896e\n",
       "\"\"\"Split code using a AST parser.\n",
       "\n",
       "    Add metadata about the scope of the code block and relationships between\n",
       "    code blocks.\n",
       "    \"\"\"\n",
       "\n",
       "    @classmethod\n",
       "    def class_name(cls) -> str:\n",
       "        # Code replaced for brevity. See node_id 1f23f5f9-26c6-487f-ac48-5c0914ec243c\n",
       "\n",
       "    language: str = Field(\n",
       "        description=\"The programming language of the code being split.\"\n",
       "    )\n",
       "    signature_identifiers: Dict[str, _SignatureCaptureOptions] = Field(\n",
       "        description=(\n",
       "            \"A dictionary mapping the type of a split mapped to the first and last type\"\n",
       "            \" of itschildren which identify its signature.\"\n",
       "        )\n",
       "    )\n",
       "    min_characters: int = Field(\n",
       "        default=80,\n",
       "        description=(\n",
       "            \"Minimum number of characters per chunk.Defaults to 80 because that's about\"\n",
       "            \" how long a replacement comment is in skeleton mode.\"\n",
       "        ),\n",
       "    )\n",
       "# Code replaced for brevity. See node_id af1e10f7-f928-4160-9068-990f54981e43```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.schema import NodeRelationship\n",
    "\n",
    "node_id = uuid_from_text\n",
    "if NodeRelationship.NEXT not in split_nodes_by_id[node_id].relationships:\n",
    "    print(\"No next node found!\")\n",
    "else:\n",
    "    next_node_relationship_info = split_nodes_by_id[node_id].relationships[\n",
    "        NodeRelationship.NEXT\n",
    "    ]\n",
    "    next_node = split_nodes_by_id[next_node_relationship_info.node_id]\n",
    "    print_python(next_node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Table and Usage by the LLM\n",
    "\n",
    "Lets explore the use of this node parser in an index. We will be able to use any index which allows search by keyword, which should enable us to search for any node by it's uuid, or by any scope name.\n",
    "\n",
    "We have created a `CodeHierarchyKeywordQueryEngine` which will allow us to search for nodes by their uuid, or by their scope name. It's `.query` method can be used as a simple search tool for any LLM. Given the repo map we created earlier, or the text of a split file, the LLM should be able to figure out what to search for very naturally.\n",
    "\n",
    "Lets create the KeywordQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.packs.code_hierarchy import CodeHierarchyKeywordQueryEngine\n",
    "\n",
    "idx = CodeHierarchyKeywordQueryEngine(\n",
    "    nodes=split_nodes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the same code as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from collections import defaultdict\n",
       "from enum import Enum\n",
       "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
       "\n",
       "from llama_index.core.extractors.metadata_extractors import BaseExtractor\n",
       "from llama_index.core.node_parser.interface import NodeParser\n",
       "\n",
       "try:\n",
       "    from pydantic.v1 import BaseModel, Field\n",
       "except ImportError:\n",
       "    from pydantic import BaseModel, Field\n",
       "\n",
       "from tree_sitter import Node\n",
       "\n",
       "from llama_index.core.callbacks.base import CallbackManager\n",
       "from llama_index.core.schema import BaseNode, NodeRelationship, TextNode\n",
       "from llama_index.core.text_splitter import CodeSplitter\n",
       "from llama_index.core.utils import get_tqdm_iterable\n",
       "\n",
       "\n",
       "class _SignatureCaptureType(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 76793a53-5410-4589-901f-df08e44de8c5\n",
       "\n",
       "\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 50d35182-2106-4328-ae9e-c076b096b5c9\n",
       "# Code replaced for brevity. See node_id 7d5b7f71-a7fd-4383-9617-e17a62d9b82a```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(idx.query(split_nodes[0].node_id).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we can also search for any node by it's common sense name.\n",
    "\n",
    "For example, the class `_SignatureCaptureOptions` is a node in the hierarchy. We can search for it by name.\n",
    "\n",
    "The reason we aren't getting more detail is because our min_characters is too low, try to increase it for more detail for any individual query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "# Code replaced for brevity. See node_id 5f8ce1f3-7886-4081-a961-ce7ea8e25cbf```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(idx.query(\"_SignatureCaptureOptions\").response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by module name, in case the LLM sees something in an import statement and wants to know more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from collections import defaultdict\n",
       "from enum import Enum\n",
       "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
       "\n",
       "from llama_index.core.extractors.metadata_extractors import BaseExtractor\n",
       "from llama_index.core.node_parser.interface import NodeParser\n",
       "\n",
       "try:\n",
       "    from pydantic.v1 import BaseModel, Field\n",
       "except ImportError:\n",
       "    from pydantic import BaseModel, Field\n",
       "\n",
       "from tree_sitter import Node\n",
       "\n",
       "from llama_index.core.callbacks.base import CallbackManager\n",
       "from llama_index.core.schema import BaseNode, NodeRelationship, TextNode\n",
       "from llama_index.core.text_splitter import CodeSplitter\n",
       "from llama_index.core.utils import get_tqdm_iterable\n",
       "\n",
       "\n",
       "class _SignatureCaptureType(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 76793a53-5410-4589-901f-df08e44de8c5\n",
       "\n",
       "\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 50d35182-2106-4328-ae9e-c076b096b5c9\n",
       "# Code replaced for brevity. See node_id 7d5b7f71-a7fd-4383-9617-e17a62d9b82a```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(idx.query(\"code_hierarchy\").response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As a Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the langchain tool, just use `as_langchain_tool` on the `CodeHierarchyKeywordQueryEngine` and it will be ready to use in the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from collections import defaultdict\n",
       "from enum import Enum\n",
       "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
       "\n",
       "from llama_index.core.extractors.metadata_extractors import BaseExtractor\n",
       "from llama_index.core.node_parser.interface import NodeParser\n",
       "\n",
       "try:\n",
       "    from pydantic.v1 import BaseModel, Field\n",
       "except ImportError:\n",
       "    from pydantic import BaseModel, Field\n",
       "\n",
       "from tree_sitter import Node\n",
       "\n",
       "from llama_index.core.callbacks.base import CallbackManager\n",
       "from llama_index.core.schema import BaseNode, NodeRelationship, TextNode\n",
       "from llama_index.core.text_splitter import CodeSplitter\n",
       "from llama_index.core.utils import get_tqdm_iterable\n",
       "\n",
       "\n",
       "class _SignatureCaptureType(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 76793a53-5410-4589-901f-df08e44de8c5\n",
       "\n",
       "\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 50d35182-2106-4328-ae9e-c076b096b5c9\n",
       "# Code replaced for brevity. See node_id 7d5b7f71-a7fd-4383-9617-e17a62d9b82a```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(idx.as_langchain_tool().run(\"code_hierarchy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description for your LLM to read to learn the tool is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Code Search\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Description: \n",
       "        Search the tool by any element in this list\n",
       "        to get more information about that element.\n",
       "        If you see \"Code replaced for brevity\" then a uuid, you may also search the tool for that uuid to see the full code.\n",
       "        The list is:\n",
       "\n",
       "        - ..\n",
       "  - llama_index\n",
       "    - packs\n",
       "      - code_hierarchy\n",
       "        - code_hierarchy\n",
       "          - _SignatureCaptureType\n",
       "          - _SignatureCaptureOptions\n",
       "          - _ScopeMethod\n",
       "          - _CommentOptions\n",
       "          - _ScopeItem\n",
       "          - _ChunkNodeOutput\n",
       "          - CodeHierarchyNodeParser\n",
       "            - class_name\n",
       "            - __init__\n",
       "            - _get_node_name\n",
       "              - recur\n",
       "            - _get_node_signature\n",
       "              - find_start\n",
       "              - find_end\n",
       "            - _chunk_node\n",
       "            - get_code_hierarchy_from_nodes\n",
       "              - get_subdict\n",
       "              - recur_inclusive_scope\n",
       "              - dict_to_markdown\n",
       "            - _parse_nodes\n",
       "            - _get_indentation\n",
       "            - _get_comment_text\n",
       "            - _create_comment_line\n",
       "            - _get_replacement_text\n",
       "            - _skeletonize\n",
       "            - _skeletonize_list\n",
       "              - recur\n",
       "\n",
       "\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Name: \" + idx.as_langchain_tool().name)\n",
    "display(Markdown(\"Description: \" + idx.as_langchain_tool().description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets finally actually make an agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Answer the following questions as best you can. You have access to the following tools:\n",
       "\n",
       "Code Search: \n",
       "        Search the tool by any element in this list\n",
       "        to get more information about that element.\n",
       "        If you see \"Code replaced for brevity\" then a uuid, you may also search the tool for that uuid to see the full code.\n",
       "        The list is:\n",
       "\n",
       "        - ..\n",
       "  - llama_index\n",
       "    - packs\n",
       "      - code_hierarchy\n",
       "        - code_hierarchy\n",
       "          - _SignatureCaptureType\n",
       "          - _SignatureCaptureOptions\n",
       "          - CodeHierarchyNodeParser\n",
       "            - __init__\n",
       "            - _get_node_name\n",
       "            - _get_node_signature\n",
       "              - find_start\n",
       "              - find_end\n",
       "            - _chunk_node\n",
       "            - get_code_hierarchy_from_nodes\n",
       "              - recur_inclusive_scope\n",
       "              - dict_to_markdown\n",
       "            - _parse_nodes\n",
       "            - _get_indentation\n",
       "            - _create_comment_line\n",
       "            - _get_replacement_text\n",
       "            - _skeletonize\n",
       "            - _skeletonize_list\n",
       "\n",
       "\n",
       "        \n",
       "\n",
       "Use the following format:\n",
       "\n",
       "Question: the input question you must answer\n",
       "Thought: you should always think about what to do\n",
       "Action: the action to take, should be one of [Code Search]\n",
       "Action Input: the input to the action\n",
       "Observation: the result of the action\n",
       "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
       "Thought: I now know the final answer\n",
       "Final Answer: the final answer to the original input question\n",
       "\n",
       "Begin!\n",
       "\n",
       "Question: $INPUT\n",
       "Thought:$AGENT_SCRATCHPAD"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo understand how the CodeHierarchyNodeParser works, starting from the _parse_nodes method and exploring each function called within it seems like the best approach.\n",
      "Action: Code Search\n",
      "Action Input: _parse_nodes\u001b[0m\u001b[36;1m\u001b[1;3mdef _parse_nodes(\n",
      "        self,\n",
      "        nodes: Sequence[BaseNode],\n",
      "        show_progress: bool = False,\n",
      "        **kwargs: Any,\n",
      "    ) -> List[BaseNode]:\n",
      "# Code replaced for brevity. See node_id b7b5ad87-aeda-458a-b170-c246455faf0e\u001b[0m\u001b[32;1m\u001b[1;3mTo understand how _parse_nodes works, I need to see the full code referenced by the uuid b7b5ad87-aeda-458a-b170-c246455faf0e.\n",
      "Action: Code Search\n",
      "Action Input: b7b5ad87-aeda-458a-b170-c246455faf0e\u001b[0m\u001b[36;1m\u001b[1;3m# Code replaced for brevity. See node_id 70002a26-2741-4d33-9b53-049ea412597c\n",
      "\"\"\"\n",
      "        The main public method of this class.\n",
      "\n",
      "        Parse documents into nodes.\n",
      "        \"\"\"\n",
      "        out: List[BaseNode] = []\n",
      "\n",
      "        try:\n",
      "            import tree_sitter_languages\n",
      "        except ImportError:\n",
      "            raise ImportError(\n",
      "                \"Please install tree_sitter_languages to use CodeSplitter.\"\n",
      "            )\n",
      "\n",
      "        try:\n",
      "            parser = tree_sitter_languages.get_parser(self.language)\n",
      "        except Exception as e:\n",
      "            print(\n",
      "                f\"Could not get parser for language {self.language}. Check \"\n",
      "                \"https://github.com/grantjenks/py-tree-sitter-languages#license \"\n",
      "                \"for a list of valid languages.\"\n",
      "            )\n",
      "            raise e  # noqa: TRY201\n",
      "\n",
      "        nodes_with_progress = get_tqdm_iterable(\n",
      "            nodes, show_progress, \"Parsing documents into nodes\"\n",
      "        )\n",
      "# Code replaced for brevity. See node_id 588991c7-f38f-407e-9e65-0b415ca8c5e3\u001b[0m\u001b[32;1m\u001b[1;3mTo fully understand how _parse_nodes works, I should see the continuation of the code by looking up the uuid 588991c7-f38f-407e-9e65-0b415ca8c5e3.\n",
      "Action: Code Search\n",
      "Action Input: 588991c7-f38f-407e-9e65-0b415ca8c5e3\u001b[0m\u001b[36;1m\u001b[1;3m# Code replaced for brevity. See node_id b7b5ad87-aeda-458a-b170-c246455faf0e\n",
      "for node in nodes_with_progress:\n",
      "            text = node.text\n",
      "            tree = parser.parse(bytes(text, \"utf-8\"))\n",
      "\n",
      "            if (\n",
      "                not tree.root_node.children\n",
      "                or tree.root_node.children[0].type != \"ERROR\"\n",
      "            ):\n",
      "                # Chunk the code\n",
      "                _chunks = self._chunk_node(tree.root_node, node.text)\n",
      "                assert _chunks.this_document is not None, \"Root node must be a chunk\"\n",
      "                chunks = _chunks.all_documents\n",
      "\n",
      "                # Add your metadata to the chunks here\n",
      "                for chunk in chunks:\n",
      "                    chunk.metadata = {\n",
      "                        \"language\": self.language,\n",
      "                        **chunk.metadata,\n",
      "                        **node.metadata,\n",
      "                    }\n",
      "                    chunk.relationships[\n",
      "                        NodeRelationship.SOURCE\n",
      "                    ] = node.as_related_node_info()\n",
      "\n",
      "                if self.skeleton:\n",
      "                    self._skeletonize_list(chunks)\n",
      "\n",
      "                # Now further split the code by lines and characters\n",
      "                # TODO: Test this and the relationships it creates\n",
      "                if self.code_splitter:\n",
      "                    new_nodes = []\n",
      "                    for original_node in chunks:\n",
      "                        new_split_nodes = self.code_splitter.get_nodes_from_documents(\n",
      "                            [original_node], show_progress=show_progress, **kwargs\n",
      "                        )\n",
      "\n",
      "                        # Force the first new_split_node to have the\n",
      "                        # same id as the original_node\n",
      "                        new_split_nodes[0].id_ = original_node.id_\n",
      "\n",
      "                        # Add the UUID of the next node to the end of all nodes\n",
      "                        for i, new_split_node in enumerate(new_split_nodes[:-1]):\n",
      "                            new_split_node.text = (\n",
      "                                new_split_node.text\n",
      "                                + \"\\n\"\n",
      "                                + self._create_comment_line(new_split_nodes[i + 1], 0)\n",
      "                            ).strip()\n",
      "\n",
      "                        # Add the UUID of the previous node to the beginning of all nodes\n",
      "                        for i, new_split_node in enumerate(new_split_nodes[1:]):\n",
      "                            new_split_node.text = (\n",
      "                                self._create_comment_line(new_split_nodes[i])\n",
      "                                + new_split_node.text\n",
      "                            ).strip()\n",
      "\n",
      "                        # Add the parent child info to all the new_nodes_\n",
      "                        # derived from node\n",
      "                        for new_split_node in new_split_nodes:\n",
      "                            new_split_node.relationships[\n",
      "                                NodeRelationship.CHILD\n",
      "                            ] = original_node.child_nodes  # type: ignore\n",
      "                            new_split_node.relationships[\n",
      "                                NodeRelationship.PARENT\n",
      "                            ] = original_node.parent_node  # type: ignore\n",
      "\n",
      "                        # Go through chunks and replace all\n",
      "                        # instances of node.node_id in relationships\n",
      "                        # with new_nodes_[0].node_id\n",
      "                        for old_node in chunks:\n",
      "                            # Handle child nodes, which are a list\n",
      "                            new_children = []\n",
      "                            for old_nodes_child in old_node.child_nodes or []:\n",
      "                                if old_nodes_child.node_id == original_node.node_id:\n",
      "                                    new_children.append(\n",
      "                                        new_split_nodes[0].as_related_node_info()\n",
      "                                    )\n",
      "                                new_children.append(old_nodes_child)\n",
      "                            old_node.relationships[\n",
      "                                NodeRelationship.CHILD\n",
      "                            ] = new_children\n",
      "\n",
      "                            # Handle parent node\n",
      "                            if (\n",
      "                                old_node.parent_node\n",
      "                                and old_node.parent_node.node_id\n",
      "                                == original_node.node_id\n",
      "                            ):\n",
      "                                old_node.relationships[\n",
      "                                    NodeRelationship.PARENT\n",
      "                                ] = new_split_nodes[0].as_related_node_info()\n",
      "\n",
      "                        # Now save new_nodes_\n",
      "                        new_nodes += new_split_nodes\n",
      "\n",
      "                    chunks = new_nodes\n",
      "\n",
      "                # Or just extract metadata\n",
      "                if self.metadata_extractor:\n",
      "                    chunks = self.metadata_extractor.process_nodes(  # type: ignore\n",
      "                        chunks\n",
      "                    )\n",
      "\n",
      "                out += chunks\n",
      "            else:\n",
      "                raise ValueError(f\"Could not parse code with language {self.language}.\")\n",
      "\n",
      "        return out\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The `CodeHierarchyNodeParser` works as follows:\n",
      "\n",
      "1. Initially, it attempts to import `tree_sitter_languages` to utilize its parser. If the package is not installed, it raises an ImportError.\n",
      "2. It then tries to get a parser for the specified language. If it fails, it prints an error message and raises an exception.\n",
      "3. The method processes each node in the provided sequence of nodes. It uses a progress bar if `show_progress` is set to True. Each node's text is parsed into a tree using the `tree_sitter` parser for the specified language.\n",
      "4. For each tree, it checks if the root node's children do not start with an \"ERROR\" node. If the check passes:\n",
      "   - The code is chunked using `_chunk_node`, and for each chunk, metadata (including the language and any node-specific metadata) is added. The source relationship of the node is also set.\n",
      "   - If skeletonization is enabled, `_skeletonize_list` is called on the chunks.\n",
      "   - If code splitting is enabled, it further splits the code by lines and characters, using the code splitter to get new nodes from documents. It ensures that the first new node has the same ID as the original node and annotates nodes with UUIDs of adjacent nodes for easy reference. It also updates parent and child relationships based on the new splits.\n",
      "   - If metadata extraction is enabled, it processes the chunks through a metadata extractor.\n",
      "5. If the root node's first child is of type \"ERROR\", it raises a ValueError indicating that the code could not be parsed for the specified language.\n",
      "6. Finally, it returns the processed chunks or nodes as the output.\n",
      "\n",
      "Throughout this process, several helper methods are utilized, including `_chunk_node`, `_skeletonize_list`, and various methods for handling metadata, relationships, and code splitting. Each of these methods contributes to transforming the input nodes into a structured and enriched output that represents the code hierarchy and relationships more clearly, serving the purpose of parsing and analyzing code documents in a refined manner.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The `CodeHierarchyNodeParser` works as follows:\n",
       "\n",
       "1. Initially, it attempts to import `tree_sitter_languages` to utilize its parser. If the package is not installed, it raises an ImportError.\n",
       "2. It then tries to get a parser for the specified language. If it fails, it prints an error message and raises an exception.\n",
       "3. The method processes each node in the provided sequence of nodes. It uses a progress bar if `show_progress` is set to True. Each node's text is parsed into a tree using the `tree_sitter` parser for the specified language.\n",
       "4. For each tree, it checks if the root node's children do not start with an \"ERROR\" node. If the check passes:\n",
       "   - The code is chunked using `_chunk_node`, and for each chunk, metadata (including the language and any node-specific metadata) is added. The source relationship of the node is also set.\n",
       "   - If skeletonization is enabled, `_skeletonize_list` is called on the chunks.\n",
       "   - If code splitting is enabled, it further splits the code by lines and characters, using the code splitter to get new nodes from documents. It ensures that the first new node has the same ID as the original node and annotates nodes with UUIDs of adjacent nodes for easy reference. It also updates parent and child relationships based on the new splits.\n",
       "   - If metadata extraction is enabled, it processes the chunks through a metadata extractor.\n",
       "5. If the root node's first child is of type \"ERROR\", it raises a ValueError indicating that the code could not be parsed for the specified language.\n",
       "6. Finally, it returns the processed chunks or nodes as the output.\n",
       "\n",
       "Throughout this process, several helper methods are utilized, including `_chunk_node`, `_skeletonize_list`, and various methods for handling metadata, relationships, and code splitting. Each of these methods contributes to transforming the input nodes into a structured and enriched output that represents the code hierarchy and relationships more clearly, serving the purpose of parsing and analyzing code documents in a refined manner."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "# First I'm going to use a bigger max_chars and chunk_lines to get a bigger context window\n",
    "split_nodes = CodeHierarchyNodeParser(\n",
    "    language=\"python\",\n",
    "    chunk_min_characters=500,\n",
    "    # You can further parameterize the CodeSplitter to split the code\n",
    "    # into \"chunks\" that match your context window size using\n",
    "    # chunck_lines and max_chars parameters, here we just use the defaults\n",
    "    code_splitter=CodeSplitter(language=\"python\", max_chars=5000, chunk_lines=100),\n",
    ").get_nodes_from_documents(nodes)\n",
    "idx = CodeHierarchyKeywordQueryEngine(\n",
    "    nodes=split_nodes,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\", max_tokens=4096)\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "tools = [idx.as_langchain_tool()]\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "display(\n",
    "    Markdown(\n",
    "        agent.get_prompts()[0].format(\n",
    "            input=\"$INPUT\", agent_scratchpad=\"$AGENT_SCRATCHPAD\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "out = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"\"\"\n",
    "    How does the code hierarchy node parser work?\n",
    "    Please provide as much detail as possible, and search recursively for all unknown functions starting from _parse_nodes.\n",
    "    I want to know how it works as code, not just a description of its interface.\n",
    "    NodeParsers start at the _parse_nodes method.\"\"\"\n",
    "    }\n",
    ")\n",
    "display(Markdown(out[\"output\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
