{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Hierarchy Node Parser\n",
    "\n",
    "The `CodeHierarchyNodeParser` is useful to split long code files into more reasonable chunks. What this will do is create a \"Hierarchy\" of sorts, where sections of the code are made more reasonable by replacing the scope body with short comments telling the LLM to search for a referenced node if it wants to read that context body. This is called skeletonization, and is toggled by setting `skeleton` to `True` which it is by default. Nodes in this hierarchy will be split based on scope, like function, class, or method scope, and will have links to their children and parents so the LLM can traverse the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Import\n",
    "\n",
    "First be sure to install the necessary [tree-sitter](https://tree-sitter.github.io/tree-sitter/) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tree-sitter in /home/ryanpeach/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages (0.20.4)\n",
      "Requirement already satisfied: tree-sitter-languages in /home/ryanpeach/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages (1.10.2)\n",
      "Requirement already satisfied: python-dotenv in /home/ryanpeach/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tree-sitter tree-sitter-languages python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.text_splitter import CodeSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.packs.code_hierarchy import CodeHierarchyNodeParser\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def print_python(python_text):\n",
    "    \"\"\"This function prints python text in ipynb nicely formatted.\"\"\"\n",
    "    display(Markdown(\"```python\\n\" + python_text + \"```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a directory you want to scan, and glob for all the code files you want to import.\n",
    "\n",
    "In this case I'm going to glob all \"*.py\" files in the `llama_index/node_parser` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[Path(\"../llama_index/packs/code_hierarchy/code_hierarchy.py\")],\n",
    "    file_metadata=lambda x: {\"filepath\": x},\n",
    ")\n",
    "nodes = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be the code hierarchy node parser itself. Lets have it parse itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 33387\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from collections import defaultdict\n",
       "from enum import Enum\n",
       "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
       "\n",
       "from llama_index.core.extractors.metadata_extractors import BaseExtractor\n",
       "from llama_index.core.node_parser.interface import NodeParser\n",
       "\n",
       "try:\n",
       "    from pydantic.v1 import BaseModel, Field\n",
       "except ImportError:\n",
       "    from pydantic import BaseModel, Field\n",
       "\n",
       "from tree_sitter import Node\n",
       "\n",
       "from llama_index.core.callbacks.base import CallbackManager\n",
       "from llama_index.core.schema import BaseNode, NodeRelationship, TextNode\n",
       "from llama_index.core.text_splitter import CodeSplitter\n",
       "from llama_index.core.utils import get_tqdm_iterable\n",
       "\n",
       "\n",
       "class _SignatureCaptureType(BaseModel):\n",
       "    \"\"\"\n",
       "    Unfortunately some languages need special options for how to make a signature.\n",
       "\n",
       "    For example, html element signatures should include their closing >, there is no\n",
       "    easy way to include this using an always-exclusive system.\n",
       "\n",
       "    However, using an always-inclusive system, python decorators don't work,\n",
       "    as there isn't an easy to define terminator for decorators that is inclusive\n",
       "    to their signature.\n",
       "    \"\"\"\n",
       "\n",
       "    type: str = Field(description=\"The type string to match on.\")\n",
       "    inclusive: bool = Field(\n",
       "        description=(\n",
       "            \"Whether to include the text of the node matched by this type or not.\"\n",
       "        ),\n",
       "    )\n",
       "\n",
       "\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "    \"\"\"\n",
       "    Options for capturing the signature of a node.\n",
       "    \"\"\"\n",
       "\n",
       "    start_signature_types: Optional[List[_SignatureCa\n",
       "\n",
       "# ...```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Length of text: {len(nodes[0].text)}\")\n",
    "print_python(nodes[0].text[:1500] + \"\\n\\n# ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is way too long to fit into the context of our LLM. So what are we to do? Well we will split it. We are going to use the `CodeHierarchyNodeParser` to split the nodes into more reasonable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes after splitting: 90\n"
     ]
    }
   ],
   "source": [
    "split_nodes = CodeHierarchyNodeParser(\n",
    "    language=\"python\",\n",
    "    # You can further parameterize the CodeSplitter to split the code\n",
    "    # into \"chunks\" that match your context window size using\n",
    "    # chunck_lines and max_chars parameters, here we just use the defaults\n",
    "    code_splitter=CodeSplitter(language=\"python\", max_chars=1000, chunk_lines=10),\n",
    ").get_nodes_from_documents(nodes)\n",
    "print(\"Number of nodes after splitting:\", len(split_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So that split up our data from 1 node into quite a few nodes! Whats the max length of any of these nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest text in nodes: 1152\n"
     ]
    }
   ],
   "source": [
    "print(f\"Longest text in nodes: {max(len(n.text) for n in split_nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much shorter than before! Let's look at a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from collections import defaultdict\n",
       "from enum import Enum\n",
       "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
       "\n",
       "from llama_index.core.extractors.metadata_extractors import BaseExtractor\n",
       "from llama_index.core.node_parser.interface import NodeParser\n",
       "\n",
       "try:\n",
       "    from pydantic.v1 import BaseModel, Field\n",
       "except ImportError:\n",
       "    from pydantic import BaseModel, Field\n",
       "\n",
       "from tree_sitter import Node\n",
       "\n",
       "from llama_index.core.callbacks.base import CallbackManager\n",
       "from llama_index.core.schema import BaseNode, NodeRelationship, TextNode\n",
       "from llama_index.core.text_splitter import CodeSplitter\n",
       "from llama_index.core.utils import get_tqdm_iterable\n",
       "\n",
       "\n",
       "class _SignatureCaptureType(BaseModel):\n",
       "    # Code replaced for brevity. See node_id ae4ec924-ae0a-41f7-8768-8e31f5f862eb\n",
       "\n",
       "\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 47336dc3-f8f3-4a63-a44b-fd10f7d2d3e0\n",
       "# Code replaced for brevity. See node_id 215ed2e6-6ea3-4a40-8198-e9dc5ac8bf02```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(split_nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without even needing a long printout we can see everything this module imported in the first document (which is at the module level) and some classes it defines.\n",
    "\n",
    "We also see that it has put comments in place of code that was removed to make the text size more reasonable.\n",
    "These can appear at the beginning or end of a chunk, or at a new scope level, like a class or function declaration.\n",
    "\n",
    "`# Code replaced for brevity. See node_id {node_id}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Hierarchy\n",
    "\n",
    "These scopes can be listed by the `CodeHierarchyNodeParser`, giving a \"repo map\" of sorts.\n",
    "The namesake of this node parser, it creates a tree of scope names to use to search the code.\n",
    "Put this in your context to give the LLM a default search hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruct an LLM using the KeywordQueryEngine (shown later) as a tool to:\n",
    "\n",
    "```\n",
    "\"Search the tool by any element in this list, or any uuid found in the resulting code, to get more information about that element.\"\n",
    "```\n",
    "\n",
    "Then append this to your context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ..\n",
      "  - llama_index\n",
      "    - packs\n",
      "      - code_hierarchy\n",
      "        - code_hierarchy\n",
      "          - _SignatureCaptureType\n",
      "          - _SignatureCaptureOptions\n",
      "          - _ScopeMethod\n",
      "          - _CommentOptions\n",
      "          - _ScopeItem\n",
      "          - _ChunkNodeOutput\n",
      "          - CodeHierarchyNodeParser\n",
      "            - class_name\n",
      "            - __init__\n",
      "            - _get_node_name\n",
      "              - recur\n",
      "            - _get_node_signature\n",
      "              - find_start\n",
      "              - find_end\n",
      "            - _chunk_node\n",
      "            - get_code_hierarchy_from_nodes\n",
      "              - get_subdict\n",
      "              - recur_inclusive_scope\n",
      "              - dict_to_markdown\n",
      "            - _parse_nodes\n",
      "            - _get_indentation\n",
      "            - _get_comment_text\n",
      "            - _create_comment_line\n",
      "            - _get_replacement_text\n",
      "            - _skeletonize\n",
      "            - _skeletonize_list\n",
      "              - recur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(CodeHierarchyNodeParser.get_code_hierarchy_from_nodes(split_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration by the Programmer\n",
    "\n",
    "So that we understand what is going on under the hood, what if we go to that node_id we found above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to print the node with UUID: 1eabd396-269b-4717-88e6-46cd5ed691ff\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class CodeHierarchyNodeParser(NodeParser):\n",
       "# Code replaced for brevity. See node_id 32a9da0e-4371-4fb6-9ad4-c9a50e45644d```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_nodes_by_id = {n.node_id: n for n in split_nodes}\n",
    "uuid_from_text = split_nodes[9].text.splitlines()[-1].split(\" \")[-1]\n",
    "print(\"Going to print the node with UUID:\", uuid_from_text)\n",
    "print_python(split_nodes_by_id[uuid_from_text].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the next split in the file. It is prepended with the node before it and appended with the node after it as a comment.\n",
    "\n",
    "We can also see the relationships on this node programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1eabd396-269b-4717-88e6-46cd5ed691ff', node_type=<ObjectType.TEXT: '1'>, metadata={'language': 'python', 'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}], 'start_byte': 6293, 'end_byte': 33386, 'filepath': '../llama_index/packs/code_hierarchy/code_hierarchy.py'}, hash='d0e521764e7cfdcf78121308a29d1799d4b1dbbd2766b1cd3df28c063a40275d'),\n",
       " <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='32a9da0e-4371-4fb6-9ad4-c9a50e45644d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='32828ff99ca6784694e7d9acd28509aec7d5ed90664f89d51df0ab2b89f8c57b'),\n",
       " <NodeRelationship.CHILD: '5'>: [RelatedNodeInfo(node_id='bbc17523-f348-4e2c-8c93-606398059434', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': 'class_name', 'type': 'function_definition', 'signature': 'def class_name(cls) -> str:'}], 'start_byte': 6495, 'end_byte': 6597}, hash='0a1d51ecc201c8edf92d96677720b4468e3190917fb5b76c2dd2db424fa74fa6'),\n",
       "  RelatedNodeInfo(node_id='ce9ae145-df66-4fd5-90ad-6225ec6f3009', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '__init__', 'type': 'function_definition', 'signature': 'def __init__(\\n        self,\\n        language: str,\\n        skeleton: bool = True,\\n        signature_identifiers: Optional[Dict[str, _SignatureCaptureOptions]] = None,\\n        code_splitter: Optional[CodeSplitter] = None,\\n        callback_manager: Optional[CallbackManager] = None,\\n        metadata_extractor: Optional[BaseExtractor] = None,\\n        min_characters: int = 80,\\n    ):'}], 'start_byte': 7896, 'end_byte': 9032}, hash='6a6fe96a93b4935ce04400483e9aa8a5ab6a7ae4ab7d4d4dc698e49a630c5c46'),\n",
       "  RelatedNodeInfo(node_id='619ff56f-a9c0-42ab-b631-eb25a7f4a393', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_get_node_name', 'type': 'function_definition', 'signature': 'def _get_node_name(self, node: Node) -> str:'}], 'start_byte': 9034, 'end_byte': 9581}, hash='86602175e569c471815d1cf39af5f5d2a2af14bec2ddb51dc5568a83fdb8f333'),\n",
       "  RelatedNodeInfo(node_id='c92ce23d-a595-4d66-b657-3d3055d631b8', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_get_node_signature', 'type': 'function_definition', 'signature': 'def _get_node_signature(self, text: str, node: Node) -> str:'}], 'start_byte': 9583, 'end_byte': 11175}, hash='bc8a90f9a87287fd2f10b0c00093694af3b47d29837c6c0a35f6db2f1ec7b63a'),\n",
       "  RelatedNodeInfo(node_id='211c3e86-d554-42d0-8287-d11ebb9fe6bb', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_chunk_node', 'type': 'function_definition', 'signature': 'def _chunk_node(\\n        self,\\n        parent: Node,\\n        text: str,\\n        _context_list: Optional[List[_ScopeItem]] = None,\\n        _root: bool = True,\\n    ) -> _ChunkNodeOutput:'}], 'start_byte': 11177, 'end_byte': 17256}, hash='8cd30269432ecf9c6dc8823a818a80c2059666a594f14eb46bf3b3dce682dd66'),\n",
       "  RelatedNodeInfo(node_id='5cc2fbed-1ffb-4858-8108-dae4804935f2', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': 'get_code_hierarchy_from_nodes', 'type': 'function_definition', 'signature': 'def get_code_hierarchy_from_nodes(\\n        nodes: Sequence[BaseNode],\\n        max_depth: int = -1,\\n    ) -> str:'}], 'start_byte': 17276, 'end_byte': 19511}, hash='457277107831732e9b5a6a6fcb9eb2acd13fdd903f1feff9f79618ffe4af10a7'),\n",
       "  RelatedNodeInfo(node_id='62b82238-6361-49d0-abc0-b54d0aaf48be', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_parse_nodes', 'type': 'function_definition', 'signature': 'def _parse_nodes(\\n        self,\\n        nodes: Sequence[BaseNode],\\n        show_progress: bool = False,\\n        **kwargs: Any,\\n    ) -> List[BaseNode]:'}], 'start_byte': 19513, 'end_byte': 25484}, hash='ea1a383566bbc99612fb9ba1b3c72e0c33f6267067b3130cbdaebc16e2046c80'),\n",
       "  RelatedNodeInfo(node_id='a3e01110-0c3e-458b-8e1e-fad86e4913bd', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_get_indentation', 'type': 'function_definition', 'signature': 'def _get_indentation(text: str) -> Tuple[str, int, int]:'}], 'start_byte': 25504, 'end_byte': 27829}, hash='2509d11f921baf6eb994bccd87ec6085d372dbb14ccc80de0716e4e306b6f423'),\n",
       "  RelatedNodeInfo(node_id='ab233f93-e25b-411e-baf5-33da8f7bf0f6', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_get_comment_text', 'type': 'function_definition', 'signature': 'def _get_comment_text(node: TextNode) -> str:'}], 'start_byte': 27849, 'end_byte': 28047}, hash='d9728168fd5aee5f2d924fdc5192668e52ee227a424611ce9fd5c18166b00519'),\n",
       "  RelatedNodeInfo(node_id='b90a8f7b-c21d-4e23-be2d-10317772c925', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_create_comment_line', 'type': 'function_definition', 'signature': 'def _create_comment_line(cls, node: TextNode, indention_lvl: int = -1) -> str:'}], 'start_byte': 28066, 'end_byte': 29187}, hash='912654de7717d00fa8fa2bb324d3448222ae56ae45720405a3fafc9313cbf574'),\n",
       "  RelatedNodeInfo(node_id='74dec2de-3a6c-46f6-a83f-294c1bc515cf', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_get_replacement_text', 'type': 'function_definition', 'signature': 'def _get_replacement_text(cls, child_node: TextNode) -> str:'}], 'start_byte': 29206, 'end_byte': 32029}, hash='9fb40f8cb21bab2219516f8c96ae6ad57c1171fe2475a5affce8a6200845b7f4'),\n",
       "  RelatedNodeInfo(node_id='266e05b7-22af-48be-be0d-bb3d41cb8a86', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_skeletonize', 'type': 'function_definition', 'signature': 'def _skeletonize(cls, parent_node: TextNode, child_node: TextNode) -> None:'}], 'start_byte': 32048, 'end_byte': 32714}, hash='f533b663a986447f87827b5c7144e1e47164e32cdf3c95df78a3c93b14fe57cb'),\n",
       "  RelatedNodeInfo(node_id='06f30d76-0e66-4ab8-9f1d-20b993a1c60e', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'CodeHierarchyNodeParser', 'type': 'class_definition', 'signature': 'class CodeHierarchyNodeParser(NodeParser):'}, {'name': '_skeletonize_list', 'type': 'function_definition', 'signature': \"def _skeletonize_list(cls, nodes: List[TextNode]) -> None:\\n        # Create a convenient map for mapping node id's to nodes\"}], 'start_byte': 32733, 'end_byte': 33386}, hash='e3014c168122f456230ae064d821eb7ea9d60b36a9942fe52926b4e67d946fc9')],\n",
       " <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='3496b8cc-5113-4e9a-a046-ff12b17b0177', node_type=<ObjectType.TEXT: '1'>, metadata={'language': 'python', 'inclusive_scopes': [], 'start_byte': 0, 'end_byte': 33387, 'filepath': '../llama_index/packs/code_hierarchy/code_hierarchy.py'}, hash='b7000fdd8d6e993914512706bbd71b934c4520d9e65bc5efb7170eb4c0927a68')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_nodes_by_id[uuid_from_text].relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NEXT` `PREV` relationships come from the `CodeSplitter` which is a component of the `CodeHierarchyNodeParser`. It is responsible for cutting up the nodes into chunks that are a certain character length. For more information about the `CodeSplitter` read this:\n",
    "\n",
    "[Code Splitter](https://docs.llamaindex.ai/en/latest/api/llama_index.node_parser.CodeSplitter.html)\n",
    "\n",
    "The `PARENT` and `CHILD` relationships come from the `CodeHierarchyNodeParser` which is responsible for creating the hierarchy of nodes. Things like classes, functions, and methods are nodes in this hierarchy.\n",
    "\n",
    "The `SOURCE` is the original file that this node came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "# Code replaced for brevity. See node_id 1eabd396-269b-4717-88e6-46cd5ed691ff\n",
       "\"\"\"Split code using a AST parser.\n",
       "\n",
       "    Add metadata about the scope of the code block and relationships between\n",
       "    code blocks.\n",
       "    \"\"\"\n",
       "\n",
       "    @classmethod\n",
       "    def class_name(cls) -> str:\n",
       "        # Code replaced for brevity. See node_id bbc17523-f348-4e2c-8c93-606398059434\n",
       "\n",
       "    language: str = Field(\n",
       "        description=\"The programming language of the code being split.\"\n",
       "    )\n",
       "    signature_identifiers: Dict[str, _SignatureCaptureOptions] = Field(\n",
       "        description=(\n",
       "            \"A dictionary mapping the type of a split mapped to the first and last type\"\n",
       "            \" of itschildren which identify its signature.\"\n",
       "        )\n",
       "    )\n",
       "    min_characters: int = Field(\n",
       "        default=80,\n",
       "        description=(\n",
       "            \"Minimum number of characters per chunk.Defaults to 80 because that's about\"\n",
       "            \" how long a replacement comment is in skeleton mode.\"\n",
       "        ),\n",
       "    )\n",
       "# Code replaced for brevity. See node_id 602fcfc5-9dca-46f0-8c92-4daf218110eb```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.schema import NodeRelationship\n",
    "\n",
    "node_id = uuid_from_text\n",
    "if NodeRelationship.NEXT not in split_nodes_by_id[node_id].relationships:\n",
    "    print(\"No next node found!\")\n",
    "else:\n",
    "    next_node_relationship_info = split_nodes_by_id[node_id].relationships[\n",
    "        NodeRelationship.NEXT\n",
    "    ]\n",
    "    next_node = split_nodes_by_id[next_node_relationship_info.node_id]\n",
    "    print_python(next_node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Table and Usage by the LLM\n",
    "\n",
    "Lets explore the use of this node parser in an index. We will be able to use any index which allows search by keyword, which should enable us to search for any node by it's uuid, or by any scope name.\n",
    "\n",
    "We have created a `CodeHierarchyKeywordQueryEngine` which will allow us to search for nodes by their uuid, or by their scope name. It's `.query` method can be used as a simple search tool for any LLM. Given the repo map we created earlier, or the text of a split file, the LLM should be able to figure out what to search for very naturally.\n",
    "\n",
    "Lets create the KeywordQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.packs.code_hierarchy import CodeHierarchyKeywordQueryEngine\n",
    "\n",
    "idx = CodeHierarchyKeywordQueryEngine(\n",
    "    nodes=split_nodes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the same code as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from collections import defaultdict\n",
       "from enum import Enum\n",
       "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
       "\n",
       "from llama_index.core.extractors.metadata_extractors import BaseExtractor\n",
       "from llama_index.core.node_parser.interface import NodeParser\n",
       "\n",
       "try:\n",
       "    from pydantic.v1 import BaseModel, Field\n",
       "except ImportError:\n",
       "    from pydantic import BaseModel, Field\n",
       "\n",
       "from tree_sitter import Node\n",
       "\n",
       "from llama_index.core.callbacks.base import CallbackManager\n",
       "from llama_index.core.schema import BaseNode, NodeRelationship, TextNode\n",
       "from llama_index.core.text_splitter import CodeSplitter\n",
       "from llama_index.core.utils import get_tqdm_iterable\n",
       "\n",
       "\n",
       "class _SignatureCaptureType(BaseModel):\n",
       "    # Code replaced for brevity. See node_id ae4ec924-ae0a-41f7-8768-8e31f5f862eb\n",
       "\n",
       "\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 47336dc3-f8f3-4a63-a44b-fd10f7d2d3e0\n",
       "# Code replaced for brevity. See node_id 215ed2e6-6ea3-4a40-8198-e9dc5ac8bf02```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(idx.query(split_nodes[0].node_id).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we can also search for any node by it's common sense name.\n",
    "\n",
    "For example, the class `_SignatureCaptureOptions` is a node in the hierarchy. We can search for it by name.\n",
    "\n",
    "The reason we aren't getting more detail is because our min_characters is too low, try to increase it for more detail for any individual query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "# Code replaced for brevity. See node_id d740dee7-c3d1-4153-b35f-af3913c77012```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(idx.query(\"_SignatureCaptureOptions\").response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by module name, in case the LLM sees something in an import statement and wants to know more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from collections import defaultdict\n",
       "from enum import Enum\n",
       "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
       "\n",
       "from llama_index.core.extractors.metadata_extractors import BaseExtractor\n",
       "from llama_index.core.node_parser.interface import NodeParser\n",
       "\n",
       "try:\n",
       "    from pydantic.v1 import BaseModel, Field\n",
       "except ImportError:\n",
       "    from pydantic import BaseModel, Field\n",
       "\n",
       "from tree_sitter import Node\n",
       "\n",
       "from llama_index.core.callbacks.base import CallbackManager\n",
       "from llama_index.core.schema import BaseNode, NodeRelationship, TextNode\n",
       "from llama_index.core.text_splitter import CodeSplitter\n",
       "from llama_index.core.utils import get_tqdm_iterable\n",
       "\n",
       "\n",
       "class _SignatureCaptureType(BaseModel):\n",
       "    # Code replaced for brevity. See node_id ae4ec924-ae0a-41f7-8768-8e31f5f862eb\n",
       "\n",
       "\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 47336dc3-f8f3-4a63-a44b-fd10f7d2d3e0\n",
       "# Code replaced for brevity. See node_id 215ed2e6-6ea3-4a40-8198-e9dc5ac8bf02```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(idx.query(\"code_hierarchy\").response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As a Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the langchain tool, just use `as_langchain_tool` on the `CodeHierarchyKeywordQueryEngine` and it will be ready to use in the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from collections import defaultdict\n",
       "from enum import Enum\n",
       "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
       "\n",
       "from llama_index.core.extractors.metadata_extractors import BaseExtractor\n",
       "from llama_index.core.node_parser.interface import NodeParser\n",
       "\n",
       "try:\n",
       "    from pydantic.v1 import BaseModel, Field\n",
       "except ImportError:\n",
       "    from pydantic import BaseModel, Field\n",
       "\n",
       "from tree_sitter import Node\n",
       "\n",
       "from llama_index.core.callbacks.base import CallbackManager\n",
       "from llama_index.core.schema import BaseNode, NodeRelationship, TextNode\n",
       "from llama_index.core.text_splitter import CodeSplitter\n",
       "from llama_index.core.utils import get_tqdm_iterable\n",
       "\n",
       "\n",
       "class _SignatureCaptureType(BaseModel):\n",
       "    # Code replaced for brevity. See node_id ae4ec924-ae0a-41f7-8768-8e31f5f862eb\n",
       "\n",
       "\n",
       "class _SignatureCaptureOptions(BaseModel):\n",
       "    # Code replaced for brevity. See node_id 47336dc3-f8f3-4a63-a44b-fd10f7d2d3e0\n",
       "# Code replaced for brevity. See node_id 215ed2e6-6ea3-4a40-8198-e9dc5ac8bf02```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_python(idx.as_langchain_tool().run(\"code_hierarchy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description for your LLM to read to learn the tool is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Code Search\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Description: \n",
       "        Search the tool by any element in this list,\n",
       "        or any uuid found in the code,\n",
       "        to get more information about that element.\n",
       "\n",
       "        - ..\n",
       "  - llama_index\n",
       "    - packs\n",
       "      - code_hierarchy\n",
       "        - code_hierarchy\n",
       "          - _SignatureCaptureType\n",
       "          - _SignatureCaptureOptions\n",
       "          - _ScopeMethod\n",
       "          - _CommentOptions\n",
       "          - _ScopeItem\n",
       "          - _ChunkNodeOutput\n",
       "          - CodeHierarchyNodeParser\n",
       "            - class_name\n",
       "            - __init__\n",
       "            - _get_node_name\n",
       "              - recur\n",
       "            - _get_node_signature\n",
       "              - find_start\n",
       "              - find_end\n",
       "            - _chunk_node\n",
       "            - get_code_hierarchy_from_nodes\n",
       "              - get_subdict\n",
       "              - recur_inclusive_scope\n",
       "              - dict_to_markdown\n",
       "            - _parse_nodes\n",
       "            - _get_indentation\n",
       "            - _get_comment_text\n",
       "            - _create_comment_line\n",
       "            - _get_replacement_text\n",
       "            - _skeletonize\n",
       "            - _skeletonize_list\n",
       "              - recur\n",
       "\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Name: \" + idx.as_langchain_tool().name)\n",
    "display(Markdown(\"Description: \" + idx.as_langchain_tool().description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets finally actually make an agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ryanpeach/Documents/Workspace/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/examples/CodeHierarchyNodeParserUsage.ipynb Cell 41\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryanpeach/Documents/Workspace/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/examples/CodeHierarchyNodeParserUsage.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m agent \u001b[39m=\u001b[39m create_react_agent(llm, tools, prompt)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ryanpeach/Documents/Workspace/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/examples/CodeHierarchyNodeParserUsage.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m agent_executor \u001b[39m=\u001b[39m AgentExecutor(agent\u001b[39m=\u001b[39magent, tools\u001b[39m=\u001b[39mtools, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_iterations\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ryanpeach/Documents/Workspace/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/examples/CodeHierarchyNodeParserUsage.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m agent_executor\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mHow does the code hierarchy node parser work?\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[39mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    154\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain/agents/agent.py:1391\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1391\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m   1392\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1393\u001b[0m         color_mapping,\n\u001b[1;32m   1394\u001b[0m         inputs,\n\u001b[1;32m   1395\u001b[0m         intermediate_steps,\n\u001b[1;32m   1396\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m   1397\u001b[0m     )\n\u001b[1;32m   1398\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1399\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1400\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1401\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain/agents/agent.py:1097\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_next_step\u001b[39m(\n\u001b[1;32m   1089\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1090\u001b[0m     name_to_tool_map: Dict[\u001b[39mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1095\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[39mstr\u001b[39m]]]:\n\u001b[1;32m   1096\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1097\u001b[0m         [\n\u001b[1;32m   1098\u001b[0m             a\n\u001b[1;32m   1099\u001b[0m             \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iter_next_step(\n\u001b[1;32m   1100\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1101\u001b[0m                 color_mapping,\n\u001b[1;32m   1102\u001b[0m                 inputs,\n\u001b[1;32m   1103\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1104\u001b[0m                 run_manager,\n\u001b[1;32m   1105\u001b[0m             )\n\u001b[1;32m   1106\u001b[0m         ]\n\u001b[1;32m   1107\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain/agents/agent.py:1097\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_next_step\u001b[39m(\n\u001b[1;32m   1089\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1090\u001b[0m     name_to_tool_map: Dict[\u001b[39mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1095\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[39mstr\u001b[39m]]]:\n\u001b[1;32m   1096\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1097\u001b[0m         [\n\u001b[1;32m   1098\u001b[0m             a\n\u001b[1;32m   1099\u001b[0m             \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iter_next_step(\n\u001b[1;32m   1100\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1101\u001b[0m                 color_mapping,\n\u001b[1;32m   1102\u001b[0m                 inputs,\n\u001b[1;32m   1103\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1104\u001b[0m                 run_manager,\n\u001b[1;32m   1105\u001b[0m             )\n\u001b[1;32m   1106\u001b[0m         ]\n\u001b[1;32m   1107\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain/agents/agent.py:1125\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     intermediate_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m   1124\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[1;32m   1126\u001b[0m         intermediate_steps,\n\u001b[1;32m   1127\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1128\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m   1129\u001b[0m     )\n\u001b[1;32m   1130\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1131\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain/agents/agent.py:387\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39m# Use streaming to make sure that the underlying LLM is invoked in a streaming\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# fashion to make it possible to get access to the individual LLM tokens\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m# when using stream_log with the Agent Executor.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m final_output: Any \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunnable\u001b[39m.\u001b[39;49mstream(inputs, config\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m: callbacks}):\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;49;00m final_output \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[1;32m    389\u001b[0m         final_output \u001b[39m=\u001b[39;49m chunk\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain_core/runnables/base.py:2446\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstream\u001b[39m(\n\u001b[1;32m   2441\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2442\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[1;32m   2443\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2444\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2445\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2446\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(\u001b[39miter\u001b[39m([\u001b[39minput\u001b[39m]), config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain_core/runnables/base.py:2433\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2427\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\n\u001b[1;32m   2428\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2429\u001b[0m     \u001b[39minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   2430\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2431\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2432\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2433\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m   2434\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m   2435\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform,\n\u001b[1;32m   2436\u001b[0m         patch_config(config, run_name\u001b[39m=\u001b[39m(config \u001b[39mor\u001b[39;00m {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname),\n\u001b[1;32m   2437\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2438\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain_core/runnables/base.py:1513\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1512\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1513\u001b[0m         chunk: Output \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mrun(\u001b[39mnext\u001b[39m, iterator)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m         \u001b[39myield\u001b[39;00m chunk\n\u001b[1;32m   1515\u001b[0m         \u001b[39mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain_core/runnables/base.py:2397\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[0;34m(self, input, run_manager, config)\u001b[0m\n\u001b[1;32m   2388\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m steps:\n\u001b[1;32m   2389\u001b[0m     final_pipeline \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39mtransform(\n\u001b[1;32m   2390\u001b[0m         final_pipeline,\n\u001b[1;32m   2391\u001b[0m         patch_config(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2394\u001b[0m         ),\n\u001b[1;32m   2395\u001b[0m     )\n\u001b[0;32m-> 2397\u001b[0m \u001b[39mfor\u001b[39;49;00m output \u001b[39min\u001b[39;49;00m final_pipeline:\n\u001b[1;32m   2398\u001b[0m     \u001b[39myield\u001b[39;49;00m output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain_core/runnables/base.py:1051\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m final: Input\n\u001b[1;32m   1049\u001b[0m got_first_val \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39minput\u001b[39;49m:\n\u001b[1;32m   1052\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m got_first_val:\n\u001b[1;32m   1053\u001b[0m         final \u001b[39m=\u001b[39;49m chunk\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain_core/runnables/base.py:4173\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\n\u001b[1;32m   4168\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4169\u001b[0m     \u001b[39minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   4170\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   4171\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   4172\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 4173\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbound\u001b[39m.\u001b[39mtransform(\n\u001b[1;32m   4174\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m   4175\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4176\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs},\n\u001b[1;32m   4177\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain_core/runnables/base.py:1061\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         final \u001b[39m=\u001b[39m final \u001b[39m+\u001b[39m chunk  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39mif\u001b[39;00m got_first_val:\n\u001b[0;32m-> 1061\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream(final, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain_core/language_models/llms.py:452\u001b[0m, in \u001b[0;36mBaseLLM.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(\n\u001b[1;32m    447\u001b[0m         e,\n\u001b[1;32m    448\u001b[0m         response\u001b[39m=\u001b[39mLLMResult(\n\u001b[1;32m    449\u001b[0m             generations\u001b[39m=\u001b[39m[[generation]] \u001b[39mif\u001b[39;00m generation \u001b[39melse\u001b[39;00m []\n\u001b[1;32m    450\u001b[0m         ),\n\u001b[1;32m    451\u001b[0m     )\n\u001b[0;32m--> 452\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    453\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_end(LLMResult(generations\u001b[39m=\u001b[39m[[generation]]))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain_core/language_models/llms.py:436\u001b[0m, in \u001b[0;36mBaseLLM.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m generation: Optional[GenerationChunk] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 436\u001b[0m     \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream(\n\u001b[1;32m    437\u001b[0m         prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    438\u001b[0m     ):\n\u001b[1;32m    439\u001b[0m         \u001b[39myield\u001b[39;49;00m chunk\u001b[39m.\u001b[39;49mtext\n\u001b[1;32m    440\u001b[0m         \u001b[39mif\u001b[39;49;00m generation \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/langchain_openai/llms/base.py:252\u001b[0m, in \u001b[0;36mBaseOpenAI._stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invocation_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n\u001b[1;32m    251\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_sub_prompts(params, [prompt], stop)  \u001b[39m# this mutates params\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m \u001b[39mfor\u001b[39;00m stream_resp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(prompt\u001b[39m=\u001b[39;49mprompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams):\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(stream_resp, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    254\u001b[0m         stream_resp \u001b[39m=\u001b[39m stream_resp\u001b[39m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/openai/resources/completions.py:506\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    480\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    505\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Completion \u001b[39m|\u001b[39m Stream[Completion]:\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    507\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    508\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    509\u001b[0m             {\n\u001b[1;32m    510\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    511\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt,\n\u001b[1;32m    512\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mbest_of\u001b[39;49m\u001b[39m\"\u001b[39;49m: best_of,\n\u001b[1;32m    513\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mecho\u001b[39;49m\u001b[39m\"\u001b[39;49m: echo,\n\u001b[1;32m    514\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    515\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    516\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    517\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    518\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    519\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    520\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    521\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    522\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    523\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39msuffix\u001b[39;49m\u001b[39m\"\u001b[39;49m: suffix,\n\u001b[1;32m    524\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    525\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    526\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    527\u001b[0m             },\n\u001b[1;32m    528\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    529\u001b[0m         ),\n\u001b[1;32m    530\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    531\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    532\u001b[0m         ),\n\u001b[1;32m    533\u001b[0m         cast_to\u001b[39m=\u001b[39;49mCompletion,\n\u001b[1;32m    534\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    535\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[Completion],\n\u001b[1;32m    536\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    890\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    891\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    892\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    893\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    894\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    895\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    966\u001b[0m         options,\n\u001b[1;32m    967\u001b[0m         cast_to,\n\u001b[1;32m    968\u001b[0m         retries,\n\u001b[1;32m    969\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    970\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    971\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    972\u001b[0m     )\n\u001b[1;32m    974\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1014\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1015\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1016\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m   1017\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1018\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1019\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    966\u001b[0m         options,\n\u001b[1;32m    967\u001b[0m         cast_to,\n\u001b[1;32m    968\u001b[0m         retries,\n\u001b[1;32m    969\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    970\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    971\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    972\u001b[0m     )\n\u001b[1;32m    974\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1014\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1015\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1016\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m   1017\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1018\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1019\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/envs/llama_index/lib/python3.11/site-packages/openai/_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    977\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m    979\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 980\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m    983\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m    984\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_openai import OpenAI\n",
    "from langchain import hub\n",
    "\n",
    "llm = OpenAI()\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "tools = [idx.as_langchain_tool()]\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=5)\n",
    "agent_executor.invoke({\"input\": \"How does the code hierarchy node parser work?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
