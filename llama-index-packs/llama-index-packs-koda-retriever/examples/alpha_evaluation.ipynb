{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG w/ Alpha Tuning\n",
    "Evaluation is a crucial piece of development when building a RAG pipeline. Likewise, alpha tuning can be a time consuming exercise to build out, so what does the performance benefit look like for all of this extra effort? Let's dig into that.\n",
    "\n",
    "### Fixtures\n",
    "- For our dataset: Llama2 Paper\n",
    "- Our vector db: [Pinecone](https://www.pinecone.io/)\n",
    "- For our embedding model: [ada-002](https://platform.openai.com/docs/models/embeddings) \n",
    "- For our LLM: [GPT-3.5 Turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)\n",
    "\n",
    "These fixtures were chosen because they're well integrated within LlamaIndex to make this notebook very transferrable to those looking to reproduce this.\n",
    "Likewise, Pinecone supports hybrid search, which is a requirement for hybrid searches to be possible. Koda Retriever will also be used!\n",
    "\n",
    "### Testing\n",
    "\n",
    "Koda Retriever was largely inspired from the alpha tuning [blog post written by Ravi Theja](https://blog.llamaindex.ai/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00) from Llama Index. For that reason, we'll follow a similar pattern and evaluate with:\n",
    "- MRR (Mean Reciprocal Rank)\n",
    "- Hit Rate\n",
    "\n",
    "### Agenda:\n",
    "- Fixture Setup\n",
    "- Data Ingestion\n",
    "- Synthetic Query Generation\n",
    "- Alpha Mining\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary modules\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.koda_retriever import KodaRetriever\n",
    "import os\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixture Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "index = pc.Index(\"llama2-paper\")  # this was previously created in my pinecone account\n",
    "\n",
    "Settings.llm = OpenAI()\n",
    "Settings.embed_model = OpenAIEmbedding()\n",
    "\n",
    "# if you recreate this using the default dataset in pinecone, you'll want to set `text_key = \"summary\"`\n",
    "vector_store = PineconeVectorStore(pinecone_index=index)\n",
    "vector_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "reranker = LLMRerank(llm=Settings.llm)  # optional\n",
    "\n",
    "koda_retriever = KodaRetriever(\n",
    "    index=vector_index,\n",
    "    llm=Settings.llm,\n",
    "    reranker=reranker,  # optional\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "vanilla_retriever = vector_index.as_retriever()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
