{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-13 14:29:49--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10q/uber_10q_march_2022.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1260185 (1.2M) [application/octet-stream]\n",
      "Saving to: ‘data/10q/uber_10q_march_2022.pdf’\n",
      "\n",
      "data/10q/uber_10q_m 100%[===================>]   1.20M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-02-13 14:29:50 (19.4 MB/s) - ‘data/10q/uber_10q_march_2022.pdf’ saved [1260185/1260185]\n",
      "\n",
      "--2024-02-13 14:29:50--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10q/uber_10q_june_2022.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1238483 (1.2M) [application/octet-stream]\n",
      "Saving to: ‘data/10q/uber_10q_june_2022.pdf’\n",
      "\n",
      "data/10q/uber_10q_j 100%[===================>]   1.18M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2024-02-13 14:29:50 (22.2 MB/s) - ‘data/10q/uber_10q_june_2022.pdf’ saved [1238483/1238483]\n",
      "\n",
      "--2024-02-13 14:29:51--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10q/uber_10q_sept_2022.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1178622 (1.1M) [application/octet-stream]\n",
      "Saving to: ‘data/10q/uber_10q_sept_2022.pdf’\n",
      "\n",
      "data/10q/uber_10q_s 100%[===================>]   1.12M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2024-02-13 14:29:51 (22.4 MB/s) - ‘data/10q/uber_10q_sept_2022.pdf’ saved [1178622/1178622]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/10q/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10q/uber_10q_march_2022.pdf' -O 'data/10q/uber_10q_march_2022.pdf'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10q/uber_10q_june_2022.pdf' -O 'data/10q/uber_10q_june_2022.pdf'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10q/uber_10q_sept_2022.pdf' -O 'data/10q/uber_10q_sept_2022.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_2022 = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/10q/uber_10q_march_2022.pdf\"]\n",
    ").load_data()\n",
    "june_2022 = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/10q/uber_10q_june_2022.pdf\"]\n",
    ").load_data()\n",
    "sept_2022 = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/10q/uber_10q_sept_2022.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "march_index = VectorStoreIndex.from_documents(march_2022)\n",
    "june_index = VectorStoreIndex.from_documents(june_2022)\n",
    "sept_index = VectorStoreIndex.from_documents(sept_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import AgentRunner\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.packs.query_understanding_agent.step import (\n",
    "    QueryUnderstandingAgentWorker,\n",
    ")\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_engine = march_index.as_query_engine(similarity_top_k=3, llm=llm)\n",
    "june_engine = june_index.as_query_engine(similarity_top_k=3, llm=llm)\n",
    "sept_engine = sept_index.as_query_engine(similarity_top_k=3, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "query_tool_sept = QueryEngineTool.from_defaults(\n",
    "    query_engine=sept_engine,\n",
    "    name=\"sept_2022\",\n",
    "    description=(\n",
    "        f\"Provides information about Uber quarterly financials ending\"\n",
    "        f\" September 2022\"\n",
    "    ),\n",
    ")\n",
    "query_tool_june = QueryEngineTool.from_defaults(\n",
    "    query_engine=june_engine,\n",
    "    name=\"june_2022\",\n",
    "    description=(\n",
    "        f\"Provides information about Uber quarterly financials ending June\" f\" 2022\"\n",
    "    ),\n",
    ")\n",
    "query_tool_march = QueryEngineTool.from_defaults(\n",
    "    query_engine=march_engine,\n",
    "    name=\"march_2022\",\n",
    "    description=(\n",
    "        f\"Provides information about Uber quarterly financials ending March\" f\" 2022\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = llm.callback_manager\n",
    "agent_worker = QueryUnderstandingAgentWorker.from_tools(\n",
    "    [query_tool_sept, query_tool_june, query_tool_march],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "agent = AgentRunner(agent_worker, callback_manager=callback_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Current Input: What is the financials for those two months?\n",
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question asks for financials for two months, and the choice (2) provides information about Uber quarterly financials ending June 2022, which includes two months..\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, got FieldInfo",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWhat is the financials for those two months?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/llama_index/core/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/llama_index/core/agent/runner/base.py:573\u001b[0m, in \u001b[0;36mAgentRunner.chat\u001b[0;34m(self, message, chat_history, tool_choice)\u001b[0m\n\u001b[1;32m    568\u001b[0m     tool_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_tool_choice\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    570\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mAGENT_STEP,\n\u001b[1;32m    571\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mMESSAGES: [message]},\n\u001b[1;32m    572\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 573\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatResponseMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWAIT\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n\u001b[1;32m    577\u001b[0m     e\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: chat_response})\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/llama_index/core/agent/runner/base.py:518\u001b[0m, in \u001b[0;36mAgentRunner._chat\u001b[0;34m(self, message, chat_history, tool_choice, mode)\u001b[0m\n\u001b[1;32m    515\u001b[0m result_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001b[39;00m\n\u001b[0;32m--> 518\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_step_output\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m    523\u001b[0m         result_output \u001b[38;5;241m=\u001b[39m cur_step_output\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/llama_index/core/agent/runner/base.py:370\u001b[0m, in \u001b[0;36mAgentRunner._run_step\u001b[0;34m(self, task_id, step, input, mode, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# not clear when you would do that by theoretically possible\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT:\n\u001b[0;32m--> 370\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mSTREAM:\n\u001b[1;32m    372\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_worker\u001b[38;5;241m.\u001b[39mstream_step(step, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/llama_index/core/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/llama_index/packs/query_understanding_agent/step.py:197\u001b[0m, in \u001b[0;36mQueryUnderstandingAgentWorker.run_step\u001b[0;34m(self, step, task, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, step: TaskStep, task: Task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskStepOutput:\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run step.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     output, is_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_task_step_response(output, step, is_done)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# sync step state with task state\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/llama_index/packs/query_understanding_agent/step.py:161\u001b[0m, in \u001b[0;36mQueryUnderstandingAgentWorker._run_step\u001b[0;34m(self, state, task, input)\u001b[0m\n\u001b[1;32m    155\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m    156\u001b[0m     [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_input), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(response))]\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Then, check for errors\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# dynamically create pydantic program for structured output extraction based on template\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m chat_prompt_tmpl \u001b[38;5;241m=\u001b[39m \u001b[43mget_chat_prompt_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurrent_reasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m llm_program \u001b[38;5;241m=\u001b[39m LLMTextCompletionProgram\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[1;32m    165\u001b[0m     output_parser\u001b[38;5;241m=\u001b[39mPydanticOutputParser(output_cls\u001b[38;5;241m=\u001b[39mResponseEval),\n\u001b[1;32m    166\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mchat_prompt_tmpl,\n\u001b[1;32m    167\u001b[0m     llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm,\n\u001b[1;32m    168\u001b[0m )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# run program, look at the result\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/llama_index/packs/query_understanding_agent/step.py:57\u001b[0m, in \u001b[0;36mget_chat_prompt_template\u001b[0;34m(system_prompt, current_reasoning)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m         messages\u001b[38;5;241m.\u001b[39mappend(ChatMessage(role\u001b[38;5;241m=\u001b[39mMessageRole\u001b[38;5;241m.\u001b[39mASSISTANT, content\u001b[38;5;241m=\u001b[39mraw_msg[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatPromptTemplate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_templates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/llama_index/core/prompts/base.py:237\u001b[0m, in \u001b[0;36mChatPromptTemplate.__init__\u001b[0;34m(self, message_templates, prompt_type, output_parser, metadata, template_var_mappings, function_mappings, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m template_vars \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message_template \u001b[38;5;129;01min\u001b[39;00m message_templates:\n\u001b[0;32m--> 237\u001b[0m     template_vars\u001b[38;5;241m.\u001b[39mextend(\u001b[43mget_template_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    240\u001b[0m     message_templates\u001b[38;5;241m=\u001b[39mmessage_templates,\n\u001b[1;32m    241\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m     function_mappings\u001b[38;5;241m=\u001b[39mfunction_mappings,\n\u001b[1;32m    247\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/llama_index/core/prompts/utils.py:12\u001b[0m, in \u001b[0;36mget_template_vars\u001b[0;34m(template_str)\u001b[0m\n\u001b[1;32m      9\u001b[0m variables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m formatter \u001b[38;5;241m=\u001b[39m Formatter()\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, variable_name, _, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate_str\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable_name:\n\u001b[1;32m     14\u001b[0m         variables\u001b[38;5;241m.\u001b[39mappend(variable_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/string.py:288\u001b[0m, in \u001b[0;36mFormatter.parse\u001b[0;34m(self, format_string)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string):\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _string\u001b[38;5;241m.\u001b[39mformatter_parser(format_string)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, got FieldInfo"
     ]
    }
   ],
   "source": [
    "agent.chat(\"What is the financials for those two months?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/sasha/miniconda3/envs/llama/lib/python3.11/string.py\u001b[0m(288)\u001b[0;36mparse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    286 \u001b[0;31m    \u001b[0;31m#  with format_spec and conversion and then used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    287 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 288 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0m_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    289 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    290 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
